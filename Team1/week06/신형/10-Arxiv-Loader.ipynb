{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b099f91d",
   "metadata": {},
   "source": [
    "# Arxiv\n",
    "\n",
    "[arXiv](https://arxiv.org/)은 물리학, 수학, 컴퓨터 과학, 정량 생물학, 정량 금융, 통계, 전기공학 및 시스템 과학, 경제학 분야의 200만 편의 학술 논문을 위한 오픈 액세스 아카이브입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07926d71",
   "metadata": {},
   "source": [
    "[API 도큐먼트](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.arxiv.ArxivLoader.html#langchain_community.document_loaders.arxiv.ArxivLoader)\n",
    "\n",
    "Arxiv 문서 로더에 접근하려면 `arxiv`, `PyMuPDF` 및 `langchain-community` 통합 패키지를 설치해야 합니다. \n",
    "\n",
    "`PyMuPDF` 는 arxiv.org 사이트에서 다운로드한 PDF 파일을 텍스트 형식으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2694ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 설치\n",
    "!pip install -qU langchain-community arxiv pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19348f",
   "metadata": {},
   "source": [
    "## 객체 생성\n",
    "\n",
    "이제 **model** 객체를 인스턴스화하고 문서를 로드할 수 있다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28d77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Query 에 검색하고자 하는 논문의 주제를 입력합니다.\n",
    "loader = ArxivLoader(\n",
    "    query=\"Chain of thought\",\n",
    "    load_max_docs=2,  # 최대 문서 수\n",
    "    load_all_available_meta=True,  # 메타데이터 전체 로드 여부\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3037bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-10-04', 'Title': 'Understanding Reasoning in Chain-of-Thought from the Hopfieldian View', 'Authors': 'Lijie Hu, Liang Liu, Shu Yang, Xin Chen, Zhen Tan, Muhammad Asif Ali, Mengdi Li, Di Wang', 'Summary': \"Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on improving performance, lacking a comprehensive framework to explain and understand the fundamental factors behind CoT's success. To bridge this gap, we introduce a novel perspective grounded in the Hopfieldian view of cognition in cognitive neuroscience. We establish a connection between CoT reasoning and key cognitive elements such as stimuli, actions, neural populations, and representation spaces. From our view, we can understand the reasoning process as the movement between these representation spaces. Building on this insight, we develop a method for localizing reasoning errors in the response of CoTs. Moreover, we propose the Representation-of-Thought (RoT) framework, which leverages the robustness of low-dimensional representation spaces to enhance the robustness of the reasoning process in CoTs. Experimental results demonstrate that RoT improves the robustness and interpretability of CoT reasoning while offering fine-grained control over the reasoning process.\", 'entry_id': 'http://arxiv.org/abs/2410.03595v1', 'published_first_time': '2024-10-04', 'comment': '28 pages, a new version of \"A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning\"', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI', 'cs.CL', 'cs.LG'], 'links': ['https://arxiv.org/abs/2410.03595v1', 'https://arxiv.org/pdf/2410.03595v1']}, page_content='Preprint. Under review.\\nUNDERSTANDING\\nREASONING\\nIN\\nCHAIN-OF-\\nTHOUGHT FROM THE HOPFIELDIAN VIEW\\nLijie Hu∗,1,2, Liang Liu∗,1,2,3, Shu Yang∗,1,2, Xin Chen4,\\nZhen Tan5, Muhammad Asif Ali1,2, Mengdi Li1,2, and Di Wang1,2\\n1Provable Responsible AI and Data Analytics (PRADA) Lab\\n2King Abdullah University of Science and Technology\\n3Soochow University\\n4University of Southampton\\n5Arizona State University\\nABSTRACT\\nLarge Language Models have demonstrated remarkable abilities across various\\ntasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to\\nenhance reasoning capabilities. However, existing research primarily focuses on\\nimproving performance, lacking a comprehensive framework to explain and un-\\nderstand the fundamental factors behind CoT’s success. To bridge this gap, we\\nintroduce a novel perspective grounded in the Hopfieldian view of cognition in\\ncognitive neuroscience. We establish a connection between CoT reasoning and\\nkey cognitive elements such as stimuli, actions, neural populations, and represen-\\ntation spaces. From our view, we can understand the reasoning process as the\\nmovement between these representation spaces. Building on this insight, we de-\\nvelop a method for localizing reasoning errors in the response of CoTs. Moreover,\\nwe propose the Representation-of-Thought (RoT) framework, which leverages the\\nrobustness of low-dimensional representation spaces to enhance the robustness of\\nthe reasoning process in CoTs. Experimental results demonstrate that RoT im-\\nproves the robustness and interpretability of CoT reasoning while offering fine-\\ngrained control over the reasoning process.\\n1\\nINTRODUCTION\\nLarge Language Models (LLMs) have demonstrated exceptional capabilities in following the nat-\\nural language instructions (Ouyang et al., 2022; Jin et al., 2024) and excelling across a variety of\\ndownstream tasks (Hu et al., 2023a; Zhang et al., 2023; Yang et al., 2024a;c;b). As reasoning skills\\nare crucial for tasks such as commonsense and mathematical reasoning (Rae et al., 2021), there is a\\ngrowing focus on enhancing these capabilities. One prominent approach is Chain-of-Thought (CoT)\\nprompting (Wei et al., 2022; Kojima et al., 2022), a simple yet highly effective technique to unleash\\nthe reasoning capability of LLMs. However, despite its success, a natural and fundamental research\\nquestion remains: How does the reasoning capability emerge through CoT prompting?\\nNumerous studies have sought to identify the key factors or elements that enable CoT to enhance the\\nreasoning capabilities of LLMs (Kojima et al., 2022; Wang et al., 2023a; Tang et al., 2023; Merrill &\\nSabharwal, 2023). Some works focus on improving CoT reasoning through query-based corrections\\n(Kim et al., 2023), knowledge-enhanced frameworks (Zhao et al., 2023), and symbolic reasoning\\nchains for faithful CoT (Lyu et al., 2023; Lanham et al., 2023). Other research has examined how\\nthe sequence of demonstrations, random labels (Min et al., 2022), or even meaningless tokens (Pfau\\net al., 2024) can positively influence reasoning performance. However, these works primarily fo-\\ncus on improving the model’s reasoning performance, and they do not provide a comprehensive\\nframework to explain the underlying factors driving CoT’s success.\\nTo understand the reasoning process in CoTs more deeply, we draw inspiration from cognitive neuro-\\nscience, specifically the relationship between cognition and brain function. In this field, the Hopfiel-\\ndian view (Hopfield, 1982) and the Sherringtonian view (Sherrington, 1906) represent two different\\nways of understanding neural computational models and cognitive mechanisms. While the Sherring-\\ntonian view of cognitive explanation focuses on specific connections between neurons in the brain,\\n∗The first three authors contributed equally to this work.\\n1\\narXiv:2410.03595v1  [cs.AI]  4 Oct 2024\\nPreprint. Under review.\\nAction axis\\nMotion Strength\\nAction +\\nAction -\\nAction +\\nAction -\\nStimuli +\\nStimuli -\\nAction axis\\nMotion axis\\nInput\\nNeuron Activations\\nOutput\\nOutput +\\nOutput -\\nInstruction +\\nNo Instruction -\\nRepresentation \\nSpace\\nRepresentation \\nSpace\\nActivated \\nNeurons\\nCognitive Brain\\nChain-of-Thought\\nMotion axis\\nNeural \\nPopulations\\nFigure 1: Illustration of the emergence of cognition in the brain and CoT reasoning from the Hop-\\nfieldian view.\\nthe Hopfieldian view emphasizes distributed computation across neural populations, where informa-\\ntion is not encoded by a single neuron but rather by the cooperative activity of many neurons. This\\nperspective is particularly suited to explaining complex cognitive functions like memory storage,\\npattern recognition, and reasoning. Thus, the Hopfieldian view is generally considered more ad-\\nvanced than the Sherringtonian view, especially in the context of explaining distributed computation\\nand the dynamics of neural networks (Barack & Krakauer, 2021). Based on these, a natural question\\nis: whether we can understand the reasoning in CoTs from the Hopfieldian view of cognition?\\nThe Hopfieldian view explains the production of behavioral actions as emerging from transforma-\\ntions or movements within neural populations in response to stimuli in the brain (Barack & Krakauer,\\n2021) (cf. Figure 1). This perspective approaches cognition at the level of representations, disre-\\ngarding the detailed roles of individual molecules, cells, and circuits, thus allowing the potential for\\na more conceptual and semantic understanding of complex cognitive systems. Viewing the CoT-\\ntriggered reasoning process in LLMs through this lens is intuitive: CoT prompting induces shifts in\\nthe model’s trajectory in much the same way that external stimuli shape cognitive responses, driving\\nrepresentation changes without altering the underlying system. Specifically, similar to the Hop-\\nfieldian mechanism, where the shift or movement in neural populations happens during cognition\\nitself, CoT influences reasoning during inference, controlling the logical steps without modifying\\nthe model’s parameters.\\nGiven the parallels between the CoT-triggered reasoning process and the Hopfieldian view of cogni-\\ntion in the brain, we first establish a connection between these two by aligning key elements: stim-\\nuli and actions, neural populations, and representation spaces. Particularly, we provide a general\\nframework for identifying the “representation spaces” of the “stimuli” given by CoTs. We conceptu-\\nalize the reasoning process elicited by CoT prompting as movement between representation spaces,\\nenabling us to improve and deepen our understanding of CoTs. Based on these connections, we then\\nleverage the strength of the Hopfieldian view to improve or further understand CoTs. Specifically,\\nby leveraging the “representation spaces” in CoTs, we develop a method for localizing the reasoning\\nerror in the responses. Moreover, by leveraging the robustness of low-dimensional representation\\nspaces, we propose a new framework, namely Representation-of-Thought (RoT), which enhances\\nthe robustness of CoTs. We summarize the key contributions of our work as follows:\\n1. We establish a connection between the reasoning process in CoTs and the Hopfieldian view\\nof cognition, grounded in cognitive neuroscience, to identify the key factors driving CoT’s\\nsuccess in zero-shot and few-shot settings. To the best of our knowledge, this is the first\\nknown attempt to leverage cognitive science for CoT interpretability by associating its core\\nelements with the Hopfieldian framework.\\n2. Based on these connections, we leverage the strength of the Hopfieldian view to understand\\nand further improve CoTs. We first consider how to localize the reasoning error based\\non the low-dimensional representation spaces. Then, by leveraging the robustness of the\\n2\\nPreprint. Under review.\\nHopfieldian view, we propose a new framework, RoT, to enhance the robustness of CoTs’\\nperformance.\\n3. Comprehensive experiments on three tasks, including arithmetic reasoning, commonsense\\nreasoning, and symbolic reasoning, reveal that our framework can provide intuitive and\\ninterpretable analysis, allowing error tracing and control for CoT reasoning.\\n2\\nRELATED WORK\\nChain-of-Thought (CoT).\\nThe CoT is a prompting technique that engages LLMs in step-by-step\\nreasoning rather than directly providing the answers (Nye et al., 2021). Studies have shown that\\nintroducing intermediate steps or learning from demonstrations can significantly improve the rea-\\nsoning performance of LLMs (Wei et al., 2022; Kojima et al., 2022). Given the success of CoT,\\nnumerous studies have explored its application to a variety of complex problems, including arith-\\nmetic, commonsense, symbolic reasoning (Wang et al., 2023c; Zhou et al., 2023; Wang & Zhou,\\n2024), and logic tasks (Creswell & Shanahan, 2022; Pan et al., 2023; Weng et al., 2023). Recently,\\nnumerous endeavors have been made to enhance the reasoning capabilities in LLMs (Wang et al.,\\n2023a; Dutta et al., 2024). For example, Kim et al. (2023) proposed a query-based approach to cor-\\nrect erroneous reasoning steps within a CoT. Zhao et al. (2023) introduced a knowledge-enhanced\\nmethod to improve the factual correctness for multi-pole open-domain QA tasks. Lyu et al. (2023)\\ndeveloped “faithful CoT”, i.e., a framework that first translates natural language queries into sym-\\nbolic reasoning chains and then solves the problem using CoT. Additionally, several studies have\\nalso focused on the sequence and quantity of demonstrations within the context, investigating their\\ncontributions to the final reasoning performance. For this, Min et al. (2022) discovered that even\\nrandom labels or ineffective reasoning steps can still improve the model’s reasoning performance.\\nLanham et al. (2023) demonstrated the impact of intervening in the CoT process by adding mistakes\\nor paraphrases. Pfau et al. (2024) showed that using meaningless filler tokens in place of a chain-of-\\nthought can surprisingly boost reasoning performance. However, these studies primarily focused on\\nhow to improve the CoT’s reasoning performance and do not provide a framework to analyze the fun-\\ndamental reasons, i.e., how does the reasoning capability emerge through CoT? Dutta et al. (2024)\\ninvestigates the neural sub-structures within LLMs that manifest Chain-of-Thought (CoT) reasoning\\non the Llama-2-7B model. Similarly, Rai & Yao (2024) explores neurons in the feed-forward layers\\nof LLMs to analyze their arithmetic reasoning capabilities on the Llama-2-7B model. Both studies\\nare grounded in the Sherringtonian view of neural activity. In contrast, we adopt the Hopfieldian\\nperspective to bridge this gap, focusing on representations rather than individual neurons. We apply\\nour approach across three different downstream tasks and can further extend our analysis to larger\\nmodels like Llama-2-70B.\\nInterpretability of LLMs. Interpretability plays a key role in a deeper understanding of LLMs\\nto identify potential risks and better meet human requirements (Zou et al., 2023). Common inter-\\npretability strategies include (i) Salience maps, which rely on highlighting the regions in the input\\nthat are attended by the model (Simonyan et al., 2014; Smilkov et al., 2017; Clark et al., 2019;\\nHu et al., 2023c;b; Lai et al., 2024); (ii) Feature visualization, which creates representative inputs\\nindicative of particular neurons’ activations (Szegedy et al., 2014; Nguyen et al., 2016; Fong &\\nVedaldi, 2018; Nguyen et al., 2019); and (iii) Mechanistic interpretability, which employs reverse-\\nengineering tools to explain networks based on circuits and node-to-node connections (Olah et al.,\\n2020; Olsson et al., 2022; Wang et al., 2023b). However, these methods often require substantial\\nhuman intervention and are limited in terms of scalability or interpretability, especially for the large\\nlanguage models (Fong & Vedaldi, 2018; Jain & Wallace, 2019; Hu et al., 2024). Thus, these meth-\\nods cannot be directly used to interpret CoT reasoning. Additionally, most current approaches focus\\non representation-level analysis without considering how these representations connect to concepts\\nlearned during pre-training (Bricken et al., 2023; Templeton et al., 2024). Other works investigate\\nthe localization and representation of concepts in the network (Kim et al., 2018; Li et al., 2024),\\nlinear classifier probing to uncover input properties (Belinkov, 2022), fact localization and edit-\\ning (Meng et al., 2022; Zhong et al., 2023; Cheng et al., 2024a;b), concept erasure (Shao et al.,\\n2023; Gandikota et al., 2023), and corrective analysis (Burns et al., 2023), etc. These observations\\nare aligned with RepE (Zou et al., 2023), which emphasized the nearly linear nature of LLM repre-\\nsentations (Park et al., 2024). However, none of these approaches directly address the inner workings\\nof CoT reasoning. While recent work has begun exploring connections between LLM interpretabil-\\nity and cognitive neuroscience (Vilas et al., 2024). However, it does not discuss the Hopfieldian view\\n3\\nPreprint. Under review.\\nand also does not discuss how to explain the reasoning process in CoTs via cognitive neuroscience.\\nOur work provides the first attempt to interpret CoT reasoning from the Hopfieldian perspective.\\n3\\nPRELIMINARIES\\nLarge Language Models and Prompting. Prompts can take various forms, such as a single sen-\\ntence or longer paragraphs, and may include additional information or constraints to guide the\\nmodel’s behavior. Let M : X 7→Y be an LLM that takes an input sequence x = (x1, x2, . . . , xq) ∈\\nX and produces an output sequence y = (y1, y2, . . . , ym) ∈Y. The model is typically trained to op-\\ntimize the conditional probability distribution pr(y|x), which assigns a probability to each possible\\noutput sequence y given x. To incorporate a prompt w with the input sequence x, we can concate-\\nnate them into a new sequence ˆx = (w, x1, x2, . . . , xq). The conditional probability distribution\\npr(ˆy|ˆx) is then computed using ˆx. Formally, the probability of the output sequence ˆy given ˆx is:\\npr(ˆy|ˆx) =\\nm\\nY\\ni=1\\npr(yi|y<i, ˆx),\\nwhere y<i represents the prefix of the sequence y up to position i −1, and pr(yi|y<i, ˆx) denotes the\\nprobability of generating yi given y<i and ˆx.\\nThe Hopfieldian View. In cognitive neuroscience, two prominent perspectives aim to explain cog-\\nnition: the Sherringtonian view and the Hopfieldian view.1 The Hopfieldian view focuses on un-\\nderstanding behavior through computation and representation within neural spaces, rather than the\\nspecific biological details of neurons, ion flows, or molecular interactions (Hopfield, 1982; 1984;\\nHopfield & Tank, 1986). It operates at a higher level of abstraction, emphasizing the role of repre-\\nsentations and the computations performed on them.\\nThis approach conceptualizes cognition as transformations between representation spaces. At the\\nimplementation level, the collective activity of neurons is mapped onto a representation space, which\\ncontains a low-dimensional representational manifold. Algorithmically, Hopfieldian computation\\nviews these representation spaces as fundamental entities, with movements within or transformations\\nbetween them as the central operations. The representations themselves are structured as basins\\nof attraction within a state space, and while they are implemented by neural structures (whether\\nindividual neurons, neural populations, or other components), the focus is on the dynamics of the\\nsystem rather than its specific biological mechanisms. Most Hopfieldian models, in practice, center\\non the activity of neural populations.\\nA parameter space defines the dimensions of variation within these representational spaces, aligning\\nwith quality-space approaches from philosophy, where content is similarly structured. Computa-\\ntions over these representations are understood as dynamic transformations between spaces or shifts\\nwithin them, characterized by features like attractors, bifurcations, limit cycles, and trajectories.\\nUltimately, cognitive functions are realized through these dynamic movements within or between\\nrepresentational spaces.\\nLinear Representations in Language Models. Recent investigations into the internal mechanics\\nof LLMs have revealed intriguing properties of their learned representations. Park et al. (2024)\\nposited that high-level semantic features such as gender or honesty could be linearly represented\\nas directions within the model’s representation space. This can be illustrated by the well-known\\nword analogy task using a word embedding model (Mikolov et al., 2013). By defining M(·) as a\\nfunction of extracting the representations of a given word by a word embedding model, the oper-\\nation M(Spain) −M(Madrid) + M(Paris) often results in an output close to M(France), where\\nM(Spain) −M(Madrid) can be considered as the representation vector of the abstract “capital of”\\nfeature in the embedding space. Concurrently, research on interpretable neurons (Dale et al., 2023;\\nOrtiz-Jiménez et al., 2023; Voita et al., 2024) has identified neurons that consistently activate for\\nspecific input features or tasks, suggesting that these features may also be represented as directions\\nin the LLMs’ neuron space. For instance, Tigges et al. (2023) use the PCA vector between LLMs’\\nhidden states on instructions “positive” and “negative” to find the sentiment direction in LLMs. Ad-\\nditionally, recent works (Zou et al., 2023; Arditi et al., 2024) show the effectiveness of engineering\\non language models using these directions. For example, adding multiples of the “honesty” direction\\nto some hidden states has been sufficient to make the model more honest and reduce hallucinations.\\n1See Appendix A for an introduction to the Sherringtonian view. For a detailed comparison between these\\ntwo views, refer to (Barack & Krakauer, 2021) and (Bechtel, 2007).\\n4\\nPreprint. Under review.\\n4\\nBRIDGING REASONING IN COTS AND THE HOPFIELDIAN VIEW\\nIn this section, we aim to build a bridge between the reasoning process in CoTs and the cogni-\\ntive brain from the Hopfieldian view. We will particularly associate the main elements (stimuli,\\nneural populations, and representation spaces) in the Hopfieldian view. After understanding these\\nelements, we can leverage the strength of the Hopfieldian view to deepen our understanding of the\\nreasoning process in current CoTs and further improve it. Note that we will leave other elements in\\nthe Hopfieldian view, such as attractors and state space, as future work.\\nStimuli and Actions. Stimuli and actions are key components of how the brain processes informa-\\ntion and interacts with the environment. Actions refer to the motor responses or behaviors that result\\nfrom cognitive processing, which are responses given by LLMs through CoTs.\\nStimuli refer to external or internal events, objects, or changes in the environment that are detected\\nby the sensory systems and can influence cognitive processes and behavior. Based on this, we can\\nadopt the term “stimuli” from cognitive science in the context of CoTs to refer to specific prompt\\ntext or instructions that trigger CoT reasoning. Specifically, in the zero-shot setting, we define the\\nstimulus as szero to represent a set of supplementary instructions in the prompt that encourage the\\nmodel to provide more intermediate reasoning steps before arriving at a final answer. For example,\\nit can be “let’s think step by step” or “make sure to give steps before your answer”. In the few-shot\\nsetting, the stimulus sfew is defined as the sequence of demonstrations D = {(˜q1, ˜a1), (˜q2, ˜a2), . . . }\\nin the prompt, where ˜qi represents the query and ˜ai is the corresponding response. In the following\\ndiscussion, we use s+ to indicate that stimuli are included in the model’s input and s−to indicate\\nthat no stimuli are added. Note that we avoid using explicitly negative stimuli, such as “please be\\ncareless and answer the following question”, because a well-aligned model would likely refuse to\\nbehave in such a manner (Ouyang et al., 2022).\\nNeural Populations. As we mentioned, in the Hopfieldian view, representations are realized by\\nvarious forms of neural organization, especially populations. Identifying these “neural populations”\\nin CoTs is especially important. In our framework, there are two steps for finding them.\\n(i) Stimulus Set Designing. Here our goal is to elucidate the sensitivity of LLMs to different CoT\\nprompts with stimuli. Understanding such sensitivity could help us know the neural populations\\nraised from the stimuli.\\nIn detail, we construct a prompt set.\\nFor each query q, we consider\\ntwo forms of prompts: positive one (with stimuli) as p+ = T(s+, q) and negative one (with-\\nout stimuli) as p−= T(s−, q), where T is the prompt template. Specifically, for each query qi,\\nwe construct M number of prompts for both of them with different stimuli, which is denoted as\\nPi = {pi,−\\n1 , pi,+\\n1 , pi,−\\n2 , pi,+\\n2 , . . . , pi,−\\nM , pi,+\\nM }. Such construction is to make our following neural\\npopulations less dependent on the specific template form. Thus, in total, we have a stimulus set\\nP ∗= {P1, P2, · · · , PN}, where N is the number of queries. These contrastive pairs of prompts\\nwill be used to identify neural populations given by these stimuli.\\n(ii) Identifying Neural Populations. Intuitively, the neural populations should be the most influ-\\nential activation vectors of these prompts or stimuli. In detail, for each prompt in P ∗, the next\\nstep is capturing the network architecture’s corresponding neural populations. Since LLMs rely on\\ntransformer-based architecture to store distinct representations intended for different purposes, it\\nis crucial to design the extraction process to capture task-specific representations carefully. For a\\ngiven prompt p+ or p−, we will find the “most representative token”, which encapsulates rich and\\nhighly generalizable representations of the stimuli. Here we select the last token after tokenizing the\\nprompt, which is based on the observation in Zou et al. (2023) that it is the most informative token\\nfor decoder-only or auto-regressive architecture models.\\nOnce the last token position is identified, we can naturally select some of its activations (hidden\\nstate) in hidden layers. Previous studies (Fan et al., 2024; Cosentino & Shekkizhar, 2024) have\\nshown that not all layers store important information about reasoning; thus we focus on a subset of\\nthem to reduce the computation cost, whose indices are denoted as a set K (in practice, K is always\\nthe last several layers). Thus, we have a collection of activation vectors. However, since we are\\nfocusing on the reasoning of CoT, studying the neural populations raised from the stimuli rather\\nthan the whole prompt is more important. Thus, we consider the difference in the activations of\\npairs of prompts. Specifically, for a pair (p+, p−), we can get their activations for all selected layers\\nK: {hk(p+)}k∈K and {hk(p−)}k∈K, where hk(p) refers to the activation vector of the k-th layer for\\na given input prompt p. Then the differences of activations {˜hk(p)}k∈K are the neural populations\\n5\\nPreprint. Under review.\\nfor such stimuli, where ˜hk(p) = hk(p+) −hk(p−) represents the most influential information we\\nget from the stimuli for the query. Based on this, for each hidden layer in K, we have the neural\\npopulation for all queries, which is denoted as\\nh∗\\nk = {˜hk(P1), ˜hk(P2), . . . , ˜hk(PN)}.\\n(1)\\nRepresentation Spaces. After we have the neural populations for each selected hidden layers,\\nour final goal is to find the representation space. In the Hopfieldian view, the representation of\\ninformation is thought to occur within low-dimensional space embedded within higher-dimensional\\nneural spaces. Thus, these representation spaces will be the most informative subspaces of the neural\\npopulations. Here we adopt the s-PCA to find such an s dimensional subspace. Specifically, for the\\nk-th layer where k ∈K, we perform PCA analysis on h∗\\nk:\\nRk = PCA(h∗\\nk).\\n(2)\\nThen, the space spanned by this eigenvector will be the representation space for this layer. Motivated\\nby the previous linear representation introduced in Section 3, here we set s = 1, i.e., we only\\nconsider the principal component. Intuitively, this means each representation space will focus on\\none “concept”.\\n5\\nAPPLICATIONS OF HOPFIELDIAN VIEW TO COTS\\nIn the previous section, we mainly discussed how each element in the Hopfieldian view corresponds\\nto the reasoning in CoTs. From our previous view, we can understand the reasoning process as the\\nmovement between these representation spaces. Based on these connections, we can leverage the\\nstrength of the Hopfieldian view to improve or further understand CoTs. In this section, we first\\nconsider how to localize the reasoning error based on the low dimensional representation spaces.\\nThen, by leveraging the robustness of the Hopfieldian view, we propose a new framework, namely\\nRepresentation of Thought, that enhances the performance robustness of CoTs.\\n5.1\\nREASONING ERROR LOCALIZATION\\nIn this task, for a given query, we want to check if there are some reasoning errors in the response\\nby CoTs. If so, we aim to localize these errors. As in the Hopfieldian view, cognition occurs\\nwithin low-dimensional representation spaces. Reasoning errors can be identified by analyzing the\\nstructure of these spaces, such as when certain directions Rk (representing specific cognitive factors)\\nare disproportionately activated or suppressed. This can help localize the source of the error within\\nthe cognitive process. Motivated by this, we can leverage the internal structure of spaces we have\\nlearned via PCA to locate the reasoning error for a given query in CoTs.\\nIntuitively, since the reasoning occurs within these representation spaces, if there is a reasoning error\\nin the response, then during the reasoning process, some tokens make the activations (hidden states)\\nof the response far from the corresponding representation spaces. This is because if these activations\\nare far from the spaces, CoTs do not reason the corresponding “concepts” in the response. Motivated\\nby this, our idea is to iteratively check the tokens in the response to see whether they are far from\\nthe representation spaces.\\nMathematically, for a given prompt T via CoT of query x with its response y = (y1, y2, · · · , ym),\\nwe will iteratively feed the prompt with a part of the response, i.e., Ti = T ⊕y≤i, where ⊕is the\\nstring concatenation. If the activations of Ti−1 are close to while those of Ti are very far from the\\nrepresentation spaces {Rk}k∈K in (2), then we can think the i-th token yi makes an reasoning error.\\nWe use the following criterion to access and/or evaluate the quality of the rationale for Ti:\\nscores(Ti) = Mean({scoresk(Ti)}k∈K), where scoresk(Ti) = hk(Ti)⊤Rk −δ.\\n(3)\\nHere δ is the threshold, scoresk(Ti) is the rationale for the k-th representation space, and scores(Ti)\\nis the average score across all layers in K. When the score is less than 0, it indicates that the\\nactivations of prompt Ti are far from the representation spaces. See Algorithm 1 for details.\\n5.2\\nREPRESENTATION OF THOUGHT\\nThe Hopfieldian view of cognition offers a framework that can potentially be used to control or\\ninfluence cognitive processes. Specifically, influencing neural populations directly offers a more\\n6\\nPreprint. Under review.\\nAlgorithm 1 Reasoning Error Localization\\nRequire: Prompt T for query x; response y = (y1, · · · , ym) of the prompt T via a CoT; threshold\\nδ > 0; representation vectors {Rk}k∈K in (2) with layer set K.\\n1: for i = 1, · · · , m do\\n2:\\nDenote a new prompt Ti = T ⊕y≤i. Using the same process as in Section 4 to get the\\nactivations of Ti in layers in the set K, which are denoted as hk(Ti), k ∈K.\\n3:\\nCalculate scores(Ti) = Mean({scoresk(Ti)}k∈K) in (3).\\n4:\\nif scores(Ti) < 0 and scores(qT −1) ≥0 then\\n5:\\nMark token yi as a “reasoning error”.\\n6:\\nend if\\n7: end for\\nrobust way to control cognition compared to simply providing different stimuli. Firstly, influenc-\\ning neural populations directly allows the manipulation of the core dynamics of neural state spaces,\\nincluding attractor states, bifurcations, and transitions between cognitive states. This direct inter-\\nvention bypasses the variability and unpredictability associated with external stimuli, which depend\\non the individual’s perception, attention, and prior experiences. Moreover, external stimuli are sub-\\nject to various forms of noise and variability, including sensory processing errors, environmental\\ndistractions, and individual differences in interpretation. Direct manipulation of neural populations\\ncan reduce these sources of noise, providing a cleaner and more consistent pathway to controlling\\ncognitive states.\\nOur RoT leverages representation spaces’ structure to enhance the robustness of reasoning in CoTs.\\nIntuitively, we can manipulate a given query’s activations to be closer to the representation spaces\\nto enhance robustness since these spaces are the inherent entities in the reasoning process. After the\\nmanipulation, the hidden states will be less dependent on the specific form of the prompt, query, and\\nstimuli but will be more dependent on the intrinsic entities of the reasoning task.\\nMathematically, for a given prompt T via CoTs of query x. By using a similar procedure as in the\\nNeural Populations section, we can get its neural populations {hk(T)}k∈K. In RoT, motivated by\\n(Zou et al., 2023; Arditi et al., 2024), we can manipulate them by injecting the directions of their\\ncorresponding representation spaces to make them closer to these spaces:\\nh′\\nk(T) =\\n\\x1ahk(T) + αRk\\nif k ∈K\\nhk(p)\\notherwise ,\\n(4)\\nwhere h′\\nk(T) denotes the manipulated hidden state, α is a scaling factor controlling the manipulation\\nstrength. Its sign should follow the sign of hk(T)⊤Rk.\\nBy directly manipulating neural populations, RoT offers a more precise and interpretable method\\nfor influencing the model’s output compared to traditional prompt engineering techniques. This\\napproach not only enhances control over the model’s behavior but also improves the transparency\\nand predictability of the generation process.\\n6\\nEXPERIMENTS\\nIn this section, we will perform experimental studies on the above two applications to verify the\\ncorrectness of our understanding from the Hopfieldian view.\\n6.1\\nEXPERIMENTAL SETUP\\nDatasets. Our experiments are performed on benchmark datasets for diverse reasoning problems.\\nWe consider 6 datasets for 3 different tasks: Arithmetic Reasoning, Commonsense Reasoning, and\\nSymbolic Reasoning. Specifically, for Arithmetic Reasoning, we select GSM8K (Cobbe et al., 2021)\\nand SVAMP (Patel et al., 2021); we study StrategyQA (Geva et al., 2021) and CommonsenseQA\\n(CSQA) (Talmor et al., 2019) for Commonsense Reasoning; lastly, for Symbolic Reasoning, we\\nchoose the Coin Flip (Wei et al., 2022) and Random Letter datasets, where the latter one is con-\\nstructed from the Last Letter dataset (Wei et al., 2022). More details and statistics of the datasets are\\nprovided in Appendix B.1.\\nLLMs. We employ Llama-2-7B-Chat (Touvron et al., 2023) and Llama-3-8B-Instruct (Meta, 2024)\\nto evaluate their precision performance (accuracy) both before and after applying RoT to differ-\\n7\\nPreprint. Under review.\\nTable 1: Results of RoT and CoT based on different LLMs on a variety of reasoning tasks. Green\\nindicates an equal or improved accuracy compared to the Base method, while red indicates an accu-\\nracy decrease. It can be observed that, compared to CoT prompting, RoT achieves more consistent\\naccuracy improvements across a variety of tasks.\\nMethod\\nArithmetic\\nCommonsense\\nSymbolic\\nGSM8K\\nSVAMP\\nCSQA\\nStrategyQA\\nCoin Flip\\nRandom Letter\\nLlama-2-7B-Chat\\nBase\\n26.00\\n54.00\\n47.75\\n63.62\\n44.80\\n20.33\\n+ CoTZ\\n26.31\\n46.00\\n43.41\\n62.05\\n52.75\\n24.33\\n+ RoTZ\\n26.23\\n54.33\\n48.24\\n63.54\\n45.45\\n20.67\\nBase\\n26.00\\n54.00\\n47.75\\n63.62\\n44.80\\n20.33\\n+ CoTF\\n4.62\\n38.67\\n53.07\\n59.26\\n47.60\\n31.00\\n+ RoTF\\n25.55\\n56.00\\n48.16\\n63.80\\n45.50\\n20.33\\nLlama-3-8B-Instruct\\nBase\\n73.31\\n80.67\\n72.65\\n65.07\\n68.90\\n44.00\\n+ CoTZ\\n74.45\\n82.33\\n72.24\\n66.07\\n90.45\\n43.00\\n+ RoTZ\\n74.83\\n83.33\\n72.89\\n65.24\\n76.35\\n47.67\\nBase\\n73.31\\n80.67\\n72.65\\n65.07\\n68.90\\n44.00\\n+ CoTF\\n72.02\\n81.00\\n73.63\\n62.75\\n96.50\\n50.67\\n+ RoTF\\n74.37\\n83.67\\n73.30\\n65.94\\n70.30\\n43.66\\nent datasets. Furthermore, we use Llama-2-13B-Chat (Touvron et al., 2023) and Llama-2-70B-\\nChat (Touvron et al., 2023) to show that our method performs effectively in larger-scale models.\\nBaselines. Since our goal is to analyze the performance and robustness before and after control\\nmodel reasoning in both zero-shot and few-shot settings, we focus on three baselines in our study:\\n1) Base: as the simplest approach with LLMs for reasoning, feed the model with only one question\\nquery. 2) CoTZ (Kojima et al., 2022): the most common zero-shot CoT is employed to provide\\na thought path. 3) CoTF (Wei et al., 2022): directly using some demonstrations before asking a\\nquestion to LLMs.\\nEvaluation Metrics. We consider the performance of RoT zero-shot (RoTZ) and few-shot (RoTF )\\nsettings. Besides the utility of performance, which is evaluated by accuracy, we also conducted\\nresults on the robustness against forms of prompts. For zero-shot settings, we selected three different\\nspecific instructions: (1) Let’s think step by step. (2) Let’s think about this logically. (3) Let’s solve\\nthis problem by splitting it into steps. For few-shot settings, we conducted two studies: 1) Using\\nthe original order of the given demonstrations, shown in Appendix C.3. 2) Based on experiment\\n1, we randomly shuffled the order of the demonstrations. Then we use the accuracy difference to\\nconsider the robust performance of our approach. Specifically, given a list of accuracy results from\\nA = { ˜A1, ˜A2, · · · , ˜An} given by different prompts mentioned above, the robust score is calculated\\nby their pairwise difference: Pn\\ni=1\\nPn\\nj=i+1 | ˜Ai −˜Aj|. The answer extraction process is based on\\nthe methodology outlined by Kojima et al. (2022). Detailed procedures and results are provided in\\nthe Appendix C.2.\\nExperimental Settings.\\nIf not explicitly stated, in all experiments, we set the number of stimuli\\nprompts M = 1, the sample number N = 128, and select the samples by high perplexity. At the\\nsame time, we set the max new tokens to 512 in the generation stage and pick the last 5 layers to\\ncontrol. We choose α based on the accuracy performance on each dataset. In the reasoning error\\nlocalization experiment, we set δ = 10. We use float16 to load large language models and employ\\ngreedy search as our decoding strategy. All experiments are conducted using one NVIDIA L20 GPU\\n(except Llama-2-70B-Chat which uses three NVIDIA A100 GPUs).\\n6.2\\nEXPERIMENTAL RESULTS\\nUtility Performance. We first consider the utility performance of RoT. As shown in Table 1, we\\ncan see that: 1) The original CoT performs unstable on different tasks. Generally speaking, CoTZ\\nand CoTF appear better, but they are lower than Base in some datasets, such as the CSQA dataset\\nin the zero-shot scenario, which is consistent with the observation in (Kojima et al., 2022). At the\\nsame time, for few-shot, CoTF performs extremely poorly in the GSM8k dataset because Llama-\\n8\\nPreprint. Under review.\\nTable 2: The robust results of our approach and different general baselines with CoT on each task.\\nBold text indicates optimal results in a single dataset.\\nMethod\\nArithmetic\\nCommonsense\\nSymbolic\\nGSM8K\\nSVAMP\\nCSQA\\nStrategyQA\\nCoin Flip\\nRandom Letter\\nLlama-2-7B-Chat\\nCoTZ\\n5.46\\n11.34\\n6.54\\n6.04\\n8.80\\n14.00\\nRoTZ\\n3.02\\n1.32\\n1.64\\n0.70\\n0.30\\n0.68\\nCoTF\\n1.44\\n0.00\\n2.78\\n0.48\\n2.70\\n2.00\\nRoTF\\n0.08\\n0.67\\n0.00\\n1.88\\n0.00\\n0.00\\nLlama-3-8B-Instruct\\nCoTZ\\n33.36\\n85.32\\n2.94\\n5.94\\n13.80\\n18.66\\nRoTZ\\n2.58\\n2.66\\n0.82\\n1.14\\n16.40\\n11.34\\nCoTF\\n0.23\\n0.33\\n0.74\\n0.26\\n0.45\\n1.00\\nRoTF\\n0.37\\n0.34\\n0.33\\n0.26\\n0.45\\n1.00\\n7B\\n13B\\n70B\\nModel Size\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nAccuracy (%)\\n26.00\\n35.63\\n52.08\\n25.55\\n36.47\\n52.39\\nBase\\nRoT\\n(a) Larger scale\\n32\\n64\\n128\\n256\\n512\\nLayer\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nMean Values (%)\\n54.33\\n54.33\\n54.33\\n53.78\\n53.67\\nCoT accuracy = 46\\n(b) Numbers of samples.\\n1\\n3\\n5\\n10\\n15\\nLayer\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nMean Values (%)\\n53.67\\n53.67\\n54.33\\n53.67\\n45.0\\nCoT accuracy = 46\\n(c) Last layer\\nFigure 2: Ablation study of our approch. (a) Results on a larger scale on the GSM8K dataset. (b)\\nResults on the number of samples on the SVAMP dataset. (c) Results on the number of selected\\nlayers on the SVAMP dataset.\\n2-7B-Chat repeats the given demonstrations, resulting in a reduction in the number of valid tokens.\\nCompared to CoTs, our RoT performs strongly in generalization on these datasets but may have\\nlower accuracy in some cases. This is because, in RoT, we add additional directions to the hidden\\nstates of the prompt. These manipulations will cause a loss of information regarding the original\\nquery, making the accuracy lower. 2) In terms of different models, the Llama-3-8B-Instruct model\\nhas been improved more significantly. For example, with Llama-2-7B-Chat as the backbone, RoTZ\\nis improved by only 0.23 and 0.33 compared with Base on the GSM8K and SVAMP datasets, re-\\nspectively; with Llama-3-8B-Instruct, the improvements are 1.52 and 2.66, respectively. This is\\nprimarily because the model is trained on a larger corpus and has learned more knowledge, so the\\nactivations contain richer information and can better capture related representations.\\nRobustness Analysis. We also conducted experiments on robustness, and the results are shown in\\nTable 2 (more results are included in Appendix B.2). From this table, we can observe that RoT\\ndemonstrates a remarkable advancement over CoT in terms of robustness. We found that CoT\\nmethods are very sensitive to prompt design and sometimes fail to output the corresponding response\\nbased on the given instruction. However, our RoT extracts more essential information from the\\nrepresentation engineering level, making it more adaptable to various prompts. Note that for Llama-\\n3-8B-Instruct, there are two datasets (SVAMP and Coin Flip) that do not provide robust performance\\ngains. This is because Llama-3-8B-Instruct is a very strong model, while Coin Flip and SVAMP are\\ntwo relatively easy tasks (as can be seen from the Table 1, the accuracy of CoTs in the SVAMP\\ndataset is greater than 81%, and in the Coin Flip dataset is greater than 90%). These two factors may\\ncause it to over-capture too many irrelevant concepts from the stimuli, thus pointing to the wrong\\nreasoning direction.\\nResults on Larger Models. To further demonstrate the effectiveness of our approach, we conduct\\nresearch on a larger scale. Specifically, we follow the few-shot settings, and evaluate two larger\\nmodels (Llama-2-13B-Chat and Llama-2-70B-Chat) on the GSM8K dataset. From Figure 2a, we\\ncan see that our approach has improved performance on both 13B and 70B, but the improvement on\\n70B is smaller. We speculate that this is because the 70B model contains very rich knowledge, and\\nthe knowledge contained in a few demonstrations has limited improvement on the model.\\n9\\nPreprint. Under review.\\nJohn collects peaches for 3 hours. He can collect 2 peaches a minute. How many peaches does\\nhe collect?\\nTo find out how many peaches John collects, we need to multiply the time he spends collecting\\xa0\\npeaches (3 hours) by the number of peaches he can collect per minute (2 peaches/minute). So\\n-0.09\\n0.62\\xa0 \\xa0\\xa0\\nJohn collects:\\xa03 hours x 2 peaches/minute = 6 peaches.\\xa0Therefore, John collects 6 peaches.\\n-0.15\\n0.62\\n-0.32\\n0.62\\xa0 \\xa0\\n0.62\\n-0.06\\nFigure 3: A real case of reasoning error localization by using Llama-2-7B-Chat in a zero-shot sce-\\nnario on GSM8K using Algorithm 1. The green bar indicates that the reasoning snippet is correct,\\nand the red bar means that the reasoning snippet may be wrong. The numbers in the bar are the\\nscores calculated by Algorithm 1.\\nCase Study of Reasoning Error Localization. We conducted a reasoning error localization ex-\\nperiment. We can calculate the token-level score with Algorithm 1 through our approach. Figure 3\\nshows that our approach can localize those errors in the response through CoT. In this case, Llama-\\n2-7B-Chat did not really understand the known information in the given question and calculated\\ndifferent units (hour and minute). Specifically, before calculating the hour and minute tokens, the\\nscores of the tokens are all greater than zero, indicating no potential errors, while when calculating\\nthe hour and minute tokens, our method detects potential conflicts with previous knowledge and\\nthus obtains a score less than zero. We also show our additional case study in Appendix D.\\nTable 3: Results for different sample selection strategies.\\nModel\\nPerlexity\\nRandom\\nLow\\nHigh\\nSeed1\\nSeed2\\nSeed3\\nSeed Avg.\\nLlama-2-7B-Chat\\n23.43\\n25.55\\n25.32\\n25.24\\n25.32\\n25.30\\nLlama-3-8B-Instruct\\n74.22\\n74.37\\n74.52\\n74.37\\n73.92\\n74.27\\n6.3\\nABLATION STUDY\\nNumber of Samples. We conducted an ablation study on how to select samples and how many\\nsamples N in the stimulus set for constructing neural populations are sufficient. For the sample\\nselection strategy, we focus on two different strategies and evaluate these on the GSM8K dataset:\\n1) Random strategy. We randomly select samples in the training dataset using three random seeds.\\n2) Low Perplexity strategy. We select samples based on low perplexity. 3) High Perplexity strategy.\\nSimilar to the low perplexity strategy, we select samples based on high perplexity. As shown in Ta-\\nble 3, we can observe that the high perplexity strategy has better and more generalized performance.\\nThis is because high perplexity usually means low confidence in LLMs. Therefore, if a question has\\na higher perplexity, the question has more latent knowledge information.\\nFor the number of samples N, we consider the set N = {32, 64, 128, 256, 512} and calculate their\\naverage accuracy scores on the SVAMP dataset using three different seeds. From Figure 2b, we can\\nsee that the performance is quite stable for different numbers of samples. However, there is still a\\nlittle decrease when N is large enough. This is because when N is large enough, the representation\\nspaces contain richer information. Thus, adding the directions in (4) will make the query lose more\\nof its query information, causing a lower accuracy.\\nNumber of Selected Layers. Here we study the effect of different numbers of selected layers |K|\\nfor neural populations. While LLMs have many layers, such as Llama-2-7B, which contains 32\\nlayers, recent studies have shown that not all layers store important information about reasoning and\\nthat this information is usually found in the last layers of the model (Fan et al., 2024; Cosentino &\\nShekkizhar, 2024). Therefore, we consider the last L layers, where L = {1, 3, 5, 10, 15}.\\nIn this experiment, we evaluate it with three different seeds. Figure 2c displays the result of average\\naccuracy scores on the SVAMP dataset. From this figure, we can see that the accuracy first increases\\nand then shows a decreasing trend as the number of control layers increases. This is because when\\nthe number of layers is very small, each manipulation will correct some of the reasoning errors.\\nHowever, in RoT we have to manipulate each activation in the layer of the set K, and each manip-\\n10\\nPreprint. Under review.\\nulation will lose some information about the query. Thus, the accuracy decreases when the number\\nof layers is larger.\\n7\\nCONCLUSION\\nIn this paper, we proposed a novel framework to explain and understand the fundamental factors\\nbehind CoT’s success. Specifically, we first connected CoT reasoning and the Hopfieldian view\\nof cognition in cognitive neuroscience. Then, we developed a method for localizing reasoning er-\\nrors and proposed the RoT framework to enhance the robustness of the reasoning process in CoTs.\\nExperimental results demonstrate that RoT improves the robustness and interpretability of CoT rea-\\nsoning while offering fine-grained control over the reasoning process.\\n11\\nPreprint. Under review.\\nREFERENCES\\nAndy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, and Neel\\nNanda. Refusal in language models is mediated by a single direction. CoRR, abs/2406.11717,\\n2024.\\nDavid L Barack and John W Krakauer. Two views on the cognitive brain. Nature Reviews Neuro-\\nscience, 22(6):359–371, 2021.\\nHorace B Barlow. Summation and inhibition in the frog’s retina. The Journal of physiology, 119(1):\\n69, 1953.\\nWilliam Bechtel. Mental mechanisms: Philosophical perspectives on cognitive neuroscience. Psy-\\nchology Press, 2007.\\nYonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Comput. Linguistics,\\n48(1):207–219, 2022.\\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly,\\nNicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monoseman-\\nticity: Decomposing language models with dictionary learning. Transformer Circuits Thread,\\n2023.\\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in lan-\\nguage models without supervision. In Proceedings of ICLR 2023, 2023.\\nKeyuan Cheng, Muhammad Asif Ali, Shu Yang, Gang Lin, Yuxuan Zhai, Haoyang Fei, Ke Xu,\\nLu Yu, Lijie Hu, and Di Wang. Leveraging logical rules in knowledge editing: A cherry on the\\ntop. CoRR, abs/2405.15452, 2024a.\\nKeyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan Zhai, Lu Yu, Muhammad Asif Ali, Lijie Hu,\\nand Di Wang.\\nMulti-hop question answering under temporal knowledge editing.\\nCoRR,\\nabs/2404.00492, 2024b.\\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look\\nat? an analysis of bert’s attention. In Proceedings of ACL 2019, pp. 276–286, 2019.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\\nsolve math word problems. CoRR, abs/2110.14168, 2021.\\nRomain Cosentino and Sarath Shekkizhar. Reasoning in large language models: A geometric per-\\nspective. CoRR, abs/2407.02678, 2024.\\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models. CoRR,\\nabs/2208.14271, 2022.\\nDavid Dale, Elena Voita, Loïc Barrault, and Marta R. Costa-jussà. Detecting and mitigating halluci-\\nnations in machine translation: Model internal workings alone do well, sentence similarity even\\nbetter. In Proceedings of ACL 2023, pp. 36–50, 2023.\\nSubhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, and Tanmoy Chakraborty.\\nHow to\\nthink step-by-step:\\nA mechanistic understanding of chain-of-thought reasoning.\\nCoRR,\\nabs/2402.18312, 2024.\\nSiqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and\\nZhongyuan Wang. Not all layers of llms are necessary during inference. CoRR, abs/2403.02181,\\n2024.\\nRuth Fong and Andrea Vedaldi. Net2vec: Quantifying and explaining how concepts are encoded by\\nfilters in deep neural networks. In Proceedings of CVPR 2018, pp. 8730–8738, 2018.\\nRohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts\\nfrom diffusion models. In Proceedings of ICCV 2023, pp. 2426–2436, 2023.\\n12\\nPreprint. Under review.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\\nuse a laptop? A question answering benchmark with implicit reasoning strategies. Trans. Assoc.\\nComput. Linguistics, 9:346–361, 2021.\\nJohn J Hopfield. Neural networks and physical systems with emergent collective computational\\nabilities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.\\nJohn J Hopfield. Neurons with graded response have collective computational properties like those\\nof two-state neurons. Proceedings of the national academy of sciences, 81(10):3088–3092, 1984.\\nJohn J Hopfield and David W Tank. Computing with neural circuits: A model. Science, 233(4764):\\n625–633, 1986.\\nLijie Hu, Ivan Habernal, Lei Shen, and Di Wang. Differentially private natural language models:\\nRecent advances and future directions. CoRR, abs/2301.09112, 2023a.\\nLijie Hu, Yixin Liu, Ninghao Liu, Mengdi Huai, Lichao Sun, and Di Wang. Improving faithfulness\\nfor vision transformers. CoRR, abs/2311.17983, 2023b.\\nLijie Hu, Yixin Liu, Ninghao Liu, Mengdi Huai, Lichao Sun, and Di Wang. SEAT: stable and\\nexplainable attention. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Proceedings\\nof AAAI 2023, pp. 12907–12915, 2023c.\\nLijie Hu, Chenyang Ren, Zhengyu Hu, Cheng-Long Wang, and Di Wang. Editable concept bottle-\\nneck models. CoRR, abs/2405.15476, 2024.\\nSarthak Jain and Byron C. Wallace. Attention is not explanation. In Proceedings of NAACL-HLT\\n2019, pp. 3543–3556, 2019.\\nMingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang,\\nand Mengnan Du.\\nThe impact of reasoning step length on large language models.\\nCoRR,\\nabs/2401.04925, 2024.\\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai, James Wexler, Fernanda B. Viégas,\\nand Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with concept\\nactivation vectors (TCAV). In Proceedings of ICML 2018, pp. 2673–2682, 2018.\\nSeungone Kim, Se June Joo, Yul Jang, Hyungjoo Chae, and Jinyoung Yeo. Cotever: Chain of\\nthought prompting annotation toolkit for explanation verification. In Proceedings of EACL 2023,\\npp. 195–208, 2023.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. In Proceedings of NeurIPS 2022, 2022.\\nSongning Lai, Lijie Hu, Junxiao Wang, Laure Berti-Équille, and Di Wang. Faithful vision-language\\ninterpretation via concept bottleneck models. In Proceedings of ICLR 2024, 2024.\\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Her-\\nnandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness\\nin chain-of-thought reasoning. CoRR, abs/2307.13702, 2023.\\nJia Li, Lijie Hu, Zhixian He, Jingfeng Zhang, Tianhang Zheng, and Di Wang. Text guided image\\nediting with automatic concept locating and forgetting. CoRR, abs/2405.19708, 2024.\\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki,\\nand Chris Callison-Burch. Faithful chain-of-thought reasoning. In Proceedings of IJCNLP 2023,\\npp. 305–329, 2023.\\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.\\nLocating and editing factual\\nassociations in GPT. In Proceedings of NeurIPS 2022, 2022.\\nWilliam Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought.\\nCoRR, abs/2310.07923, 2023.\\n13\\nPreprint. Under review.\\nMeta. Introducing meta llama 3: The most capable openly available llm to date. Meta blog, 2024.\\nTomás Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word\\nrepresentations. In Proceedings of HLT-NAACL 2013, pp. 746–751, 2013.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\\nZettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In\\nProceedings of EMNLP 2022, pp. 11048–11064, 2022.\\nGordon J Mogenson. The neurobiology of Behavior: an introduction. Routledge, 2018.\\nAnh Nguyen, Jason Yosinski, and Jeff Clune. Understanding neural networks via feature visualiza-\\ntion: A survey. In Proceedings of LNCS 2019, pp. 55–76, 2019.\\nAnh Mai Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing\\nthe preferred inputs for neurons in neural networks via deep generator networks. In Proceedings\\nof NeurIPS 2016, pp. 3387–3395, 2016.\\nMaxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:\\nScratchpads for intermediate computation with language models. CoRR, abs/2112.00114, 2021.\\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\\nZoom in: An introduction to circuits. Distill, 5(3):e00024–001, 2020.\\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,\\nBen Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction\\nheads. CoRR, abs/2209.11895, 2022.\\nGuillermo Ortiz-Jiménez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent\\nspace: Improved editing of pre-trained models. In Proceedings of NeurIPS 2023, 2023.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\\nfollow instructions with human feedback. In Proceedings of NeurIPS 2022, 2022.\\nLiangming Pan, Alon Albalak, Xinyi Wang, and William Wang.\\nLogic-lm: Empowering large\\nlanguage models with symbolic solvers for faithful logical reasoning. In Proceedings of EMNLP\\n2023, pp. 3806–3824, 2023.\\nKiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry\\nof large language models. In Proceedings of ICML 2024, 2024.\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple\\nmath word problems? In Proceedings of NAACL-HLT 2021, pp. 2080–2094, 2021.\\nJacob Pfau, William Merrill, and Samuel R. Bowman. Let’s think dot by dot: Hidden computation\\nin transformer language models. CoRR, abs/2404.15758, 2024.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song,\\nJohn Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\\nMethods, analysis & insights from training gopher. CoRR, abs/2112.11446, 2021.\\nDaking Rai and Ziyu Yao. An investigation of neuron activation as a unified lens to explain chain-\\nof-thought eliciting arithmetic reasoning of llms. arXiv preprint arXiv:2406.12288, 2024.\\nShun Shao, Yftah Ziser, and Shay B. Cohen. Gold doesn’t always glitter: Spectral removal of linear\\nand nonlinear guarded attribute information. In Proceedings of EACL 2023, pp. 1603–1614, 2023.\\nCharles Scott Sherrington. Observations on the scratch-reflex in the spinal dog. The Journal of\\nphysiology, 34(1-2):1, 1906.\\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:\\nVisualising image classification models and saliency maps. In Proceedings of ICLR 2014, 2014.\\n14\\nPreprint. Under review.\\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Viégas, and Martin Wattenberg. Smooth-\\ngrad: removing noise by adding noise. CoRR, abs/1706.03825, 2017.\\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-\\nlow, and Rob Fergus. Intriguing properties of neural networks. In Proceedings of ICLR 2014,\\n2014.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge. In Proceedings of NAACL-HLT 2019,\\npp. 4149–4158, 2019.\\nXiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan\\nZhang. Large language models are in-context semantic reasoners rather than symbolic reasoners.\\nCoRR, abs/2305.14825, 2023.\\nAdly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam\\nPearce, Craig Citro, Emmanuel Ameisen, Andy Jones, et al. Scaling monosemanticity: Extracting\\ninterpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024.\\nCurt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear representations of\\nsentiment in large language models. arXiv preprint arXiv:2310.15154, 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\\ntion and fine-tuned chat models. CoRR, abs/2307.09288, 2023.\\nMartina G. Vilas, Federico Adolfi, David Poeppel, and Gemma Roig. Position: An inner inter-\\npretability framework for AI inspired by lessons from cognitive neuroscience. In Proceedings of\\nICML 2024, 2024.\\nElena Voita, Javier Ferrando, and Christoforos Nalmpantis. Neurons in large language models:\\nDead, n-gram, positional. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings\\nof ACL 2024, pp. 1288–1301, 2024.\\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.\\nTowards understanding chain-of-thought prompting: An empirical study of what matters.\\nIn\\nProceedings of ACL 2023, pp. 2717–2739, 2023a.\\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Inter-\\npretability in the wild: a circuit for indirect object identification in GPT-2 small. In Proceedings\\nof ICLR 2023, 2023b.\\nXuezhi Wang and Denny Zhou.\\nChain-of-thought reasoning without prompting.\\nCoRR,\\nabs/2402.10200, 2024.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha\\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\\nmodels. In Proceedings of ICLR 2023, 2023c.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\\nmodels. In Proceedings of NeurIPS 2022, 2022.\\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and\\nJun Zhao. Large language models are better reasoners with self-verification. In Proceedings of\\nEMNLP 2023, pp. 2550–2575, 2023.\\nShu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, and Di Wang. Moral: Moe augmented\\nlora for llms’ lifelong learning. CoRR, abs/2402.11260, 2024a.\\nShu Yang, Lijie Hu, Lu Yu, Muhammad Asif Ali, and Di Wang. Human-ai interactions in the com-\\nmunication era: Autophagy makes large models achieving local optima. CoRR, abs/2402.11271,\\n2024b.\\n15\\nPreprint. Under review.\\nShu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, and\\nDi Wang. Dialectical alignment: Resolving the tension of 3h and security threats of llms. CoRR,\\nabs/2404.00486, 2024c.\\nZiyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui\\nWang. A survey on language models for code. CoRR, abs/2311.07989, 2023.\\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. Verify-and-edit: A\\nknowledge-enhanced chain-of-thought framework. In Proceedings of ACL 2023, pp. 5823–5840,\\n2023.\\nZexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, and Danqi Chen.\\nMquake: Assessing knowledge editing in language models via multi-hop questions.\\nIn Pro-\\nceedings of EMNLP 2023, pp. 15686–15702, 2023.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schu-\\nurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting\\nenables complex reasoning in large language models. In Proceedings of ICLR 2023, 2023.\\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan,\\nXuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A\\ntop-down approach to AI transparency. CoRR, abs/2310.01405, 2023.\\n16\\nPreprint. Under review.\\nA\\nADDITIONAL PRELIMINARIES\\nThe Sherringtonian View. Unlike the Hopfieldian perspective, the Sherringtonian view (Sherring-\\nton, 1906; Barlow, 1953) of cognitive explanation emphasizes the importance of direct neuron-to-\\nneuron connections in the brain. This view posits that the primary explanation for cognition lies\\nin the specific interactions between neurons and the computations these neurons perform within\\nwell-defined circuits (Mogenson, 2018).\\nAt an algorithmic level, the Sherringtonian view conceptualizes cognition as networks of nodes\\n(neurons) with weighted connections (synapses) between them. In this framework, neurons perform\\ndistinct computational transformations on the signals they receive from other neurons in the network.\\nCognitive processes are described by how individual neurons receive inputs, process these inputs\\nthrough neural transfer functions, and transmit the resulting signals to connected neurons. Thus,\\ncognition is explained through the computations occurring at the level of individual neurons and the\\nsignal flow across their connections.\\nZero-shot CoT.\\nZero-shot CoT is a simple but effective chain of thought (CoT) prompting ap-\\nproach proposed by Kojima et al. (2022). It allows language models to generate a step-by-step\\nexplanation or thought process to solve problems without requiring prior demonstrations or specific\\ntraining by simply adding “Let’s think step by step” before each answer. Specifically, given a query\\nq and a model M with weights θ, the generation process can be defined as:\\nR = arg max pr(R|q, t; θ)\\n(5)\\nwhere R is the response text of the model, which is generated from all potential responses R, and t\\nis a prompt text like “Let’s think sttep by step.”\\nFew-shot CoT.\\nSimilar to zero-shot CoT, few-shot CoT (Wei et al., 2022) is also a prompting tech-\\nnique that gives a few examples with step-by-step reasoning processes to stimulate the model’s rea-\\nsoning ability. Formally, given the query q and a set of demonstrations D = {(˜q1, ˜a1), (˜q2, ˜a2), . . . },\\nour aim is to generate a response with intermediate reasoning steps:\\nR = arg max pr(R|D, q; θ)\\n(6)\\nTable 4: Statistics of the data set.\\nDataset\\nTask Domain\\n# Samples\\nAnswer Format\\nGSM8K\\nArithmetic\\n1319\\nNumber\\nSVAMP\\nArithmetic\\n300\\nNumber\\nStrategyQA\\nCommonsense\\n2290\\nYes or No\\nCSQA\\nCommonsense\\n1221\\nMultiple Choices\\nCoin Flip\\nSymbolic\\n2000\\nYes or No\\nRandom Letter\\nSymbolic\\n300\\nLetter\\nB\\nOTHER EXPERIMENTAL DETAILS\\nB.1\\nDATASET\\nThe statistics of the data is shown in Table 4. The details about each data set are as follows:\\nArithmetic Reasoning.\\nThe arithmetic reasoning benchmarks aim to analyze and/or understand\\nthe model’s mathematical reasoning skills. These include: (i) GSM8K Cobbe et al. (2021), a math\\nword problems benchmark encompassing a variety of reasoning steps; (ii) SVAMP Patel et al.\\n(2021), containing math word problems with multiple structures.\\nCommonsense Reasoning.\\nThese data sets aim to analyze the ability of the model on common-\\nsense reasoning tasks. These include: (i) StrategyQA Geva et al. (2021), a commonsense benchmark\\nrequiring multi-level strategy to answer the question; (ii) CSQA Talmor et al. (2019) benchmark\\ndataset of multiple-choice questions that require different types of commonsense knowledge to pre-\\ndict the correct answers.\\n17\\nPreprint. Under review.\\nSymbolic Reasoning.\\nThese data sets aim to test the abilities of the model requiring advanced\\nsymbolic capabilities. For this task, we curated two new datasets, as follows. (i) Coin Flip dataset,\\nwe employ the data curation strategy of a previous study Wei et al. (2022) using the number of oper-\\nations as 2, 4 and 7 to come up with the complete dataset; (ii) Random Letter, an advanced version\\nof the last letter concatenation with reference to the previously studied form of word assembly Wei\\net al. (2022), where 2-4 words are randomly formed and characters are randomly drawn from them,\\ninstead of taking the beginning or the end of each word at a fixed point.\\nTable 5: The detailed robust results in the zero-shot settings.\\nMethod\\nArithmetic\\nCommonsense\\nSymbolic\\nGSM8K\\nSVAMP\\nCSQA\\nStrategyQA\\nCoin Flip\\nRandom Letter\\nLlama-2-7B-Chat\\nCoTZ1\\n26.31\\n46.00\\n43.41\\n62.05\\n52.75\\n24.33\\nCoTZ2\\n26.23\\n48.33\\n43.90\\n60.52\\n48.35\\n17.67\\nCoTZ3\\n23.58\\n51.67\\n46.68\\n63.54\\n50.10\\n17.33\\nRoTZ1\\n26.23\\n54.33\\n48.24\\n63.54\\n45.45\\n20.67\\nRoTZ2\\n24.72\\n53.67\\n47.91\\n63.58\\n45.50\\n20.67\\nRoTZ3\\n25.09\\n53.67\\n47.42\\n63.23\\n45.35\\n20.33\\nLlama-3-8B-Instruct\\nCoTZ1\\n74.45\\n82.33\\n72.24\\n66.07\\n90.45\\n43.00\\nCoTZ2\\n74.83\\n83.33\\n72.65\\n63.32\\n83.55\\n42.00\\nCoTZ3\\n58.15\\n40.67\\n73.71\\n63.10\\n89.40\\n33.67\\nRoTZ1\\n74.83\\n83.33\\n72.89\\n65.24\\n76.35\\n47.67\\nRoTZ2\\n74.91\\n83.33\\n72.73\\n64.93\\n71.95\\n46.67\\nRoTZ3\\n73.62\\n82.00\\n72.48\\n65.50\\n68.15\\n42.00\\nTable 6: The detailed robust results in the few-shot settings.\\nMethod\\nArithmetic\\nCommonsense\\nSymbolic\\nGSM8K\\nSVAMP\\nCSQA\\nStrategyQA\\nCoin Flip\\nRandom Letter\\nLlama-2-7B-Chat\\nCoTF 1\\n4.62\\n38.67\\n53.07\\n59.26\\n47.60\\n31.00\\nCoTF 2\\n3.18\\n38.67\\n50.29\\n59.74\\n50.30\\n29.00\\nRoTF 1\\n25.55\\n56.00\\n48.16\\n63.80\\n45.50\\n20.33\\nRoTF 2\\n25.63\\n55.33\\n48.16\\n65.68\\n45.50\\n20.33\\nLlama-3-8B-Instruct\\nCoTF 1\\n72.02\\n81.00\\n73.63\\n62.75\\n96.50\\n50.67\\nCoTF 2\\n72.25\\n80.67\\n72.89\\n67.47\\n95.00\\n50.67\\nRoTF 1\\n74.37\\n83.67\\n73.30\\n65.94\\n70.30\\n43.67\\nRoTF 2\\n74.00\\n83.33\\n73.63\\n65.68\\n69.85\\n42.67\\nB.2\\nDETAILS OF ROBUST EXPERIMENT\\nWe show our detailed results of the robustness in Table 5 and 6. For zero-shot settings, the terms\\nZ1, Z2, and Z3 refer to the use of three different prompts, respectively (as shown in Section 6). For\\nfew-shot settings, the terms F1 and F2 also refer to two different experiments, as shown in Section\\n6.\\nC\\nPROMPTS\\nC.1\\nPROMPT TEMPLATES\\nTable 7 illustrates the design of stimulus prompts utilized for Chain of Thought (CoT) prompting,\\ndistinguishing between zero-shot CoT and few-shot CoT methodologies. In the zero-shot CoT ap-\\nproach, the model is presented with a question devoid of preceding examples, in contrast to the\\nfew-shot CoT method, where the model is furnished with multiple exemplars. For each method, the\\nfirst row is a positive prompt and the second is a negative prompt. Red indicates stimulus token.\\n18\\nPreprint. Under review.\\nTable 7: The stimulus prompting design for CoT-style methods.\\nzero-shot CoT\\nUSER: <question>\\nASSISTANT: Let’s think step by step.\\nUSER: <question>\\nASSISTANT:\\nfew-shot CoT\\nUSER: <n different examples>\\n<question>\\nASSISTANT:\\nUSER: <question>\\nASSISTANT:\\nTable 8: Extraction templates for various tasks.\\nTask\\nExtraction Template\\nStrategyQA\\nTherefore, the answer (Yes or No) is\\nCSQA\\nTherefore, among A through E, the answer is\\nCoin Flip\\nTherefore, the answer (Yes or No) is\\nRandom Letter\\nTherefore, the answer is\\nC.2\\nANSWER EXTRACT PROMPTS\\nThe demonstration of our answer extraction method is in Table 8.\\nC.3\\nEXAMPLE PROMPTS FOR FEW-SHOT SETTING\\nWe demonstrate our example prompts for few-shot setting in Table 9, 10, 11, 12 and 13.\\nD\\nCASE DEMONSTRATIONS\\nWe show our additional cases on arithmetic, commonsense, and symbolic reasoning tasks in Fig-\\nure 4, 5, 6, and 7.\\n19\\nPreprint. Under review.\\nTable 9: Few-shot exemplars for the full chain of thought prompt for the arithmetic reasoning task.\\nThe highlighted part refers to the reasoning steps.\\nPROMPT FOR GSM8K AND SVAMP\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today.\\nAfter they are done, there will be 21 trees. How many trees did the grove workers plant\\ntoday?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted.\\nSo there must have been 21 - 15 = 6. The answer is 6.\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the\\nparking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they\\nhave left in total?\\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 =\\n74. After eating 35, they had 74 - 35 = 39. The answer is 39.\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops.\\nHow many lollipops did Jason give to Denny?\\nA: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he\\ngave Denny 20 - 12 = 8. The answer is 8.\\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad.\\nHow many toys does he have now?\\nA: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4\\nmore toys. 5 + 4 = 9. The answer is 9.\\nQ: There were nine computers in the server room. Five more computers were installed\\neach day, from Monday to Thursday. How many computers are now in the server room?\\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added.\\nSo 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\\nQ: Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost\\n2 more. How many golf balls did he have at the end of Wednesday?\\nA: Michael started with 58 golf balls. After losing 23 on Tuesday, he had 58 - 23 = 35.\\nAfter losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have\\nleft?\\nA: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has\\n23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\\n20\\nPreprint. Under review.\\nTable 10: Few-shot exemplars for the full chain of thought prompt for StrategyQA. The highlighted\\npart refers to the reasoning steps.\\nPROMPT FOR STRATEGYQA\\nQ: Do hamsters provide food for any animals?\\nA: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food\\nfor some animals. So the answer is yes.\\nQ: Could Brooke Shields succeed at University of Pennsylvania?\\nA: Brooke Shields went to Princeton University. Princeton University is about as aca-\\ndemically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also\\nsucceed at the University of Pennsylvania. So the answer is yes.\\nQ: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\\nA: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus,\\nHydrogen’s atomic number squared is less than 5. So the answer is no.\\nQ: Yes or no: Is it common to see frost during some college commencements?\\nA: College commencement ceremonies can happen in December, May, and June. De-\\ncember is in the winter, so there can be frost. Thus, there could be frost at some com-\\nmencements. So the answer is yes.\\nQ: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\\nA: The War in Vietnam was 6 months. The gestation period for a llama is 11 months,\\nwhich is more than 6 months. Thus, a llama could not give birth twice during the War in\\nVietnam. So the answer is no.\\nQ: Yes or no: Would a pear sink in water?\\nA: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense\\nthan water float. Thus, a pear would float. So the answer is no.\\n21\\nPreprint. Under review.\\nTable 11: Few-shot exemplars for the full chain of thought prompt for CSQA. The highlighted part\\nrefers to the reasoning steps. There are newlines between the answer choices that are omitted in the\\ntable for space reasons.\\nPROMPT FOR CSQA\\nQ: What do people use to absorb extra ink from a fountain pen? Answer Choices: (a)\\nshirt pocket (b) calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\\nA: The answer must be an item that can absorb ink. Of the above choices, only blotters\\nare used to absorb ink. So the answer is (e).\\nQ: What home entertainment equipment requires cable?\\nAnswer Choices: (a) radio shack (b) substation (c) television (d) cabinet\\nA: The answer must require cable. Of the above choices, only television requires cable.\\nSo the answer is (c).\\nQ: The fox walked from the city into the forest, what was it looking for? Answer Choices:\\n(a) pretty flowers (b) hen house (c) natural habitat (d) storybook\\nA: The answer must be something in the forest. Of the above choices, only natural habitat\\nis in the forest. So the answer is (b).\\nQ: Sammy wanted to go to where the people were. Where might he go? Answer Choices:\\n(a) populated areas (b) race track (c) desert (d) apartment (e) roadblock\\nA: The answer must be a place with a lot of people. Of the above choices, only populated\\nareas have a lot of people. So the answer is (a).\\nQ: Where do you put your grapes just before checking out? Answer Choices: (a) mouth\\n(b) grocery cart (c)super market (d) fruit basket (e) fruit market\\nA: The answer should be the place where grocery items are placed before checking out.\\nOf the above choices, grocery cart makes the most sense for holding grocery items. So\\nthe answer is (b).\\nQ: Google Maps and other highway and street GPS services have replaced what? Answer\\nChoices: (a) united states (b) mexico (c) countryside (d) atlas\\nA: The answer must be something that used to do what Google Maps and GPS services\\ndo, which is to give directions. Of the above choices, only atlases are used to give direc-\\ntions. So the answer is (d).\\nQ: Before getting a divorce, what did the wife feel who was doing all the work? Answer\\nChoices: (a) harder (b) anguish (c) bitterness (d) tears (e) sadness\\nA: The answer should be the feeling of someone getting divorced who was doing all the\\nwork. Of the above choices, the closest feeling is bitterness. So the answer is (c).\\n22\\nPreprint. Under review.\\nTable 12: Few-shot exemplars for the full chain of thought prompt for the random letter concatena-\\ntion task. The highlighted part refers to the reasoning steps.\\nPROMPT FOR RONDOM LETTER CONCATENATION\\nQ: Take the first letters of the words in \"James Hickman \" and concatenate them.\\nA: The first letter of \"James\" is \"J\". The first letter of \"Hickman\" is \"H\". Concatenating\\nthem is \"JH\". The answer is JH.\\nQ: Take the second letters of the words in \"Carl Mccall Bonilla \" and concatenate them.\\nA: The second letter of \"Carl\" is \"a\". The second letter of \"Mccall\" is \"c\". The second\\nletter of \"Bonilla\" is \"o\" Concatenating them is \"aco\". The answer is aco.\\nQ: Take the third letters of the words in \"Randy Tanner \" and concatenate them.\\nA: The third letter of \"Randy\" is \"n\". The third letter of \"Tanner\" is \"n\". Concatenating\\nthem is \"nn\". The answer is nn.\\nQ: Take the first letters of the words in \"Kenny Kim \" and concatenate them.\\nA: The first letter of \"Kenny\" is \"K\". The first letter of \"Kim\" is \"K\". Concatenating them\\nis \"KK\". The answer is KK.\\n23\\nPreprint. Under review.\\nTable 13: Few-shot exemplars for the full chain of thought prompt for the coin flip task. The\\nhighlighted part refers to the reasoning steps.\\nPROMPT FOR COIN FLIP\\nQ: A coin is heads up. Ka flips the coin. Sherrie flips the coin. Is the coin still heads up?\\nA: The coin was flipped by Ka and Sherrie. So the coin was flipped 2 times, which is an\\neven number. The coin started heads up, so after an even number of flips, it will still be\\nheads up. So the answer is yes.\\nQ: A coin is heads up. Jamey flips the coin. Teressa flips the coin. Is the coin still heads\\nup?\\nA: The coin was flipped by Jamey and Teressa. So the coin was flipped 2 times, which is\\nan even number. The coin started heads up, so after an even number of flips, it will still\\nbe heads up. So the answer is yes.\\nQ: A coin is heads up. Maybelle flips the coin. Shalonda does not flip the coin. Is the\\ncoin still heads up?\\nA: The coin was flipped by Maybelle. So the coin was flipped 1 time, which is an odd\\nnumber. The coin started heads up, so after an odd number of flips, it will be tails up. So\\nthe answer is no.\\nQ: A coin is heads up. Millicent does not flip the coin. Conception flips the coin. Is the\\ncoin still heads up?\\nA: The coin was flipped by Conception. So the coin was flipped 1 time, which is an odd\\nnumber. The coin started heads up, so after an odd number of flips, it will be tails up. So\\nthe answer is no.\\nQ: A coin is heads up. Sal flips the coin. Raymond does not flip the coin. Is the coin still\\nheads up?\\nA: The coin was flipped by Sal. So the coin was flipped 1 time, which is an odd num-\\nber. The coin started heads up, so after an odd number of flips, it will be tails up. So the\\nanswer is no.\\nQ: A coin is heads up. Conception flips the coin. Kristian does not flip the coin. Is the\\ncoin still heads up?\\nA: The coin was flipped by Conception. So the coin was flipped 1 time, which is an odd\\nnumber. The coin started heads up, so after an odd number of flips, it will be tails up. So\\nthe answer is no.\\nQ: A coin is heads up. Inga does not flip the coin. Elanor does not flip the coin. Is the\\ncoin still heads up?\\nA: The coin was flipped by no one. So the coin was flipped 0 times. The coin started\\nheads up, and it was not flipped, so it is still heads up. So the answer is yes.\\nQ: A coin is heads up. Ryan flips the coin. Shaunda flips the coin. Is the coin still heads\\nup?\\nA: The coin was flipped by Ryan and Shaunda. So the coin was flipped 2 times, which is\\nan even number. The coin started heads up, so after an even number of flips, it will still\\nbe heads up. So the answer is yes.\\n24\\nPreprint. Under review.\\nTo find the total number of bolts needed for the robe, we need to calculate the total\\namount of blue fiber and white fiber required, and then divide that by the amount of\\nblue fiber required per bolt.\\xa0The problem states that a robe takes 2 bolts of blue fiber\\nand half that much white fiber.\\xa0\\nSo, the total amount of blue fiber required is 2 bolts x 1 bolt = 2 bolts.\\nThe total amount of white fiber required is half of 2 bolts, which is 1 bolt.\\nTherefore, the total number of bolts needed for the robe is 2 + 1 = 3 bolts.\\nQ: A robe takes 2 bolts of blue fiber and half that much white fiber.\\xa0 How many bolts in\\ntotal does it take?\\nTo find the total number of bolts needed for the robe, we need to calculate the total\\namount of blue fiber and white fiber required, and then divide that by the amount of\\nblue fiber required per bolt.\\xa0The problem states that a robe takes 2 bolts of blue fiber\\nand half that much white fiber.\\xa0\\nTo find the total amount of blue fiber required, we can multiply the number of bolts by\\nthe amount of blue fiber per bolt:\\n2 bolts x 2 = 4 bolts of blue fiber\\nTo find the total amount of white fiber required, we can multiply the number of bolts by\\nthe amount of white fiber per bolt:\\n2 bolts x 0.5 = 1 bolt of white fiber\\nNow, we need to divide the total amount of blue fiber by the amount of blue fiber\\nrequired per bolt to find the total number of bolts needed:\\n4 bolts of blue fiber / 2 = 2 bolts of blue fiber per bolt\\nTherefore, it takes 2 bolts of blue fiber and 1 bolt of white fiber to make the robe.\\n+ Control\\nFigure 4: A real case predicted by Llama-2-7B-Chat with zero-shot CoT on the GSM8K dataset.\\nThe segment highlighted in blue represents the correct output of the model. The red part shows that\\nthe model starts to reason in the wrong direction without control, while the green portion indicates\\nthe model reason in the correct direction after adding control.\\n25\\nPreprint. Under review.\\nQ:\\xa0There are 15 trees in the grove. Grove workers will plant trees in the grove today.\\nAfter they are done, there will be 21 trees. How many trees did the grove workers plant\\ntoday?\\nA:\\xa0 There are 15 trees originally. Then there were 21 trees after some more were\\nplanted. So there must have been 21 - 15 = 6.\\nQ:\\xa0If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in\\nthe parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.}\\xa0 The answer is 5.\\n...\\nQ:\\xa0 Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but\\n40% of the way through the download, Windows forces a restart to install updates,\\nwhich takes 20 minutes. Then Carla has to restart the download from the beginning.\\nHow load does it take to download the file?\\nLet\\'s break this problem down step by step!\\nInitially, Carla can download 2 GB/minute. To find out how many minutes it would take\\nto download the entire file, we can divide the file size (200 GB) by the download rate (2\\nGB/minute):\\n200 GB / 2 GB/minute = 100 minutes\\nHowever, Windows forces a restart after 40% of the download is complete. To find out\\nhow many GB are left to download, we can calculate 40% of the total file size:\\n40% of 200 GB = 0.4 x 200 GB = 80 GB\\nSo, Carla has to restart the download from the beginning, and she has 80 GB left to\\ndownload. To find out how many minutes it will take to download the remaining 80 GB,\\nwe can divide the remaining file size by the download rate:\\n80 GB / 2 GB/minute = 40 minutes.\\nAdding the initial 100 minutes to the additional 40 minutes, Carla will need a total of:\\n100 minutes + 40 minutes = 140 minutes to download the entire file.\\nLet\\'s break it down step by step:\\n1. Carla can download 2 GB/minute, so it would normally take her:\\n200 GB / 2 GB/minute = 100 minutes to download the file.\\n2. But, 40% of the way through the download, Windows forces a restart. This means\\nshe has downloaded:\\n40% of 200 GB = 80 GB at that point. This means she still has: 200 GB - 80 GB = 120\\nGB left to download.\\n3. The restart takes 20 minutes, so Carla loses 20 minutes of download time.\\n4. After the restart, Carla has to start the download from the beginning, which means\\nshe has to re-download the 80 GB she already had. This adds another 80 GB / 2\\nGB/minute = 40 minutes to the total download time.\\nTherefore, the total download time is:\\xa0\\n100 minutes (original download time) + 20 minutes (restart time) + 40 minutes (re-\\ndownloading already downloaded portion) = 160 minutes.\\n+ Control\\nFigure 5: A real case predicted by Llama-3-8B-Instruct with few-shot CoT on the GSM8K dataset.\\nThe purple part is an example of input-output pairs given by user. The segment highlighted in blue\\nrepresents the correct output of the model. The red part shows that the model starts to reason in the\\nwrong direction without control, while the green portion indicates the model reason in the correct\\ndirection after adding control.\\n26\\nPreprint. Under review.\\nQ: Do hamsters provide food for any animals?\\nA:\\xa0Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food\\nfor some animals. So the answer is yes.\\nQ:\\xa0Could Brooke Shields succeed at University of Pennsylvania?\\nA:\\xa0 Brooke Shields went to Princeton University. Princeton University is about as\\nacademically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also\\nsucceed at the University of Pennsylvania. So the answer is yes.\\n...\\nQ:\\xa0Is a platypus immune from cholera?\\nI think I can help you with that!\\xa0\\xa0\\nA: Platypuses are mammals.\\nCholera is a bacterial infection that can affect many animals, including mammals.\\xa0\\nPlatypuses do not have a natural immunity to cholera.\\xa0 So, the answer is no.\\nAccording to scientific research, platypuses are resistant to cholera.\\nThis is because they have a unique immune system that is adapted to their aquatic\\nenvironment.\\xa0Platypuses have a low body temperature, which makes it difficult for many\\npathogens, including cholera, to replicate and cause disease.\\xa0\\nAdditionally, their immune system is highly effective at recognizing and eliminating\\npathogens, and they have a strong response to bacterial infections.\\nIn fact, studies have shown that platypuses are resistant to a range of bacterial\\ninfections, including cholera, and that they are able to recover quickly from infections\\nthat would be fatal to other animals.\\nSo, to answer your question, yes, platypuses are generally immune from cholera!\\n+ Control\\nFigure 6: A real case predicted by Llama-3-8B-Instruct with few-shot CoT on the strategyQA\\ndataset. The purple part is an example of input-output pairs given by user. The segment high-\\nlighted in blue represents the correct output of the model. The red part shows that the model starts\\nto reason in the wrong direction without control, while the green portion indicates the model reason\\nin the correct direction after adding control.\\n27\\nPreprint. Under review.\\nQ:\\xa0A coin is heads up. Ka flips the coin. Sherrie flips the coin. Is the coin still heads up?\\nA:\\xa0The coin was flipped by Ka and Sherrie. So the coin was flipped 2 times, which is an\\neven number. The coin started heads up, so after an even number of flips, it will still be\\nheads up. So the answer is yes.\\nQ:\\xa0A coin is heads up. Jamey flips the coin. Teressa flips the coin. Is the coin still heads\\nup?\\nA:\\xa0The coin was flipped by Jamey and Teressa. So the coin was flipped 2 times, which is\\nan even number. The coin started heads up, so after an even number of flips, it will still\\nbe heads up. So the answer is yes.\\n...\\nQ: A coin is heads up. verley flips the coin. nytavious does not flip the coin. alsiha does\\nnot flip the coin. leroyal does not flip the coin.\\xa0 Is the coin still heads up?\\nThe coin was flipped by Verley. So the coin was flipped 1 time, which is an odd number.\\nThe coin started heads up, so after an odd number of flips, it will be tails up.\\xa0\\nHowever, no one else flipped the coin, so the coin was not flipped again. Since the coin\\nwas flipped an odd number of times initially, it will still be heads up. So the answer is\\nyes.\\nThe only person who flipped the coin is Verley. Since the coin was initially heads up,\\nVerley\\'s flip would change the side of the coin.\\xa0\\nTherefore, the coin is no longer heads up.\\n+ Control\\nFigure 7: A real case predicted by Llama-3-8B-Instruct with few-shot CoT on the coin flip dataset.\\nThe purple part is an example of input-output pairs given by user. The segment highlighted in blue\\nrepresents the correct output of the model. The red part shows that the model starts to reason in the\\nwrong direction without control, while the green portion indicates the model reason in the correct\\ndirection after adding control.\\n28\\n'),\n",
       " Document(metadata={'Published': '2023-05-26', 'Title': \"Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance\", 'Authors': 'Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot', 'Summary': 'As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.', 'entry_id': 'http://arxiv.org/abs/2305.17306v1', 'published_first_time': '2023-05-26', 'comment': 'Preprint. Code at https://github.com/FranxYao/chain-of-thought-hub', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.LG'], 'links': ['https://arxiv.org/abs/2305.17306v1', 'https://arxiv.org/pdf/2305.17306v1']}, page_content='Chain-of-Thought Hub: A Continuous Effort to\\nMeasure Large Language Models’ Reasoning Performance\\nYao Fu♠Litu Ou♠Mingyu Chen♠Yuhao Wan♦Hao Peng♣Tushar Khot♣\\n♠University of Edinburgh\\n♦University of Washington\\n♣Allen Institute for AI\\n{yao.fu, s1970716, s2331360}@ed.ac.uk\\nyuhaowan@cs.washington.edu\\n{haop, tushark}@allenai.org\\nhttps://github.com/FranxYao/chain-of-thought-hub\\nAbstract\\nAs large language models (LLMs) are continu-\\nously being developed, their evaluation becomes\\nincreasingly important yet challenging. This work\\nproposes Chain-of-Thought Hub, an open-source\\nevaluation suite on the multi-step reasoning ca-\\npabilities of large language models. We are in-\\nterested in this setting for two reasons: (1) from\\nthe behavior of GPT and PaLM model family, we\\nobserve that complex reasoning is likely to be a\\nkey differentiator between weaker and stronger\\nLLMs; (2) we envisage large language models to\\nbecome the next-generation computational plat-\\nform and foster an ecosystem of LLM-based new\\napplications, this naturally requires the founda-\\ntion models to perform complex tasks that often\\ninvolve the composition of linguistic and logical\\noperations. Our approach is to compile a suite\\nof challenging reasoning benchmarks to track the\\nprogress of LLMs. Our current results show that:\\n(1) model scale clearly correlates with reasoning\\ncapabilities; (2) As of May 2023, Claude-v1.3 and\\nPaLM-2 are the only two models that are compara-\\nble with GPT-4, while open-sourced models still\\nlag behind; (3) LLaMA-65B performs closely to\\ncode-davinci-002, indicating that with successful\\nfurther development such as reinforcement learn-\\ning from human feedback (RLHF), it has great\\npotential to be close to GPT-3.5-Turbo. Our re-\\nsults also suggest that for the open-source efforts\\nto catch up, the community may focus more on\\nbuilding better base models and exploring RLHF.\\n1. Introduction\\nRecently, the field of AI has been significantly impressed\\nby the advances in large language models. LLMs exhibit\\nmulti-dimensional capabilities, and their evaluation is chal-\\nlenging. Generally, tuning a base language model into a\\nchatbot is relatively easy, as demonstrated by the large va-\\nriety of LLaMA-based (Touvron et al., 2023) models like\\nAlpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023),\\nKoala (Geng et al., 2023), Dolly (Databricks, 2023), and so\\non. In a chitchat, all these models may perform superficially\\nsimilarly to GPT-3.5-Turbo (Gudibande et al., 2023). At the\\ncurrent stage, the community is eager to know what are the\\nkey factors that clearly differentiate the better-performing\\nmodels from the underperforming ones.\\nIn this work, we consider the evaluation of complex reason-\\ning. As noted by OpenAI (2023b), “In a casual conversation,\\nthe distinction between GPT-3.5 and GPT-4 can be subtle.\\nThe difference comes out when the complexity of the task\\nreaches a sufficient threshold”. A similar observation is\\nmade by the Google PaLM model family, as their develop-\\ners discover that large models’ chain-of-thought reasoning\\ncapability is clearly stronger than smaller models (Wei et al.,\\n2022b;a). These observations indicate that the ability to\\nperform complex tasks is a key metric.\\nThe capability of performing complex reasoning is crucial\\nfor the models to become the next-generation computation\\nplatform. One example initiative is LangChain1 where devel-\\nopers build applications powered by backend LLM engines,\\nwhich generally require the model to perform complex tasks.\\nHere the vision of pushing LLMs as the foundation of a new\\ncomputational ecosystem also serves as a strong motivation\\nto measure the models’ reasoning performance.\\nTo incentivize the research efforts in improving language\\nmodels’ reasoning performance, we propose the chain-of-\\nthought hub (CoT Hub), a continuous open-source effort\\nthat tracks LLMs’ reasoning capability using a carefully cu-\\nrated evaluation suite. CoT Hub is the first comprehensive\\ncomparison of very large LMs on reasoning benchmarks\\nand currently consists of 19 major language models’ (in-\\ncluding the GPT, Claude, PaLM and LLaMA) performance\\non 6 benchmarks and more than 100 subtasks (including\\n1https://github.com/hwchase17/langchain\\n1\\narXiv:2305.17306v1  [cs.CL]  26 May 2023\\nChain-of-Thought Hub\\nbi-lingual reasoning capabilities in Chinese), and we are\\ncontinuously adding new models and datasets.\\nObservations made in CoT Hub shed light on many insights\\ninto LLM development: (1) the reasoning performance of\\nLLMs highly correlates with models’ scales; (2) as of May\\n2023, PaLM and Claude2 are the only two model fami-\\nlies that are comparable to (yet slightly worse than) the\\nGPT model family; (2) LLaMA 65B (Touvron et al., 2023)\\nthe strongest open LLM to date, performs closely to code-\\ndavinci-002, the base model of GPT-3.5 family3. This indi-\\ncates that if aligned properly (by doing supervised finetun-\\ning (SFT) and reinforcement learning from human feedback\\n(RLHF) right) LLaMA 65B can potentially improve fur-\\nther and perform on par with ChatGPT-3.5. We hope\\nour work gives meaningful guidance to the community’s\\ndevelopment of deployable LLMs.\\n2. Method\\nIn this section we discuss the construction of Chain-of-\\nThought Hub. We first discuss our method for test data\\ncollection, then we discuss how we obtain the model per-\\nformance on our test suite. Our main goal is to curate a\\nhigh-quality collection of datasets that (1) is closely related\\nto the actual usage of LLMs; (2) clearly differentiate the\\nperformance of stronger and weaker language models. We\\nconsider the following datasets:\\nGSM8k A widely used math reasoning datasets consist-\\ning of 8k problems that jointly test models’ ability of\\narithmetic reasoning and composing math steps using\\nlanguage (Cobbe et al., 2021).\\nMATH A suite of challenging datasets consisting of 12k\\nproblems within 7 categories testing the models’ ad-\\nvanced math and science reasoning. The problems in\\nthis dataset are very hard because they come from math-\\nematics competitions written in Latex. Even GPT-4\\nhas only 42.5% performance (Hendrycks et al., 2021).\\nMMLU An evaluation suite of 15k problems within 57\\nsubjects testing model’s high-school and college-level\\nknowledge and reasoning (Hendrycks et al., 2020).\\nBigBench Hard A suite of language and symbolic rea-\\nsoning tasks consisting 6.5k problems within 23 sub-\\nsets that are particularly suitable for testing chain-of-\\nthought prompting (Suzgun et al., 2022).\\nHumanEval A handwritten dataset of 164 Python program-\\nming problems with text comments and docstrings test-\\ning the models’ coding ability (Chen et al., 2021).\\n2https://www.anthropic.com/index/introducing-claude\\n3https://platform.openai.com/docs/model-index-for-\\nresearchers\\nC-Eval A Chinese evaluation suite for foundation models\\nconsisting of 13k multi-choice questions spanning 52\\ndiverse disciplines and four difficulty levels (Huang\\net al., 2023).\\nWe note that most of these datasets are already used in\\nthe evaluation of leading large language models, such as\\nGPT-4 (OpenAI, 2023a) and PaLM-2 (Anil et al., 2023).\\nFew-Shot Chain-of-thought Prompting\\nWe use few-\\nshot chain-of-thought prompting to evaluate LLMs. This\\nmarks a clear difference between our evaluation and the\\nmajority of other concurrent evaluations like HeLM (Liang\\net al., 2022), as most of them use answer-only prompting.\\nWe also emphasize that we use few-shot, rather than zero-\\nshot prompting, because few-shot is a capability that exist\\nin both pretrained and instruction-tuned checkpoints, v.s.,\\nzero-shot is more suitable for instruction-tuned checkpoints\\nand may under-estimate the pretrained checkpoints.\\nComparison to existing and concurrent work\\nThere\\nare many great existing evaluation suites for large language\\nmodels, such as HeLM, Chatbot Arena4, and Open LLM\\nLeaderboard5. The major difference between this work\\nand these works are: (1) HeLM evaluates a significantly\\nwider spectrum of tasks while we focus on evaluating rea-\\nsoning. Most of the results from this work use chain-of-\\nthought prompting (hence the name “Chain-of-Thought\\nHub”) whereas HeLM mainly uses answer-only prompting\\n(without CoT). (2) Chatbot Arena evaluate the dialog user\\npreference we evaluate reasoning. (3) Open LLM Leader-\\nboard focus on open-sourced LLMs, we jointly consider\\nmajor LLMs, either open-sourced or not.\\nUsing final answer accuracy as a proxy for reasoning\\ncapability\\nMost of the datasets we consider share one\\npattern: to reach the final answer (either a number for math\\nproblems, a choice for multi-choice problems, or a fixed\\noutput for coding), the model needs to figure out the inter-\\nmediate steps toward that answer. When evaluating, we only\\nuse the final answer accuracy but do not consider the correct-\\nness of intermediate steps. This is because empirically, the\\ncorrectness of intermediate steps is strongly correlated with\\nthe final accuracy. If the intermediate steps are very wrong,\\nthe model is less likely to reach the final answer. If the final\\nanswer is correct, the intermediate steps are generally good\\nenough (Wei et al., 2022b; Lewkowycz et al., 2022).\\n3. Experiments\\nFirst we discuss the model families we consider. We focus\\non the popular models in production, including GPT, Claude,\\nPaLM, LLaMA, and T5 model families, specifically:\\n4https://leaderboard.lmsys.org/\\n5https://huggingface.co/spaces/HuggingFaceH4/open llm leaderboard\\n2\\nChain-of-Thought Hub\\nTable 1. Overall model performance on Chain-of-Thought Hub. Numbers with an asterisk* are from our test scripts. For model types,\\nbase means the model checkpoint after pretraining, SIFT means supervised instruction finetuning. Others are from their corresponding\\npapers. We observe: (1) there exist a gap between leading LLMs (GPT, Claude and PaLM) and open-source (LLaMA and FlanT5);\\n(2) most leading LLMs are after RLHF, indicating the opportunity of improving open-sourced models using this technique; (3). model\\nperformance is generally correlated with model scale, indicating further opportunities in scaling, especially for open-source models. We\\nfurther highlight that among open-sourced models, LLaMA 65B performs close to code-davinci-002, the base model of ChatGPT. This\\nsuggests that if RLHF is done right on LLaMA 65B, it may become close to ChatGPT.\\nModel\\n#Params\\nType\\nGSM8k\\nMATH\\nMMLU\\nBBH\\nHumanEval\\nC-Eval\\nGPT-4\\n?\\nRLHF\\n92.0\\n42.5\\n86.4\\n-\\n67.0\\n68.7*\\nclaude-v1.3\\n?\\nRLHF\\n81.8*\\n-\\n74.8*\\n67.3*\\n-\\n54.2*\\nPaLM-2\\n?\\nBase\\n80.7\\n34.3\\n78.3\\n78.1\\n-\\n-\\ngpt-3.5-turbo\\n?\\nRLHF\\n74.9*\\n-\\n67.3*\\n70.1*\\n48.1\\n54.4*\\nclaude-instant-v1.0\\n?\\nRLHF\\n70.8*\\n-\\n-\\n66.9*\\n-\\n54.9*\\ntext-davinci-003\\n?\\nRLHF\\n-\\n-\\n64.6\\n70.7\\n-\\n-\\ncode-davinci-002\\n?\\nBase\\n66.6\\n19.1\\n64.5\\n73.7\\n47.0\\n-\\nMinerva\\n540B\\nSIFT\\n58.8\\n33.6\\n-\\n-\\n-\\n-\\nFlan-PaLM\\n540B\\nSIFT\\n-\\n-\\n70.9\\n66.3\\n-\\n-\\nFlan-U-PaLM\\n540B\\nSIFT\\n-\\n-\\n69.8\\n64.9\\n-\\n-\\nPaLM\\n540B\\nBase\\n56.9\\n8.8\\n62.9\\n62.0\\n26.2\\n-\\ntext-davinci-002\\n?\\nSIFT\\n55.4\\n-\\n60.0\\n67.2\\n-\\n-\\nPaLM\\n64B\\nBase\\n52.4\\n4.4\\n49.0\\n42.3\\n-\\n-\\nLLaMA\\n65B\\nBase\\n50.9\\n10.6\\n63.4\\n-\\n23.7\\n38.8*\\nLLaMA\\n33B\\nBase\\n35.6\\n7.1\\n57.8\\n-\\n21.7\\n-\\nLLaMA\\n13B\\nBase\\n17.8\\n3.9\\n46.9\\n-\\n15.8\\n-\\nFlan-T5\\n11B\\nSIFT\\n16.1*\\n-\\n48.6\\n41.4\\n-\\n-\\nLLaMA\\n7B\\nBase\\n11.0\\n2.9\\n35.1\\n-\\n10.5\\n-\\nFlan-T5\\n3B\\nSIFT\\n13.5*\\n-\\n45.5\\n35.2\\n-\\n-\\nOpenAI GPT including GPT-4 (currently strongest), GPT-\\n3.5-Turbo (faster but less capable), text-davinci-003,\\ntext-davinci-002, and code-davinci-002 (important pre-\\nvious versions before Turbo). See Fu & Khot (2022)\\nfor a comprehensive discussion.\\nAnthropic Claude including claude-v1.3 (slower but more\\ncapable) and claude-instant-v1.0 (faster but less capa-\\nble)6. Strong competitor’s GPT models.\\nGoogle PaLM including\\nPaLM,\\nPaLM-2,\\nand\\ntheir\\ninstruction-tuned versions (FLan-PaLM and Flan-U-\\nPaLM). Strong base and instruction-tuned models.\\nMeta LLaMA including the 7B, 13B, 33B and 65B vari-\\nants. Important open-sourced base models.\\nGoogle FlanT5 instruction-tuned T5 models demonstrat-\\ning strong performance in the smaller model regime.\\nWe report these models’ performance on our CoT Hub suite.\\nWe note that due to the wide spectrum of the tasks and\\nmodels we consider, the evaluation is nontrivial and even\\nrunning inference takes effort. In addition, there are models\\n6https://console.anthropic.com/docs/api/reference\\nthat do not offer public access (such as PaLM), such that\\nevaluating them is difficult. For these reasons, we report\\nnumbers using the following strategy: if the performance\\nof a model is already reported in a paper, we refer to that\\npaper; otherwise, we test them by ourselves. Note that this\\nstrategy is not comprehensive, as we still have a fraction of\\nuntested non-public models on some datasets. This is partly\\nthe reason we view our CoT Hub as a continuous effort.\\nTable 1 shows the overall results. We rank the models using\\nGSM8k performance because it is a classical benchmark\\ntesting models’ reasoning capabilities. Numbers marked\\nby an asterisk are tested by ourselves, others are from the\\nfollowing sources: GPT-4 and PaLM-2 results are from their\\ntechnical report (OpenAI, 2023a; Anil et al., 2023) respec-\\ntively; GPT-3.5-Turbo’s performance on HumanEval is also\\nfrom OpenAI (2023a). Text-davinci-003, code-davinci-002\\nand text-davinci-002 performance are from the appendix\\nin Chung et al. (2022) and from Fu et al. (2022). Minerva’s\\nperformance is from Lewkowycz et al. (2022). PaLM’s per-\\nformance is from Chowdhery et al. (2022). Flan-PaLM and\\nFlanT5 performance are from Chung et al. (2022). LLaMA’s\\nperformance is from Touvron et al. (2023).\\nThe gap between open-source and leading LLMs\\nIn\\n3\\nChain-of-Thought Hub\\nFigure 1. X-axis means the log of the model scale measured in billion parameters. We observe that model performance is generally\\ncorrelated with scale, approximately showing a log-linear trend. Models without disclosing their scale generally perform better than\\nmodels disclosing scale information. Our observations also indicate that the open-source community may still needs to explore/ figure out\\n“the moat” about the scaling and RLHF for further improvements.\\ngeneral, we observe a performance discrepancy between\\nopen-sourced models (like LLaMA and FlanT5) and close-\\nsourced models (GPT, Claude and PaLM). Importantly, the\\nperformance of open-sourced models seems to be upper\\nbounded by LLaMA 65B.\\nLeading LLMs are after RLHF\\nWe observe that ex-\\ncept for PaLM-2, the top 6 models on the leaderboard are\\nafter reinforcement learning from human feedback. This\\nstrongly indicates the effectiveness of RLHF. Given that\\nRLHF is still an underexplored area, we strongly encourage\\nthe community to study more on this topic.\\nCorrelation between model scale and reasoning\\nWe\\nfurther study the relationship between model scale and mod-\\nels’ reasoning performance by visualizing model perfor-\\nmance against model scale. Results are shown in Fig. 1.\\nWe see that: (1) generally, model performance is correlated\\nwith model scale, showing approximately a log-linear trend;\\n(2) models that do not disclose their scale generally perform\\nbetter than models that do, indicating that there is still a gap\\nbetween open-source and close-source.\\nOn the potential of LLaMA-65B\\nFinally, we would\\nlike to highlight the impressive performance of LLaMA\\n65B. On MMLU it is close to code-davinci-002, the base\\nmodel of GPT-3.5 series. On GSM8k, it is worse (pre-\\nsumably because it is not trained on code) but close and\\nmuch better than other open-sourced models (presumably\\nbecause it is trained to Chinchilla-optimal Hoffmann et al.,\\n2022). Combining this observation with the fact that GPT-\\n3.5-Turbo (ChatGPT) is an RLHF model based on code-\\ndavinci-002, it may be possible to reproduce ChatGPT\\nbased on LLaMA 65B by applying the RLHF techniques\\ndiscussed in DeepMind Sparrow (Glaese et al., 2022) and\\nAnthropic Claude (Askell et al., 2021; Bai et al., 2022a;b).\\n4. Conclusion and Future Work\\nIn this work, we propose Chain-of-Thought Hub, an open-\\nsource, continuous effort to measure the reasoning capability\\nof very large language models. Our results clearly show the\\nperformance differences between smaller and larger models,\\nand between close-source and open-source models.\\nAfter carefully examining the results, we show two impor-\\ntant directions for further improving open-sourced models:\\nbuilding better base models and exploring RLHF. We also\\npoint out the great potential of LLaMA 65B: if aligned\\nproperly by better SFT and RLHF, it could be possible to\\nperform on par with ChatGPT-3.5.\\nIn the future, we plan to further extend CoT Hub by: (1) in-\\ncluding more carefully chozen reasoning datasets, especially\\ndatasets measuring commonsense reasoning, math theorem\\nproving, and the ability to call outside APIs; (2) including\\nmore language models, such as LLaMA-based, instruction-\\nfinetuned models like Vicuna7 and models through API\\naccess like Cohere8 and PaLM-2 chat-bison-0019. (3) ex-\\nploring methods for solving MATH, the probably most chal-\\nlenging datasets (recall that it consists of math- ematics com-\\npetitions written in Latex), by calling APIs that compute\\nsymbolic and numerical calculus (like Wolfram Alpha10).\\nIn summary, we believe our work serves as an evaluation\\nplatform that guides the development of open-source large\\nlanguage models.\\n7https://lmsys.org/blog/2023-03-30-vicuna/\\n8https://cohere.com/generate\\n9https://cloud.google.com/vertex-ai\\n10https://www.wolframalpha.com/\\n4\\nChain-of-Thought Hub\\nReferences\\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\\nZ., et al.\\nPalm 2 technical report.\\narXiv preprint\\narXiv:2305.10403, 2023.\\nAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D.,\\nHenighan, T., Jones, A., Joseph, N., Mann, B., DasSarma,\\nN., et al. A general language assistant as a laboratory for\\nalignment. arXiv preprint arXiv:2112.00861, 2021.\\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\\net al. Training a helpful and harmless assistant with rein-\\nforcement learning from human feedback. arXiv preprint\\narXiv:2204.05862, 2022a.\\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,\\nJones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-\\nnon, C., et al. Constitutional ai: Harmlessness from ai\\nfeedback. arXiv preprint arXiv:2212.08073, 2022b.\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\\nG., et al. Evaluating large language models trained on\\ncode. arXiv preprint arXiv:2107.03374, 2021.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\\nStoica, I., and Xing, E. P.\\nVicuna: An open-source\\nchatbot impressing gpt-4 with 90%* chatgpt quality,\\nMarch 2023.\\nURL https://lmsys.org/blog/\\n2023-03-30-vicuna/.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. Palm: Scaling language modeling\\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\\nS., et al. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416, 2022.\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., et al. Training verifiers to solve math word problems.\\narXiv preprint arXiv:2110.14168, 2021.\\nDatabricks.\\nFree dolly:\\nIntroducing the world’s\\nfirst\\ntruly\\nopen\\ninstruction-tuned\\nllm.\\nBlog\\npost,\\nApril\\n2023.\\nURL\\nhttps://www.\\ndatabricks.com/blog/2023/04/12/\\ndolly-first-open-commercially-viable-instruction-tuned-llm.\\nFu, Yao; Peng, H. and Khot, T.\\nHow does gpt obtain\\nits ability?\\ntracing emergent abilities of language\\nmodels to their sources.\\nYao Fu’s Notion, Dec\\n2022.\\nURL https://yaofu.notion.site/\\nHow-does-GPT-Obtain-its-Ability-Tracing-Emergent-\\nFu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\\nComplexity-based prompting for multi-step reasoning.\\narXiv preprint arXiv:2210.00720, 2022.\\nGeng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel,\\nP.,\\nLevine,\\nS.,\\nand Song,\\nD.\\nKoala:\\nA dia-\\nlogue model for academic research.\\nBlog post,\\nApril 2023. URL https://bair.berkeley.edu/\\nblog/2023/04/03/koala/.\\nGlaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu,\\nV., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M.,\\nThacker, P., et al.\\nImproving alignment of dialogue\\nagents via targeted human judgements. arXiv preprint\\narXiv:2209.14375, 2022.\\nGudibande, A., Wallace, E., Snell, C., Geng, X., Liu,\\nH., Abbeel, P., Levine, S., and Song, D.\\nThe false\\npromise of imitating proprietary llms. arXiv preprint\\narXiv:2305.15717, 2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\\nM., Song, D., and Steinhardt, J.\\nMeasuring mas-\\nsive multitask language understanding. arXiv preprint\\narXiv:2009.03300, 2020.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\\nematical problem solving with the math dataset. NeurIPS,\\n2021.\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556,\\n2022.\\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu,\\nJ., Lv, C., Zhang, Y., Lei, J., et al. C-eval: A multi-level\\nmulti-discipline chinese evaluation suite for foundation\\nmodels. arXiv preprint arXiv:2305.08322, 2023.\\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,\\nMichalewski, H., Ramasesh, V., Slone, A., Anil, C.,\\nSchlag, I., Gutman-Solo, T., et al. Solving quantitative\\nreasoning problems with language models. arXiv preprint\\narXiv:2206.14858, 2022.\\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D.,\\nYasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar,\\nA., et al. Holistic evaluation of language models. arXiv\\npreprint arXiv:2211.09110, 2022.\\n5\\nChain-of-Thought Hub\\nOpenAI.\\nGpt-4 technical report.\\narXiv preprint\\narXiv:2303.08774, 2023a.\\nOpenAI. Gpt-4, 2023b. URL https://openai.com/\\nresearch/gpt-4.\\nSuzgun, M., Scales, N., Sch¨arli, N., Gehrmann, S., Tay,\\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\\nE. H., Zhou, D., et al. Challenging big-bench tasks and\\nwhether chain-of-thought can solve them. arXiv preprint\\narXiv:2210.09261, 2022.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,\\nX., Guestrin, C., Liang, P., and Hashimoto, T. B.\\nStanford\\nalpaca:\\nAn\\ninstruction-following\\nllama\\nmodel.\\nhttps://github.com/tatsu-lab/\\nstanford_alpaca, 2023.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., et al. Llama: Open and efficient foundation lan-\\nguage models. arXiv preprint arXiv:2302.13971, 2023.\\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\\nzler, D., et al. Emergent abilities of large language models.\\narXiv preprint arXiv:2206.07682, 2022a.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\\nLe, Q., and Zhou, D. Chain of thought prompting elic-\\nits reasoning in large language models. arXiv preprint\\narXiv:2201.11903, 2022b.\\n6\\n'),\n",
       " Document(metadata={'Published': '2018-08-20', 'Title': 'Space Weather Prediction from the Ground: Case of CHAIN', 'Authors': 'Daikichi Seki, Satoru Ueno, Hiroaki Isobe, Kenichi Otsuji, Denis P. Cabezas, Kiyoshi Ichimoto, Kazunari Shibata, CHAIN team', 'Summary': \"In this article, we insist on the importance and the challenges of the prediction of solar eruptive phenomena including flares, coronal mass ejections (CME), and filament eruptions fully based on the ground-based telescopes. It is true that satellites' data are indispensable for the space weather prediction, but they are vulnerable to the space weather effects. Therefore, the ground-based telescopes can be complementary to them from the viewpoint of space weather prediction. From this view point, one possible new flare prediction method that makes use of H-alpha, red wings, and blue wings images obtained by the SDDI/SMART, the ground-based telescope at Hida Observatory, is presented. And in order to show the possibility for the actual operation based on that method, the recent progress of CHAIN project, the international observation network, is mentioned in terms of their outcomes and capacity buildings.\", 'entry_id': 'http://arxiv.org/abs/1808.06295v1', 'published_first_time': '2018-08-20', 'comment': '12 pages, accepted for publication in Sun and Geosphere', 'journal_ref': None, 'doi': None, 'primary_category': 'astro-ph.SR', 'categories': ['astro-ph.SR'], 'links': ['https://arxiv.org/abs/1808.06295v1', 'https://arxiv.org/pdf/1808.06295v1']}, page_content=' \\nSpace Weather Prediction from the Ground: \\nCase of CHAIN \\n \\nDaikichi SEKI1,2, Satoru UeNo1, Hiroaki Isobe3, Kenichi Otsuji1, Denis \\nP. Cabezas1, Kiyoshi Ichimoto1, Kazunari Shibata1, \\nand CHAIN team1. \\n \\n1. Kwasan and Hida Observatories, Kyoto University, Japan \\n2. GSAIS*, Kyoto University, Japan \\n3. Faculty of Fine Arts, Kyoto City University of Arts, Japan; \\n*Graduate School of Advanced Integrated Studies in Human Survivability \\n \\nE-mail (seki@kwasan.kyoto-u.ac.jp) \\n \\nAbstract \\nIn this article, we insist on the importance and the challenges of the prediction of solar eruptive phenomena \\nincluding flares, coronal mass ejections (CME), and filament eruptions fully based on the ground-based \\ntelescopes. It is true that satellites\\' data are indispensable for the space weather prediction, but they are \\nvulnerable to the space weather effects. Therefore, the ground-based telescopes can be complementary to \\nthem from the viewpoint of space weather prediction. \\nFrom this view point, one possible new flare prediction method that makes use of H-alpha, red wings, and \\nblue wings images obtained by the SDDI/SMART, the ground-based telescope at Hida Observatory, is \\npresented. And in order to show the possibility for the actual operation based on that method, the recent \\nprogress of CHAIN project, the international observation network, is mentioned in terms of their outcomes and \\ncapacity buildings. \\n \\nKeywords: space weather prediction, ground-based telescopes, CHAIN project, multi-wavelength H-alpha \\nimaging, filament eruption \\n \\n \\n \\n \\n1. INTRODUCTION \\nSpace weather is the disturbances in the interplanetary plasmas and in the interplanetary magnetic fields \\nmainly caused by the solar activity. It is known that space weather has potential risks to damage human \\ntechnologies in forms of satellite anomalies, satellite air drag, single events, radiation exposure, and huge \\nblackout (Schrijver et al., 2015). As a global navigation satellite system like GPS is now one of the essential social \\ninfrastructures, human beings will be getting more and more dependent on space technology in future. \\nTherefore, in order to mitigate and prevent space weather disaster, it is highly significant to predict when and \\nhow large space weather events will happen.  \\n     Currently, space weather prediction is mainly done by using space-borne instruments such as the \\nAtmospheric Imaging Assembly (Lemen et al., 2012) and the Helioseismic and Magnetic Imager (Scherrer et al., \\n2012) on the Solar Dynamics Observatory, the Large Angle Spectroscopic Coronagraph (Brueckner et al., 1995) \\non the Solar and Heliospheric Observatory, and Geostationary Operational Environmental Satellite series. There \\nare basically 2 reasons for this situation. One is that they can see the sun in soft and hard X-ray and extreme \\nultraviolet which we can never observe from the ground due to the air and are essential to see the magnetic \\nfields, which give us the keys to predict solar flares. The other is that they can continue observation regardless of \\ntime and terrestrial weather. However, it does not mean that the ground-based telescopes are no longer useful \\nin terms of space weather prediction. Firstly, compared to the satellites, the ground-based telescopes are \\ncheap, and developing countries can also introduce the instruments. Secondly, the ground-based telescopes \\ndo not have any effects of space weather, while artificial satellites could be damaged by severe space \\nweather events. Therefore, the ground-based telescopes can be used as backup, and even if a satellite \\nobserving in H-alpha and its wings should launch, they would still have some complementary roles in case of \\nsatellites\\' failure by a huge solar flare. \\n     In this article, we present the possibility of space weather prediction, especially of predicting solar \\nexplosive phenomena, by the ground-based telescopes, with reference to the recent progress of the \\nContinuous H-Alpha Imaging Network (CHAIN) project (UeNo et al. 2007) worked by Kyoto University. In the next \\nsection, a possible candidate of prediction systems will be introduced. In the first subsection, we will give a \\nmethod of prediction with reference to the recent suggestion in Seki et al. (2017), and in the second subsection, \\nfrom a hardware point of view the recent progress of CHAIN will be reported. And the 3 challenges we are \\nfacing will be mentioned in the last section.  \\n \\n2. PREDICTION BY H-ALPHA IMAGE \\nThere are several observational methods from the ground such as a coronagraph, a magnetogram, a \\ncontinuum light observation, and a H-alpha observation. At Hida Observatory in Kyoto University, there is a \\n \\npowerful instrument observing the sun in H-alpha line and its wings called Solar Dynamics Doppler Imager \\n(SDDI) installed on Solar Magnetic Activity Research Telescope (SMART) (Ichimoto et al., 2017). \\n \\n2-1 Prediction of a Filament Eruption by H-alpha Images (Seki et al. (2017)) \\nSDDI has been conducting a routine observation since 2016 May 1. It takes the solar full-disk images of 73 \\nchannels at every 0.25 Å from the H-alpha line center −9.0 Å to the H-alpha line center +9.0 Å, i.e., at 36 \\npositions in the blue wing, the H-alpha line center, and 36 positions in the H-alpha red wing. Each image is \\nobtained with a time cadence of 15 seconds and a pixel size of about 1.2 arcsec (Ichimoto et al., 2017). When \\nthe weather permits, it continuously monitors the Sun during the daytime in Japan for about 10 hr. Details on the \\ninstrument and examples of images and line profiles can be found in Ichimoto et al. (2017). \\n     Making the most of the small gap between channels of 0.25 Å and the good time cadence of 15 sec, \\nSeki et al. (2017) deduced automatically the unprecedented detailed line-of-sight velocities of the filament \\nwhich erupted around 4:00UT on 2016 November 5th by utilizing Beckers’ cloud model (Beckers, 1964; Morimoto \\nand Kurokawa, 2003a,b)) (See Figure 1). The brief explanation of the method is as follows; \\n1. \\ndetermining the positions of the pixels whose intensities are lower than the subtraction between the \\naverage intensity of the full sun and the double of its standard deviation for each wavelength image, \\n2. \\ngetting all the positions together to obtain a binary image whose pixels at the same positions have 1 and \\nthe other pixels have 0, \\n3. \\nsmoothing the binary image by taking the average of 5” x 5” pixels around each pixel, \\n4. \\nconducting \"erosion-dilation-dilation-erosion\" process in the binary image (\"erosion\" and \"dilation\" are \\nfundamental techniques in the field of morphological image processing) to get the \"mask\" which cover \\nthe most of the pixels inside a filament, and  \\n5. \\ncalculating the line-of-sight velocities of a filament inside the \"mask\" based on Beckers\\' cloud model. \\n  \\n \\n  \\n   \\nAs a result, by tracking the standard deviation of \\nline-of-sight velocities for each velocity map, we found \\nthat it increased sharply around 1 hour before eruption. \\nFigure 2 shows the time transition of the standard \\ndeviation. On the previous day of the eruption, the \\nstandard deviation was almost constant around 2-3 \\nkm s-1. However, on the next day (the day of the \\neruption) it slightly increases to 3-4 km s-1 and stays \\nconstant until around 0:30UT. Then it gradually \\nincreases at a rate of 1.1 m s-2 until it peaks around \\n1:10UT, and it starts to increase again sharply at a rate \\nof 2.8 m s-2 around 2:30UT, which corresponds to \\naround 1 hour before eruption. In this case, \\naccompanied by a filament eruption, B-class flare \\nstarted around 4:00UT and the X-ray flux peaked \\naround 4:40UT, and also weak CME occurred. \\nTherefore, this work suggests that if we track the \\nstandard deviation of the line-of-sight velocities of a \\nfilament, we may be able to predict solar explosive \\nphenomena around 1 hour before it happens only by \\nusing ground-based telescopes\\' data.  \\n \\nFigure 1. Time series of H-alpha images at the line center, at +0.5 Å, at −0.5 Å, and at −1.0 Å (the top \\nfour rows), and of the images of LOS velocity of the filament (bottom row). (Seki et al., 2017) \\n \\n     From the operational point of view, it should be noted that in 2017 September we started the automatic \\ndetection and calculation of line-of-sight velocity of a filament during the observation every day. The deduced \\ndata \\nhas been \\nstored \\nat Hida \\nObservatory and everyone \\ncan \\naccess it \\nvia \\nits web \\nsite \\nhttp://www.hida.kyoto-u.ac.jp/SMART/SDDI/cloudmodel/. Combining this system and the result of Seki et al. (2017), it \\nmust be easy to monitor filaments during observation and to create an automatic alerting system of filament \\neruptions. \\n \\n2-2 CHAIN project \\n2-2-1 \\nIntroduction of CHAIN \\nWe have been working Continuous H-Alpha Imaging Network (CHAIN) project, which is to create a world-wide \\nobservational network with ground-based solar telescopes observing in multiple wavelengths including H-alpha \\nline center, red wing and blue wing for the purpose of 24-hour continuous observation of the three-dimensional \\nvelocity fields of filament eruptions and shock-wave structures on the whole solar surface (See No.6 of List of \\nISWI Projects at http://www.iswi-secretariat.org)(UeNo et al., 2014). Actually, there are other H-alpha network \\nsuch as the Global Oscillation Network Group (GONG) (Harvey et al., 2011) and the Global H-alpha Network \\n(GHN) (Steinegger et al., 2000). However, it is only this network group all over the world that is observing not only \\nH-alpha center but also its red and blue wings.  \\n     There are 3 purposes for this project; \\n1. \\nreinforcement of observations of the solar activity by formation of an international network of \\nground-based solar observations for 24 hours continuously, \\n2. \\nobservation and study of filament eruptions, shock waves (Moreton waves) with solar flares and variation \\nof UV radiation on the full-disk of the Sun in order to understand and predict the change of \\nspace-weather environment from the Sun to the Earth, \\n3. \\ninternational spread, academic exchange and promotion of the space-weather research including \\ndeveloping countries. \\nFrom the space weather point of view, there are 2 important aspects of CHAIN project, (1) constructing \\nground-based 24-hour space weather prediction system by creating an international solar observational \\nnetwork with ground-based telescopes and (2) capacity building including technical and scientific training \\nand space weather education through installing and operating the ground-based solar telescopes.  \\n2-2-2 \\nOutcomes of CHAIN \\nAs its outcomes so far, 3 countries including Japan, Peru, and Saudi Arabia are cooperating, and 17 papers \\nrelated to CHAIN have been published (See Table 1). The contents of 7 papers are related to CHAIN project \\nitself. The brief summaries of the contents are as follow: \\n \\n• \\nUeNo et al. (2007): It is the first paper related to CHAIN, and they explained about the project and their \\nplan. \\n• \\nIshitsuka et al. (2007): They introduced their plan to refurbish solar observing stations in Peru and \\nmentioned about installing FMT from Kyoto University in that context. \\n• \\nUeNo et al. (2009): This paper published in Data Science Journal is explaining about data archive and \\nobserving system of CHAIN, and they claimed the necessity of improving the information technology. \\n• \\nUeNo et al. (2010): With the introduction of CHAIN project, they mentioned that Algeria is one of the best \\ncandidates of this project and explained their plan of investigation. \\n• \\nSeghouani (2010): He claimed the necessity of a new astronomical observatory in Algeria and mentioned \\nabout Dr. UeNo\\'s visit and investigation. \\n• \\nIshitsuka et al. (2014): They showed the summaries of capacity building, observed data, and scientific \\nresults brought by the FMT installed in Peru as part of CHAIN project. \\n• \\nUeNo et al. (2014): They showed the progresses about international collaboration and academic \\nexchange of CHAIN from 2010 to 2013. \\nIn the other 10 papers, CHAIN data are used for their scientific researches (See Table 1). Brief explanations are \\nshown below, \\n• \\nNagashima et al. (2007): They used SMART H-alpha data to investigate the position and the active motion \\nof 2 filaments. \\n• \\nNarukage et al. (2008): SMART data were used to discover 3 Moreton waves on 2005 August 3. \\n• \\nAsai et al. (2009): SMART data were used to show the temporal evolution of the AR NOAA 10798 in \\nH-alpha. \\n• \\nZhang et al. (2011): FMT multi-wavelengths data were used to study the statistical properties of \\npropagating Moreton waves. \\n• \\nAsai et al. (2012): SMART data were used to detect a Moreton wave on 2011 August 9 and associated \\nfilament/prominence oscillations. \\n• \\nIshii et al. (2013): A new high-speed imaging system for solar flares installed on SMART was introduced. \\n• \\nShen et al. (2014): SMART data were used to calculate the Doppler velocity of a filament to investigate its \\noscillation property. \\n• \\nIchimoto et al. (2017): A new Hα and its wings imaging instrument, the Solar Dynamics Doppler Imager, \\nwas introduced. \\n• \\nCabezas et al. (2017): FMT data obtained at the National University San Luis Gonzaga of Ica, Peru, were \\nused to derive the 3-dimensional velocity field of a filament eruption associated with a M-class flare on \\n2011 February 16. \\n \\n• \\nSeki et al. (2017): SMART/SDDI data were used for analyzing the amplitude of the small-scale motion of the \\nfilament which erupted on 2016 November 5. \\n \\nAs for the capacity building, 7 lectures, 4 scientific educations, 2 technical trainings, and 5 data-analysis \\nworkshops have been held (See Table 2) for these 10 years.  \\n \\nTable 1. Published Papers related to CHAIN project \\nYear Title \\nAuthors \\nJournal \\n2007 \\nCHAIN-Project and Installation of the Flare \\nMonitoring Telescopes in Developing Countries  \\nUeNo, S., Shibata, \\nK., Kimura, G. at al. \\nBulletin of the Astronomical \\nSociety of India 35, 697. \\n2007 \\nTriggering Mechanism for the Filament Eruption \\non 2005 September 13 in NOAA Active Region \\n10808  \\nNagashima, K., \\nIsobe, H., \\nYokoyama, T. et al. \\nAstrophysical Journal 668, \\n533. \\n2007 \\nA solar observing station for education and \\nresearch in Peru  \\nIshitsuka, J., \\nIshitsuka, M., Avilés \\nH. T. et al. \\nBulletin of the Astronomical \\nSociety of India 35, 709 \\n2008 \\nThree Successive and Interacting Shock Waves \\nGenerated by a Solar Flare  \\nNarukage, N., Ishii, T. \\nT., Nagata, S. et al. \\nAstrophysical Journal \\nLetters 684, L45 \\n2009 \\nThe CHAIN- Project and Installation of Flare \\nMonitoring Telescopes in Developing Countries \\nUeNo, S., Shibata, \\nK., Kitai, R. et al. \\nData Science Journal 8, 30 \\n2009 \\nEvolution of Anemone AR NOAA 10798 and the \\nRelated Geo- Effective Flares and CMEs  \\nAsai, A., Shibata,K., \\nIshii, T. T. et al. \\nJournal of Geophysical \\nResearch 114, A00A21 \\n2010 \\nUn Observatoire dans la Région de Aurès \\nSeghouani, N. \\nAfrican Skies 14, 44 \\n2010 \\nContinuous H- alpha Imaging Network Project \\n(CHAIN) with Ground-based Solar Telescopes \\nfor Space Weather Research  \\nUeNo, S., Shibata, \\nK., Ichimoto, K. et al. \\nAfrican Skies 14, 17 \\n2011 \\nPropagation of Moreton Waves  \\nZhang, Y., Kitai, R., \\nNarukage, N. et al. \\nPublications of the \\nAstronomical Society of \\nJapan 63, 685 \\n \\n2012 \\nFirst Simultaneous Observation of an Hα \\nMoreton Wave, EUV Wave, and Filament/ \\nProminence Oscillations  \\nAsai, A., Ishii, T. T., \\nIsobe, H. et al. \\nAstrophysical Journal \\nLetters 745, L18 \\n2013 \\nHigh-Speed Imaging System for Solar-Flare \\nResearch at Hida Observatory  \\nIshii, T. T., Kawate, T., \\nNakatani, Y. et al. \\nPublications of the \\nAstronomical Society of \\nJapan 65, 39 \\n2014 \\nInternational Collaboration and Academic \\nExchange of the CHAIN Project in this Three \\nYears (ISWI Period)  \\nUeNo, S., Shibata, \\nK., Morita, S. et al. \\nSun and Geosphere 9, 97 \\n2014 \\nWithin the International Collaboration CHAIN: a \\nSummary of Events Observed with Flare \\nMonitoring Telescope (FMT) in Peru  \\nIshitsuka, J., Asai, A., \\nMorita, S. et al. \\nSun and Geosphere 9, 85 \\n2014 \\nA Chain of Winking (Oscillating) Filaments \\nTriggered by an Invisible Extreme-ultraviolet \\nWave  \\nShen, Y., Ichimoto, \\nK., Ishii, T. T. et al. \\nAstrophysical Journal 786, \\n151 \\n2017 \\nA New Solar Imaging System for Observing \\nHigh-Speed Eruptions: Solar Dynamics Doppler \\nImager (SDDI)  \\nIchimoto, K., Ishii, T. \\nT., Otsuji, K. et al. \\nSolar Physics 292, 63 \\n2017 \\nIncrease in the Amplitude of Line-of-sight \\nVelocities of the Small-scale Motions in a Solar \\nFilament before Eruption  \\nSeki, D., Otsuji, K., \\nIsobe, H. et al. \\nAstrophysical Journal \\nLetters 843, L24 \\n2017 \\n“Dandelion” Filament Eruption and Coronal \\nWaves Associated with a Solar Flare on 2011 \\nFebruary 16  \\nCabezas, D. P., \\nMartínez, L. M., \\nBuleje Y. J. et al. \\nAstrophysical Journal 836, \\n33 \\n \\n \\nTable 2. Capacity Building Activities \\n \\n3. CHALLENGES FOR THIS TRIAL \\n3-1 Terrestrial Weather & Climate \\nOne of the weakest points of the ground-based telescopes is the influence of terrestrial weather. As for CHAIN \\nin the current situation, if one of the institutions cannot observe the sun, there is no other observatory that can \\ncompliment the data. Moreover, FMT in Saudi Arabia has another effect from its nature. Because of its climate, \\nthe high temperature, it reaches around 50 degrees Celsius in the observation dome in the noon on summer. It \\nType \\nDate \\nPlace \\nCountry \\nLecture \\nJanuary 2007 \\nIca (National Ica Univ.) \\nPeru \\nJanuary 2007 \\nLima(Instituto Geofísico del Perú) \\nPeru \\nMay 2008 \\nBouzaréah \\n(Centre \\nde \\nRecherche \\nen \\nAstronomie, Astrophysique et Geophysique) \\nAlgeria \\nJune 2008 \\nIca \\nPeru \\nMarch 2010 \\nIca \\nPeru \\nMay 2011 \\nRiyadh \\nSaudi Arabia \\nAugust 2015 \\nRiyadh \\nSaudi Arabia \\nScientific \\nEducation \\nJune 2010 \\nIca (National Ica Univ.) \\nPeru \\nOctober 2010 \\nIca (National Ica Univ.) \\nPeru \\nNovember 2010 \\nIca (National Ica Univ.) \\nPeru \\nOctober 2015 \\n(King Saud Univ.) \\nSaudi Arabia \\nTechnical \\nTraining \\nJanuary 2007 \\nIca (National Ica Univ.) \\nPeru \\nJuly 2009 \\nHida (Hida Observatory) \\nJapan \\nData-analysis \\nworkshop \\nNovember 2010 \\nIca \\nPeru \\nJuly 2011 \\nHida (Hida Observatory) & Mitaka (National \\nAstronomical Observatory of Japan) \\nJapan \\nMarch 2013 \\nHida (Hida Observatory) \\nJapan \\nMarch 2015 \\nKyoto (Kwasan Observatory) \\nJapan \\nFebruary 2017 \\nKyoto (Kyoto Univ. & Kwasan Observatory) \\nJapan \\n \\nis not expected for the filter installed on the telescope to become over 45 degrees Celsius so that it makes \\nerrors in observing wavelengths. \\n \\n3-2 Smooth Data Access \\nActually, all the 3 stations do observe the sun as long as it can, but as for the data sharing, some difficulties still \\nexist. Currently internet line speed is very slow and it takes more time to send each station\\'s observing data to \\nJapan than for them to observe the sun. Because of this situation, we mainly gather the data at Japan not by \\ninternet but by currying HDD. \\n3-3 Applicability of the Suggested Prediction Method \\nThe last challenge is the applicability of the new prediction method presented by Seki et al. (2017). In the paper, \\nwe presented one possibility to predict solar explosive phenomena, but the feature was investigated for only \\none event. Therefore, as the future work, it should be confirmed statistically whether the increase in the \\namplitude of line-of-sight velocity of small-scale motions in a solar filament can be seen in almost all the \\nfilaments before their eruption. In addition, if we actually try to use CHAIN as the data source of the prediction, \\nwe also should confirm whether FMT data can be used for this prediction method, because the less amount of \\nobserving wavelengths leads to the less accurate estimation of line-of-sight velocities. (SDDI has been \\nobserving at every 0.25 Å from Hα line - 9 Å to H-alpha line + 9 Å every 15 seconds, while FMTs have been \\nobserving at 3 or 5 wavelengths, H-alpha line center and H-alpha line ± 0.8 Å in Peru, or H-alpha line center, \\nH-alpha line ± 0.6 Å, and H-alpha line ± 1.2 Å in Saudi Arabia, in every 20 seconds.) Therefore, it should be \\nchecked whether the suggested precursor can also be detected from FMT data or not. \\n \\nAcknowledgement \\nWe appreciate the daily routine observation and the maintenance and development of the instruments by the \\nstaff of Hida observory members. We are also grateful to the anonymous referees for their fruitful comments \\nthat greatly improved our work. \\n \\nREFERENCES \\n[1] \\nAsai, A., Ishii, T.T., Isobe, H., Kitai, R., Ichimoto, K., UeNo, S., Nagata, S., Morita, S., Nishida, K., Shiota, D., Oi, \\nA., Akioka, M., and Shibata, K.: 2012, Astrophys. J. 745, L18. \\n[2] \\nAsai, A., Shibata, K., Ishii, T. T., Oka, M., Kataoka, R., Fujiki, K., and Gopalswamy, N.: 2009, J. Geophys. Res. \\n114, A00A21. \\n[3] \\nBeckers, J.M.: 1964, Ph.D. thesis, University of Utrecht. \\n[4] \\nBrueckner, G.E., Howard, R.A., Koomen, M.J., Korendyke, C.M., Michels, D.J., Moses, J.D., Socker, D.G., \\n \\nDere, K.P., Lamy, P.L., Llebaria, A., Bout, M.V., Schwenn, R., Simnett, G.M., Bedford, D.K., and Eyles, C.J.: \\n1995, Sol. Phys. 162, 357. \\n[5] \\nCabezas, D.P., Martínez, L.M., Buleje, Y.J., Ishitsuka, M., Ishitsuka, J.K., Morita, S., Asai, A., UeNo, S., Ishii, T.T., \\nKitai, R., Takasao, S., Yoshinaga, Y., Otsuji, K., and Shibata, K.: 2017, Astrophys. J. 836, 33. \\n[6] \\nHarvey, J.W., Bolding, J., Clark, R., Hauth, D., Hill, F., Kroll, R., Luis, G., Mills, N., Purdy, T., Henney, C., Holland, \\nD., and Winter, J.: 2011, Bull. Am. Astron. Soc. 43, 17. \\n[7] \\nIchimoto, K., Ishii, T.T., Otsuji, K., Kimura, G., Nakatani, Y., Kaneda, N., Nagata, S., UeNo, S., Hirose, K., \\nCabezas, D.P., and Morita, S.: 2017, Sol. Phys. 292, 63. \\n[8] \\nIshii, T.T., Kawate, T., Nakatani, Y., Morita, S., Ichimoto, K., and Masuda, S.: 2013, Publ. Astron. Soc. Jpn. 65, \\n39. \\n[9] \\nIshitsuka, J., Asai, A., Morita, S., Terrazas, R., Cabezas, D.P., Gutierrez, V., Martinez, L., Buleje, Y., Loayza, R., \\nNakamura, N., Takasao, S., Yoshinaga, Y., Hillier, A., Otsuji, K., Shibata, K., Ishitsuka, M., UeNo, S., Kitai, R., \\nIshii, T.T., Ichimoto, K., Nagata, S., and Narukage, N.: 2014, Sun and Geosphere 9, 85. \\n[10] \\nIshitsuka, J., Ishitsuka, M., Avilés, H.T., Sakurai, T., Nishino, Y., Miyazaki, H., Shibata, K., UeNo, S., Yumoto, K., \\nand Maeda, G.: 2007, Bull. Astron. Soc. India 35, 709. \\n[11] \\nLemen, J.R., Title, A.M., Akin, D.J., Boerner, P.F., Chou, C., Drake, J.F., Duncan, D.W., Edwards, C.G., \\nFriedlaender, F.M., Heyman, G.F., Hurlburt, N.E., Katz, N.L., Kushner, G.D., Levay, M., Lindgren, R.W., \\nMathur, D.P., McFeaters, E.L., Mitchell, S., Rehse, R.A., Schrijver, C.J., Springer, L.A., Stern, R.A., Tarbell, T.D., \\nWuelser, JP, Wolfson, C.J., Yanari, C., Bookbinder, J.A., Cheimets, P.N., Caldwell, D., Deluca, E.E., Gates, \\nR., Golub, L., Park, S., Podgorski, W.A., Bush, R.I., Scherrer, P.H., Gummin, M.A., Smith, P., Auker, G., Jerram, \\nP., Pool, P., Soufli, R., Windt, D.L., Beardsley, S., Clapp, M., Lang, J., and Waltham, N.: 2012, Sol. Phys. 275, \\n14. \\n[12] \\nMorimoto, T., and Kurokawa, H.: 2003, Publ. Astron. Soc. Jpn 55, 503. \\n[13] \\nMorimoto, T., and Kurokawa, H.: 2003, Publ. Astron. Soc. Jpn. 55, 1141. \\n[14] \\nNagashima, K., Isobe, H., Yokoyama, T., Ishii, T.T., Okamoto, T.J., and Shibata, K.: 2007, Astrophys. J. 668, \\n533. \\n[15] \\nNarukage, N., Ishii, T. T., Nagata, S., UeNo, S., Kitai, R., Kurokawa, H., Akioka, M., and Shibata, K.: 2008, \\nAstrophys. J. 684, L45. \\n[16] \\nScherrer, P. H., Schou, J., Bush, R.I., Kosovichev, A.G., Bogart, R.S., Hoeksema, J.T., Liu, Y., Duvall, T.L., Zhao, \\nJ., Title, A.M., Schrijver, C.J., Tarbell, T.D., and Tomczyk. S.: 2012, Sol. Phys. 275, 207. \\n[17] \\nSchrijver, C.J., Kauristie, K., Aylward, A.D., Denardini, C.M., Gibson, S.E., Glover, A., Gopalswamy, N., \\nGrande, M., Hapgood, M., Heynderickx, D., Jakowski, N., Kalegaev, V.V., Lapenta, G., Linker, J.A., Liu, S., \\nMandrini, C.H., Mann, I.R., Nagatsuma, T., Nandy, D., Obara, T., O’Brien, T.P., Onsager, T., Opgenoorth, \\n \\nH.J., Terkildsen, M., Valladares, C.E., and Vilmer, N.: 2015, Adv. Sp. Res. 55, 2745. \\n[18] \\nSeghouani, N.: 2010, African Skies 14, 44. \\n[19] \\nSeki, D., Otsuji, K., Isobe, H., Ishii, T.T., Sakaue, T., Hirose, K.: 2017, Astrophys. J. 843, L24. \\n[20] \\nShen, Y., Ichimoto, K., Ishii, T.T., Tian, Z., Zhao, R., and Shibata, K.: 2014, Astrophys. J. 786, 151. \\n[21] \\nSteinegger, M., Denker, C., Goode, P.R., Marquette, W.H., Varsik, J., Wang, H., Otruba, W., Freislich, H., \\nHanslmeier, A., Luo, G., Chen, D., and Zhang, Q.: 2000, The Solar Cycle and Terrestrial Climate, Solar and \\nSpace Weather Euroconference, European Space Agency 463, 617. \\n[22] \\nUeNo, S., Shibata, K., Ichimoto, K., Kitai, R., Nagata, S., Kimura, G., and Nakatani, Y.: 2010, African Skies 14, \\n17. \\n[23] \\nUeNo, S., Shibata, K., Kimura, G., Nakatani, Y., Kitai, R., and Nagata, S.: 2007, Bull. Astron. Soc. India 35, \\n697. \\n[24] \\nUeNo, S., Shibata, K., Kitai, R., Ichimoto, K., Nagata, S., Isobe, H., Kimura, G., Nakatani, Y., Kadota, M., \\nKomori, H., Ishii, T.T., Morita, S., and Otsuji, K.: 2009, Data Science J. 8, 30. \\n[25] \\nUeNo, S., Shibata, K., Morita, S., Kimura, G., Asai, A., Kitai, R., Ichimoto, K., Nagata, S., Ishii, T.T., Nakatani, \\nY., Yamaguchi, M. et al.: 2014, Sun and Geosphere 9, 97. \\n[26] \\nZhang, Y., Kitai, R., Narukage, N., Matsumoto, T., UeNo, S., Shibata, K., and Wang, J.: 2011, Publ. Astron. \\nSoc. Jpn. 63, 685. \\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 로드 결과출력\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f18bc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2024-10-04',\n",
       " 'Title': 'Understanding Reasoning in Chain-of-Thought from the Hopfieldian View',\n",
       " 'Authors': 'Lijie Hu, Liang Liu, Shu Yang, Xin Chen, Zhen Tan, Muhammad Asif Ali, Mengdi Li, Di Wang',\n",
       " 'Summary': \"Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on improving performance, lacking a comprehensive framework to explain and understand the fundamental factors behind CoT's success. To bridge this gap, we introduce a novel perspective grounded in the Hopfieldian view of cognition in cognitive neuroscience. We establish a connection between CoT reasoning and key cognitive elements such as stimuli, actions, neural populations, and representation spaces. From our view, we can understand the reasoning process as the movement between these representation spaces. Building on this insight, we develop a method for localizing reasoning errors in the response of CoTs. Moreover, we propose the Representation-of-Thought (RoT) framework, which leverages the robustness of low-dimensional representation spaces to enhance the robustness of the reasoning process in CoTs. Experimental results demonstrate that RoT improves the robustness and interpretability of CoT reasoning while offering fine-grained control over the reasoning process.\",\n",
       " 'entry_id': 'http://arxiv.org/abs/2410.03595v1',\n",
       " 'published_first_time': '2024-10-04',\n",
       " 'comment': '28 pages, a new version of \"A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning\"',\n",
       " 'journal_ref': None,\n",
       " 'doi': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'categories': ['cs.AI', 'cs.CL', 'cs.LG'],\n",
       " 'links': ['https://arxiv.org/abs/2410.03595v1',\n",
       "  'https://arxiv.org/pdf/2410.03595v1']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 메타데이터 출력\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58a907",
   "metadata": {},
   "source": [
    "`load_all_available_meta=False` 인 경우 메타데이터는 전체가 아닌 일부만 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb84ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2023-01-18',\n",
       " 'Title': 'How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection',\n",
       " 'Authors': 'Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu',\n",
       " 'Summary': \"The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 에 검색하고자 하는 논문의 주제를 입력합니다.\n",
    "loader = ArxivLoader(\n",
    "    query=\"ChatGPT\",\n",
    "    load_max_docs=2,  # 최대 문서 수\n",
    "    load_all_available_meta=False,  # 메타데이터 전체 로드 여부\n",
    ")\n",
    "\n",
    "# 문서 로드 결과출력\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 메타데이터 출력\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70249aec",
   "metadata": {},
   "source": [
    "## 요약(summary)\n",
    "\n",
    "- 논문의 전체 내용이 아닌 요약본을 출력하고자 한다면, `get_summaries_as_docs()` 함수를 호출하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8117a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\n"
     ]
    }
   ],
   "source": [
    "# 문서 요약 로딩\n",
    "docs = loader.get_summaries_as_docs()\n",
    "\n",
    "# 첫 번째 문서 접근\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788cb47",
   "metadata": {},
   "source": [
    "## lazy_load()\n",
    "\n",
    "문서를 대량으로 로드할 때 모든 로드된 문서의 부분 집합에 대해 하류 작업을 수행할 수 있다면, 메모리 사용량을 최소화하기 위해 문서를 한 번에 하나씩 지연 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b230a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "# 문서 지연 로드\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37793c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-01-18', 'Title': 'How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection', 'Authors': 'Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu', 'Summary': \"The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\"}, page_content='How Close is ChatGPT to Human Experts?\\nComparison Corpus, Evaluation, and Detection\\nBiyang Guo1†∗, Xin Zhang2∗, Ziyuan Wang1∗, Minqi Jiang1∗, Jinran Nie3∗\\nYuxuan Ding4, Jianwei Yue5, Yupeng Wu6\\n1AI Lab, School of Information Management and Engineering\\nShanghai University of Finance and Economics\\n2Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen)\\n3School of Information Science, Beijing Language and Culture University\\n4School of Electronic Engineering, Xidian University\\n5School of Computing, Queen’s University, 6Wind Information Co., Ltd\\nAbstract\\nThe introduction of ChatGPT2 has garnered widespread attention in both academic\\nand industrial communities. ChatGPT is able to respond effectively to a wide range\\nof human questions, providing ﬂuent and comprehensive answers that signiﬁcantly\\nsurpass previous public chatbots in terms of security and usefulness. On one hand,\\npeople are curious about how ChatGPT is able to achieve such strength and how\\nfar it is from human experts. On the other hand, people are starting to worry about\\nthe potential negative impacts that large language models (LLMs) like ChatGPT\\ncould have on society, such as fake news, plagiarism, and social security issues.\\nIn this work, we collected tens of thousands of comparison responses from both\\nhuman experts and ChatGPT, with questions ranging from open-domain, ﬁnancial,\\nmedical, legal, and psychological areas. We call the collected dataset the Human\\nChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the\\ncharacteristics of ChatGPT’s responses, the differences and gaps from human\\nexperts, and future directions for LLMs. We conducted comprehensive human\\nevaluations and linguistic analyses of ChatGPT-generated content compared with\\nthat of humans, where many interesting results are revealed. After that, we conduct\\nextensive experiments on how to effectively detect whether a certain text is gener-\\nated by ChatGPT or humans. We build three different detection systems, explore\\nseveral key factors that inﬂuence their effectiveness, and evaluate them in differ-\\nent scenarios. The dataset, code, and models are all publicly available at https:\\n//github.com/Hello-SimpleAI/chatgpt-comparison-detection.\\n1\\nIntroduction\\nSince its dazzling debut in November 2022, OpenAI’s ChatGPT has gained huge attention and wide\\ndiscussion in the natural language processing (NLP) community and many other ﬁelds. According\\nto OpenAI, ChatGPT is ﬁne-tuned from the GPT-3.5 series with Reinforcement Learning from\\nHuman Feedback (RLHF; [7, 32]), using nearly the same methods as InstructGPT [25], but with\\nslight differences in the data collection setup. The vast amount of knowledge in GPT-3.5 and the\\nmeticulous ﬁne-tuning based on human feedback enable ChatGPT to excel at many challenging NLP\\n∗Equal Contribution.\\n†Project Lead. Corresponding to guo_biyang@163.com\\n+Each author has made unique contributions to the project.\\n2Launched by OpenAI in November 2022. https://chat.openai.com/chat\\narXiv:2301.07597v1  [cs.CL]  18 Jan 2023\\ntasks, such as translating natural language to code [5], completing the extremely masked text [15] or\\ngenerating stories given user-deﬁned elements and styles [40], let alone typical NLP tasks like text\\nclassiﬁcation, entity extraction, translation, etc. Furthermore, the carefully collected human-written\\ndemonstrations also make ChatGPT able to admit its mistakes, challenge incorrect premises and\\nreject even inappropriate requests, as claimed by OpenAI3.\\nThe surprisingly strong capabilities of ChatGPT have raised many interests, as well as concerns:\\nOn the one hand, people are curious about how close is ChatGPT to human experts. Different\\nfrom previous LLMs like GPT-3 [4], which usually fails to properly respond to human queries,\\nInstructGPT [25] and the stronger ChatGPT have improved greatly in interactions with humans.\\nTherefore, ChatGPT has great potential to become a daily assistant for general or professional\\nconsulting purposes [20, 21]. From the linguistic or NLP perspectives, we are also interested in\\nwhere are the remaining gaps between ChatGPT and humans and what are their implicit linguistic\\ndifferences [14, 18].\\nOn the other hand, people are worried about the potential risks brought by LLMs like ChatGPT.\\nWith the free preview demo of ChatGPT going virus, a large amount of ChatGPT-generated content\\ncrowded into all kinds of UGC (User-Generated Content) platforms, threatening the quality and\\nreliability of the platforms. For example, Stack Overﬂow, the famous programming question-\\nanswering website, has temporarily banned ChatGPT-generated content4, because it believes \"the\\naverage rate of getting correct answers from ChatGPT is too low, the posting of answers created by\\nChatGPT is substantially harmful to the site and to users who are asking and looking for correct\\nanswers\". Many other applications and activities are facing similar issues, such as online exams [33]\\nand medical analysis [20]. Our empirical evaluation of ChatGPT on legal, medical, and ﬁnancial\\nquestions also reveals that potentially harmful or fake information can be generated.\\nConsidering the opaqueness of ChatGPT and the potential social risks associated with model misuse,\\nwe make the following contributions to both the academy and society:\\n1. To facilitate LLM-related research, especially the study on the comparison between humans\\nand LLMs, we collect nearly 40K questions and their corresponding answers from human\\nexperts and ChatGPT, covering a wide range of domains (open-domain, computer science,\\nﬁnance, medicine, law, and psychology), named as the Human ChatGPT Comparison\\nCorpus (HC3) dataset. The HC3 dataset is a valuable resource to analyze the linguistic and\\nstylist characteristics of both humans and ChatGPT, which helps to investigate the future\\nimprovement directions for LLMs;\\n2. We conduct comprehensive human evaluations as well as linguistic analysis on\\nhuman/ChatGPT-generated answers, discovering many interesting patterns exhibited by\\nhumans and ChatGPT. These ﬁndings can help to distinguish whether certain content is\\ngenerated by LLMs, and also provide insights about where language models should be\\nheading in the future;\\n3. Based on the HC3 dataset and the analysis, we develop several ChatGPT detecting models,\\ntargeting different detection scenarios. These detectors show decent performance in our\\nheld-out test sets. We also conclude several key factors that are essential to the detector’s\\neffectiveness.\\n4. We open-source all the collected comparison corpus, evaluations, and detection models, to\\nfacilitate future academic research and online platform regulations on AI-generated content.\\n2\\nHuman ChatGPT Comparison Corpus (HC3)\\nChatGPT is based on the GPT-3.5 series, which is pre-trained on the super-large corpus, consisting of\\nweb-crawled text, books, and codes, making it able to respond to all kinds of questions. Therefore,\\nwe are curious how will a human (especially an expert) and ChatGPT respond to the same question\\nrespectively. Inspired by [1], we also want to evaluate whether ChatGPT can keep honest (not\\nfabricate information or mislead the user), harmless (shouldn’t generate harmful or offensive content),\\n3https://openai.com/blog/chatgpt/\\n4https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned\\n2\\nHC3-English\\n# Questions\\n# Human Answers\\n# ChatGPT Answers\\nSource\\nAll\\n24322\\n58546\\n26903\\nreddit_eli5\\n17112\\n51336\\n16660\\nELI5 dataset [10]\\nopen_qa\\n1187\\n1187\\n3561\\nWikiQA dataset [39]\\nwiki_csai\\n842\\n842\\n842\\nCrawled Wikipedia (A.1)\\nmedicine\\n1248\\n1248\\n1337\\nMedical Dialog dataset [6]\\nﬁnance\\n3933\\n3933\\n4503\\nFiQA dataset [23]\\nHC3-Chinese\\n# Questions\\n# Human Answers\\n# ChatGPT Answers\\nSource\\nAll\\n12853\\n22259\\n17522\\nopen_qa\\n3293\\n7377\\n3991\\nWebTextQA & BaikeQA [38]\\nbaike\\n4617\\n4617\\n4617\\nCrawled BaiduBaike (A.1)\\nnlpcc_dbqa\\n1709\\n1709\\n4253\\nNLPCC-DBQA dataset [8]\\nmedicine\\n1074\\n1074\\n1074\\nMedical Dialog dataset [6]\\nﬁnance\\n689\\n1572\\n1983\\nChineseNlpCorpus (A.1)\\npsychology\\n1099\\n5220\\n1099\\nfrom Baidu AI Studio (A.1)\\nlaw\\n372\\n690\\n505\\nLegalQA dataset (A.1)\\nTable 1: Meta-information of the HC3 dataset. The English (resp. Chinese) contains 5 (resp. 7) splits.\\nand how helpful (provide concrete and correct solutions to the user’s question) it is compared to\\nhuman experts.\\nTaking these into account, we decided to collect a comparison corpus that consists of both human and\\nChatGPT answers to the same questions. We believe such a comparison corpus can be a valuable and\\ninteresting source to study the nature of the language of both humans and language models.\\n2.1\\nHuman Answers Collection\\nInviting human experts to manually write questions and answers is tedious and unaffordable for us\\nto collect a large amount of data, therefore we construct the comparison dataset mainly from two\\nsources:\\n• Publicly available question-answering datasets, where answers are given by experts in speciﬁc\\ndomains or the high-voted answers by web users;\\n• Wiki text. We construct question-answer pairs using the concepts and explanations from wiki\\nsources like Wikipedia5 and BaiduBaike6.\\nThe split-data source mapping is shown in Table 1, and please refer to Appendix A.1 for further\\ndetailed information.\\n2.2\\nChatGPT Answers Collection\\nBased on the collected human question-answering datasets, we use ChatGPT to generate answers\\nto these questions. Since the ChatGPT is currently only available through its preview website, we\\nmanually input the questions into the input box, and get the answers, with the aid of some automation\\ntesting tools. Answers by ChatGPT can be inﬂuenced by the chatting history, so we refresh the thread\\nfor each question.\\nTo make the answer more aligned with human answers, we add additional instructions to ChatGPT\\nfor speciﬁc datasets. For example, the human answers from the reddit-eli5 dataset split are under\\nthe context of \"Explain like I’m ﬁve\", therefore we use this context to instruct ChatGPT by adding\\n\"Explain like I’m ﬁve\" at the end of the original question. More detail can be found in the Appendix.\\n5https://www.wikipedia.org/\\n6https://baike.baidu.com/\\n3\\nChatGPT can generate different answers given the same question in different threads, which is\\nperhaps due to the random sampling in the decoding process. However, we found the differences can\\nbe very small, thereby we only collect one answer for most questions.\\n2.3\\nHuman ChatGPT Comparison Corpus (HC3)\\nFor each question, there can be more than one human/ChatGPT answer, therefore we organize the\\ncomparison data using the following format:\\n1 {\\n2\\n\"question \": \"Q1\",\\n3\\n\"human_answers \": [\"A1\", \"A2\"],\\n4\\n\" chatgpt_answers \": [\"B1\"]\\n5 }\\nOverall, we collected 24, 322 questions, 58, 546 human answers and 26, 903 ChatGPT answers for\\nthe English version, and 12, 853 questions, 22, 259 human answers and 17, 522 ChatGPT answers\\nfor the Chinese version. The meta-information of each dataset split is illustrated in Table 1.\\n3\\nHuman Evaluation & Summarization\\nIn this section, we invite many volunteer testers and conduct extensive human evaluations from\\ndifferent aspects. After the human evaluation, we make our collected comparison corpus available\\nto the volunteers and ask them to manually conclude some characteristics. We then summarize the\\nfeedback from the volunteers combined with our observations.\\n3.1\\nHuman Evaluation\\nThe human evaluation is divided into the Turing test and the Helpfulness Test. The Turing Test\\n[34] is a test of a machine’s ability to exhibit intelligent behavior that is indistinguishable from a\\nhuman. We invite 17 volunteers, divided into two groups: 8 experts (who are frequent users of\\nChatGPT) and 9 amateurs (who have never heard of ChatGPT). This is because people who are\\nfamiliar with ChatGPT may have memorized some patterns exhibited by ChatGPT, helping them to\\neasily distinguish the role.\\nWe designed four types of evaluations, using different query formats or testing groups. We introduce\\nthe speciﬁc evaluation design and results in the following parts:\\nA. Expert Turing Test, Paired Text (pair-expert)\\nThe pair-expert test is conducted in the expert group. Each tester is required to do a series of\\ntests, each test containing one question and a pair of answers (one from humans and another from\\nChatGPT). The tester needs to determine which answer is generated by ChatGPT.\\nB. Expert Turing Test, Single Text (single-expert)\\nThe single-expert test is also conducted in the expert group. Each tester is required to do a\\nseries of tests, each test containing one question and a single answer randomly given by humans or\\nChatGPT. The tester needs to determine whether the answer is generated by ChatGPT.\\nC. Amateur Turing Test, Single Text (single-amateur)\\nThe single-amateur test is conducted in the amateur group. Each tester is required to do a series\\nof tests, each test containing one question and a single answer randomly given by humans or ChatGPT.\\nThe tester needs to determine whether the answer is generated by ChatGPT.\\nD. Helpfulness Test (helpfulness)\\nWe are also curious about how helpful are the answers from ChatGPT compared with humans’\\nanswers to one question. Note that helpfulness is a very subjective metric, which can be inﬂuenced\\nby many factors, including emotion, tester personality, personal preference, etc. Therefore, simply\\nproviding more accurate information or a more detailed analysis may not always lead to a more\\nhelpful answer.\\nThe helpfulness test is conducted in the expert group. Each tester is required to do a series of tests,\\neach containing one question and a pair of answers (one from human and another from ChatGPT).\\n4\\nHuman Evaluation (En)\\nPair-expert\\nSingle-expert\\nSingle-amateur\\nHelpfulness\\nAll\\n0.90\\n0.81\\n0.48\\n0.57\\nreddit_eli5\\n0.97\\n0.94\\n0.57\\n0.59\\nopen_qa\\n0.98\\n0.78\\n0.34\\n0.72\\nwiki_csai\\n0.97\\n0.61\\n0.39\\n0.71\\nmedical\\n0.97\\n0.97\\n0.50\\n0.23\\nﬁnance\\n0.79\\n0.73\\n0.58\\n0.60\\nHuman Evaluation (Zh)\\nPair-expert\\nSingle-expert\\nSingle-amateur\\nHelpfulness\\nAll\\n0.93\\n0.86\\n0.54\\n0.54\\nopen_qa\\n1.00\\n0.92\\n0.47\\n0.50\\nbaike\\n0.76\\n0.64\\n0.60\\n0.60\\nnlpcc_dbqa\\n1.00\\n0.90\\n0.13\\n0.63\\nmedicine\\n0.93\\n0.93\\n0.57\\n0.30\\nﬁnance\\n0.86\\n0.84\\n0.84\\n0.75\\npsychology\\n1.00\\n1.00\\n0.60\\n0.67\\nlaw\\n1.00\\n0.77\\n0.56\\n0.56\\nTable 2: Human evaluations of ChatGPT generated answers for both English and Chinese.\\nEach tester is asked to pretend that the question is proposed by him/herself, and needs to determine\\nwhich answer is more helpful to him/her.\\nSettings.\\nWe sample around 30 <question, human_answer, chatgpt_answer> triplets from\\neach split (i.e., reddit_eli5, wikipedia, medical, etc.) as the samples for the human evaluation. We\\nallocate 2-5 testers for each split and report their average results. For all Turing tests, we report the\\nproportion that ChatGPT-generated answer is correctly detected by testers. For the helpfulness test,\\nwe report the proportion that ChatGPT-generated answer is considered to be more helpful.\\nResults.\\nSeveral conclusions can be drawn from the results shown in Table 2. Comparing the\\nresults of pair-expert and single-expert, we can ﬁnd that it is easier to distinguish ChatGPT-\\ngenerated content when providing a comparison pair than only providing a single answer. Com-\\nparing the results of single-expert and single-amateur, we can ﬁnd that the accuracy of\\nexperts is much higher than that of amateurs. The helpfulness test gives the proportion of\\nquestions that volunteers think the ChatGPT answer is more helpful to them. Surprisingly, results\\nshow that ChatGPT’s answers are generally considered to be more helpful than humans’ in\\nmore than half of questions, especially for ﬁnance and psychology areas. By checking the speciﬁc\\nanswers in these domains, we ﬁnd that ChatGPT can usually provide more concrete and speciﬁc\\nsuggestions. However, ChatGPT performs poorly in terms of helpfulness for the medical domain\\nin both English and Chinese. The ChatGPT often gives lengthy answers to medical consulting in\\nour collected dataset, while human experts may directly give straightforward answers or suggestions,\\nwhich may partly explain why volunteers consider human answers to be more helpful in the medical\\ndomain.\\n3.2\\nHuman Summarization\\nAfter the above evaluations, we open our collected HC3 dataset to the volunteers where they can\\nfreely browse the comparison answers from humans and ChatGPT. All dataset splits are allocated\\nto different volunteers, and each volunteer is asked to browse at least 100 groups of comparison\\ndata. After that, we ask them to summarize the characteristics of both human answers and ChatGPT\\nanswers. Eventually, we received more than 200 feedbacks, and we summarize these ﬁndings as\\nfollows:\\n5\\nDistinctive Patterns of ChatGPT\\n(a) ChatGPT writes in an organized manner, with clear logic. Without loss of generality,\\nChatGPT loves to deﬁne the core concept in the question. Then it will give out detailed\\nanswers step by step and offers a summary at the end, following the deduction and summary\\nstructure;\\n(b) ChatGPT tends to offer a long and detailed answer. This is the direct product of the\\nReinforcement Learning with Human Feedback, i.e. RLHF, and also partly related to the\\npattern (a) unless you offer a prompt such as \"Explain it to me in one sentence\";\\n(c) ChatGPT shows less bias and harmful information. ChatGPT is neutral on sensitive\\ntopics, barely showing any attitude towards the realm of politics or discriminatory toxic\\nconversations;\\n(d) ChatGPT refuses to answer the question out of its knowledge. For instance, ChatGPT\\ncannot respond to queries that require information after September 2021. Sometimes\\nChatGPT also refuses to answer what it believes it doesn’t know. It is also RLHF’s ability to\\nimplicitly and automatically determine which information is within the model’s knowledge\\nand which is not.\\n(e) ChatGPT may fabricate facts. When answering a question that requires professional\\nknowledge from a particular ﬁeld, ChatGPT may fabricate facts in order to give an answer,\\nthough [25] mentions that InstructGPT model has already shown improvements in truthful-\\nness over GPT-3. For example, in legal questions, ChatGPT may invent some non-existent\\nlegal provisions to answer the question. This phenomenon warns us to be extra careful when\\nusing ChatGPT for professional consultations. Additionally, when a user poses a question\\nthat has no existing answer, ChatGPT may also fabricate facts in order to provide a response.\\nMany of the conclusions mentioned above like (b),(c),(d) are also discussed in [12] by Fu et al.\\nMajor Differences between Human and ChatGPT\\n(a) ChatGPT’s responses are generally strictly focused on the given question, whereas\\nhumans’ are divergent and easily shift to other topics. In terms of the richness of\\ncontent, humans are more divergent in different aspects, while ChatGPT prefers focusing on\\nthe question itself. Humans can answer the hidden meaning under the question based on\\ntheir own common sense and knowledge, but the ChatGPT relies on the literal words of the\\nquestion at hand;\\n(b) ChatGPT provides objective answers, while humans prefer subjective expressions.\\nGenerally, ChatGPT generates safer, more balanced, neutral, and informative texts compared\\nto humans. As a result, ChatGPT is excellent at interpreting terminology and concepts. On\\nthe other hand, human answers are more speciﬁc and include detailed citations from sources\\nbased on legal provisions, books, and papers, especially when providing suggestions for\\nmedical, legal, and technical problems, etc.;\\n(c) ChatGPT’s answers are typically formal, meanwhile humans’ are more colloquial.\\nHumans tend to be more succinct with full of oral abbreviations and slang such as \"LOL\",\\n\"TL;DR\", \"GOAT\" etc. Humans also love to apply humor, irony, metaphors, and exam-\\nples, whereas ChatGPT never uses antiphrasis. Additionally, human communication often\\nincludes the \"Internet meme\" as a way to express themselves in a speciﬁc and vivid way;\\n(d) ChatGPT expresses less emotion in its responses, while human chooses many punctu-\\nation and grammar feature in context to convey their feelings. Human uses multiple\\nexclamation mark(’!’), question mark(’?’), ellipsis(’...’) to express their strong emotion, and\\nuse various brackets(’(’, ’)’, ’[’, ’]’) to explain things. By contrast, ChatGPT likes to use\\nconjunctions and adverbs to convey a logical ﬂow of thought, such as \"In general\", \"on the\\nother hand\", \"Firstly,..., Secondly,..., Finally\" and so on.\\nOverall, these summarised features indicate that ChatGPT has improved notably in question-\\nanswering tasks for a wide range of domains. Compared with humans, we can imagine ChatGPT\\nas a conservative team of experts. As a \"team\", it may lack individuality but can have a more\\ncomprehensive and neutral view towards questions.\\n6\\nEnglish\\navg. len.\\nvocab size\\ndensity\\nChinese\\navg. len.\\nvocab size\\ndensity\\nhuman\\nAll\\n142.50\\n79157\\n2.33\\nAll\\n102.27\\n75483\\n5.75\\nChatGPT\\n198.14\\n66622\\n1.41\\n115.3\\n45168\\n3.05\\nhuman\\nreddit_eli5\\n134.21\\n55098\\n2.46\\nnlpcc_dbqa\\n24.44\\n10621\\n25.43\\nChatGPT\\n194.84\\n44926\\n1.38\\n78.21\\n11971\\n8.96\\nhuman\\nopen_qa\\n35.09\\n9606\\n23.06\\nopen_qa\\n93.68\\n40328\\n13.13\\nChatGPT\\n131.68\\n16251\\n10.40\\n150.66\\n26451\\n5.35\\nhuman\\nwiki_csai\\n229.34\\n15859\\n8.21\\nbaike\\n112.25\\n28966\\n5.59\\nChatGPT\\n208.33\\n9741\\n5.55\\n77.19\\n14041\\n3.94\\nhuman\\nmedicine\\n92.98\\n11847\\n10.42\\nmedicine\\n92.34\\n9855\\n9.94\\nChatGPT\\n209.61\\n7694\\n3.00\\n165.41\\n7211\\n4.06\\nhuman\\nﬁnance\\n202.07\\n25500\\n3.21\\nﬁnance\\n80.76\\n2759\\n5.05\\nChatGPT\\n226.01\\n21411\\n2.41\\n120.84\\n4043\\n4.94\\nhuman\\n-\\n-\\n-\\n-\\npsychology\\n254.82\\n16160\\n5.77\\nChatGPT\\n-\\n-\\n-\\n-\\n164.53\\n5897\\n3.26\\nhuman\\n-\\n-\\n-\\n-\\nlaw\\n28.77\\n2093\\n19.55\\nChatGPT\\n-\\n-\\n-\\n-\\n143.76\\n3857\\n7.21\\nTable 3: Average answer length, vocabulary size and density comparisons on our corpus.\\n4\\nLinguistic Analysis\\nIn this section, we analyze the linguistic features of both humans’ and ChatGPT’s answers, and try to\\nﬁnd some statistical evidence for the characteristics concluded in Section 3.\\n4.1\\nVocabulary Features\\nIn this part, we analyze the vocabulary features of our collected corpus. We are interested in how\\nhumans and ChatGPT differ in the choice of words when answering the same set of questions.\\nSince the number of human/ChatGPT answers is unbalanced, we randomly sample one answer from\\nhumans and one answer from ChatGPT during our statistical process. We calculated the following\\nfeatures: average length (L), which is the average number of words in each question; vocab size (V ),\\nthe number of unique words used in all answers; we also propose another feature called density (D),\\nwhich is calculated by D = 100 × V/(L × N) where N is the number of answers. Density measures\\nhow crowded different words are used in the text. For example, if we write some articles that add up\\nto 1000 words, but only 100 different words are used, then the density is 100 × 100/1000 = 10. The\\nhigher the density is, the more different words are used in the same length of text.\\nIn Table 3, we report the vocabulary features for both English and Chinese corpus. Looking at both\\nfeatures of average length and vocab size, we can see that: compared to ChatGPT, human answers\\nare relatively shorter, but a larger vocabulary is used. This phenomenon is particularly obvious\\nin the Chinese open_qa split and the medical splits in both languages, where the average length of\\nChatGPT is nearly twice longer than that of humans, but the vocab size is signiﬁcantly smaller.\\nThis phenomenon is also reﬂected by the density factor. The word density of humans is greater than\\nChatGPT’s in every split, which further reveals that humans use a more diverse vocabulary in\\ntheir expressions.\\n4.2\\nPart-of-Speech & Dependency Analysis\\nIn this part, we compare the occurrences of different part-of-speech (POS) tags and the characteristics\\nof the dependency relations.\\n4.2.1\\nPart-of-Speech\\nFigure 1 illustrates the comparisons between humans and ChatGPT in terms of POS usage. In\\nHC3-English, ChatGPT uses more NOUN, VERB, DET, ADJ, AUX, CCONJ and PART words, while using\\nless ADV and PUNCT words.\\n7\\nNOUN\\nPUNCT\\nVERB\\nADP\\nDET\\nPRON\\nADJ\\nAUX\\nADV\\nCCONJ\\nPROPN\\nPART\\nSCONJ\\nNUM\\nSPACE\\nSYM\\nX\\nINTJ\\n0\\n5\\n10\\n15\\n20\\n25\\nProportion (%)\\n18.7\\n12.4\\n11.2\\n9.1\\n8.4\\n8.3\\n6.8\\n6.3\\n4.9\\n3.1\\n3.0\\n2.8\\n2.4\\n1.4\\n0.4\\n0.3\\n0.2\\n0.2\\n21.1\\n9.6\\n11.7\\n9.3\\n9.1\\n7.3\\n7.9\\n7.0\\n3.3\\n4.1\\n2.2\\n3.5\\n2.6\\n0.6\\n0.6\\n0.1\\n0.1\\n0.0\\nPart-of-Speech Comparison (En)\\nHuman\\nChatGPT\\nNOUN\\nVERB\\nPUNCT\\nADV\\nPART\\nNUM\\nPRON\\nADP\\nPROPN\\nADJ\\nCCONJ\\nDET\\nSPACE\\nSCONJ\\nX\\n_\\nINTJ\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nProportion (%)\\n26.0\\n20.8\\n15.0\\n9.7\\n8.7\\n4.2\\n3.6\\n2.8\\n2.6\\n2.3\\n1.4\\n1.2\\n0.9\\n0.4\\n0.4\\n0.0\\n0.0\\n27.5\\n20.3\\n13.4\\n7.5\\n8.7\\n3.0\\n4.9\\n3.5\\n2.4\\n2.4\\n2.9\\n1.5\\n0.8\\n0.7\\n0.3\\n0.0\\n0.0\\nPart-of-Speech Comparison (Zh)\\nHuman\\nChatGPT\\nFigure 1: Part-of-Speech distribution comparison between ChatGPT and human answers. Results are\\nsorted by POS proportion of human answers. The upper ﬁgure is for the HC3-English dataset and the\\nlower is for the HC3-Chinese dataset.\\nA high proportion of nouns (NOUN) often indicates that the text is more argumentative, exhibiting\\ninformativeness and objectivity [24]. Accordingly, adposition (ADP) and adjective (ADJ) words\\nalso tend to appear more frequently [11]. The frequent co-occurrence of conjunctions (CCONJ)\\nalong with nouns, verbs, and adposition words indicates that the structure of the article and the\\nrelationships of cause-and-effect, progression, or contrast are clear. The above are also typical\\ncharacteristics in academic papers or ofﬁcial documents [29]. We believe the RLHF training process\\nhas a great inﬂuence on ChatGPT’s writing style, which partly explains the difference in the POS\\ntags distribution.\\n4.2.2\\nDependency Parsing\\nDependency parsing is a technique that analyzes the grammatical structure of a sentence by identifying\\nthe dependencies between its words. We parse the answers in the corpus and compare the proportion\\nof different dependency relations and their corresponding dependency distances. Figure 2 shows the\\ncomparison between humans and ChatGPT in HC3-English. Due to the limited space, the Chinese\\nversion is placed in the Appendix A.2.\\nThe comparison of dependency relations exhibits similar characteristics to that of POS tags, where\\nChatGPT uses more determination, conjunction, and auxiliary relations. In terms of the dependency\\ndistance, ChatGPT has much longer distances for the punct and dep relations, which is perhaps due\\nto the fact that CharGPT tends to use longer sentences. However, ChatGPT has obviously shorter\\nconj relations. According to the analysis of POS tags, ChatGPT usually uses more conjunctions than\\nhumans to make the content more logical, this may explain why the conj relations of ChatGPT are\\nrelatively shorter than humans.\\n4.3\\nSentiment Analysis\\nHumans are emotional beings, it is natural that our emotions are reﬂected in our words, to some\\nextent. ChatGPT is learned on large-scale human-generated text, but it is further ﬁne-tuned with\\nhuman instructions. Therefore we are curious how \"emotional\" ChatGPT is compared with humans.\\nWe use a multilingual sentiment classiﬁcation model7 ﬁne-tuned on Twitter corpus [2] to conduct\\nsentiment analysis for both English and Chinese comparison data. Note that deep learning-based\\nmodels can be greatly inﬂuenced by some indicating words (such as \"but\" and \"sorry\" can easily\\n7https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment\\n8\\npunct\\nprep\\ndet\\nnsubj\\npobj\\nadvmod\\namod\\ndobj\\nROOT\\naux\\nconj\\ncompoundcc\\nadvcl\\nmark\\nccomp\\nposs\\nattr\\nxcomp\\nrelcl\\nacomp\\nneg\\nauxpass\\nnummod\\nnsubjpass\\npcomp\\ndep\\nappos\\nacl\\nprt\\nnpadvmod\\nnmod\\nexpl\\ncase\\nagent\\nquantmod\\noprd\\nintj\\ndative\\ncsubj\\npredet\\nparataxis\\npreconj\\nmeta\\ncsubjpass\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nProportion (%)\\n12.5\\n8.3\\n8.3\\n8.1\\n7.7\\n5.5\\n5.3\\n4.9\\n4.9\\n4.4\\n3.4\\n3.1\\n3.0\\n1.9\\n1.8\\n1.7\\n1.3\\n1.3\\n1.2\\n1.2\\n1.2\\n1.0\\n0.9\\n0.8\\n0.8\\n0.6\\n0.6\\n0.6\\n0.5\\n0.5\\n0.5\\n0.4\\n0.3\\n0.2\\n0.2\\n0.2\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.0\\n0.0\\n0.0\\n9.7\\n8.7\\n9.0\\n7.5\\n8.2\\n3.9\\n5.8\\n5.2\\n3.9\\n5.6\\n4.5\\n3.0\\n4.1\\n2.0\\n1.9\\n1.5\\n1.5\\n1.2\\n1.9\\n1.7\\n1.6\\n0.6\\n1.3\\n0.4\\n1.1\\n0.7\\n0.7\\n0.3\\n0.5\\n0.3\\n0.2\\n0.2\\n0.2\\n0.3\\n0.2\\n0.1\\n0.2\\n0.0\\n0.1\\n0.1\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\nDependency Relation Comparison (En)\\nHuman\\nChatGPT\\npunct\\nprep\\ndet\\nnsubj\\npobj\\nadvmod\\namod\\ndobj\\nROOT\\naux\\nconj\\ncompoundcc\\nadvcl\\nmark\\nccomp\\nposs\\nattr\\nxcomp\\nrelcl\\nacomp\\nneg\\nauxpass\\nnummod\\nnsubjpass\\npcomp\\ndep\\nappos\\nacl\\nprt\\nnpadvmod\\nnmod\\nexpl\\ncase\\nagent\\nquantmod\\noprd\\nintj\\ndative\\ncsubj\\npredet\\nparataxis\\npreconj\\nmeta\\ncsubjpass\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\nProportion (%)\\n7.3\\n2.4\\n1.6\\n2.0\\n2.1\\n2.3\\n1.4\\n2.3\\n0.0\\n1.4\\n7.0\\n1.1\\n4.6\\n8.6\\n3.7\\n7.2\\n1.5\\n2.9\\n2.5\\n3.6\\n1.7\\n1.2\\n1.2\\n1.4\\n3.5\\n1.9\\n3.6\\n4.5\\n2.6\\n1.2\\n3.9\\n2.4\\n1.1\\n1.0\\n1.4\\n1.5\\n2.8\\n4.0\\n1.7\\n5.6\\n2.3\\n7.2\\n1.8\\n5.1\\n7.1\\n9.4\\n2.3\\n1.5\\n2.0\\n2.1\\n2.3\\n1.3\\n2.2\\n0.0\\n1.2\\n5.5\\n1.1\\n3.6\\n8.8\\n3.7\\n6.7\\n1.6\\n3.0\\n2.7\\n3.7\\n1.4\\n1.2\\n1.2\\n1.4\\n3.2\\n1.6\\n9.9\\n4.4\\n2.5\\n1.2\\n3.4\\n2.8\\n1.1\\n1.0\\n1.2\\n1.3\\n2.4\\n3.8\\n2.1\\n6.5\\n2.3\\n6.4\\n1.6\\n5.2\\n8.4\\nDependency Distance Comparison (En)\\nHuman\\nChatGPT\\nFigure 2: Top-30 dependency relations (upper) and corresponding dependency distances (lower)\\ncomparison between human and ChatGPT answers in HC3-English. Results are sorted by relations\\nproportion of human answers.\\nNeutral\\nPositive\\nNegative\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nProportion (%)\\n54\\n4\\n44\\n72\\n3\\n25\\nSentiment Distribution (English)\\nHuman\\nChatGPT\\n(a) Sentiment distribution of HC3-English\\nNeutral\\nPositive\\nNegative\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nProportion (%)\\n67\\n5\\n27\\n78\\n4\\n18\\nSentiment Distribution (Chinese)\\nHuman\\nChatGPT\\n(b) Sentiment distribution of the HC3-Chinese\\nFigure 3: Proportions of three kinds of sentiments (neutral, positive, and negative) in our corpus.\\nfool the classiﬁer to predict the \"negative\" label), making the predictions biased [16]. Therefore, the\\nsentiment given by the classiﬁer is only a reference to the true sentiment behind the text.\\nFigure 3 shows the comparison of the sentiment distribution of humans and ChatGPT. Several ﬁndings\\ncan be drawn from the results: First, we ﬁnd that the proportion of neutral emotions is the largest for\\nboth humans and ChatGPT, which is in line with our expectations. However, ChatGPT generally\\nexpresses more neutral sentiments than humans. Then, the proportion of negative emotions is\\nsigniﬁcantly higher than that of positive emotions. Notably, humans express signiﬁcantly more\\nnegative emotions than ChatGPT. The proportion of humans’ positive emotions is also slightly\\nhigher than that of ChatGPT. Overall, ChatGPT is less emotional than humans, though it is not\\ncompletely emotionless.\\n4.4\\nLanguage Model Perplexity\\nThe perplexity (PPL) is commonly used as a metric for evaluating the performance of language\\nmodels (LM). It is deﬁned as the exponential of the negative average log-likelihood of the text under\\n9\\n1\\n25\\n50\\n75\\n110\\n150\\n190\\n0.00\\n0.01\\n0.02\\n0.03\\n0.04\\nProportion\\nlabel\\nChatGPT\\nHuman\\n(a) English text ppl\\n1\\n25\\n50\\n75\\n110\\n150\\n190\\n0.000\\n0.005\\n0.010\\n0.015\\n0.020\\n0.025\\n0.030\\nProportion\\nlabel\\nChatGPT\\nHuman\\n(b) English sent ppl\\n1 2 3 4 5 6 7 8\\n10\\n12\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nProportion\\nlabel\\nChatGPT\\nHuman\\n(c) Chinese text ppl\\n1 2 3 4 5 6 7 8\\n10\\n12\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nProportion\\nlabel\\nChatGPT\\nHuman\\n(d) Chinese sent ppl\\nFigure 4: PPL distributions on both English and Chinese data, as well as both text and sentence levels.\\nthe LM. A lower PPL indicates that the language model is more conﬁdent in its predictions, and\\nis therefore considered to be a better model. The training of LMs is carried out on large-scale text\\ncorpora, it can be considered that it has learned some common language patterns and text structures.\\nTherefore, we can use PPL to measure how well a text conforms to common characteristics.\\nWe use the open-source GPT-2 small8 (Wenzhong-GPT2-110M9 for Chinese) model to compute the\\nPPL (both text-level and sentence-level10 PPLs) of the collected texts. The PPL distributions of text\\nwritten by humans and text generated by ChatGPT are shown in Figure 4.\\nIt is clearly observed that, regardless of whether it is at the text level or the sentence level, the content\\ngenerated by ChatGPT has relatively lower PPLs compared to the text written by humans. ChatGPT\\ncaptured common patterns and structures in the text it was trained on, and is very good at reproducing\\nthem. As a result, text generated by ChatGPT have relatively concentrated low PPLs.\\nHumans have the ability to express themselves in a wide variety of ways, depending on the context,\\naudience, and purpose of the text they are writing. This can include using creative or imaginative\\nelements, such as metaphors, similes, and unique word choices, which can make it more difﬁcult for\\nGPT2 to predict. Therefore, human-written texts have more high-PPL values, and show a long-tailed\\ndistribution, as demonstrated in Figure 4.\\n5\\nChatGPT Content Detection\\nAI-generated content (AIGC) is becoming increasingly prevalent on the internet, and it can be\\ndifﬁcult to distinguish it from human-generated content, as shown in our human evaluation (sec 3.1).\\nTherefore, AIGC detectors are needed to help identify and ﬂag content that has been created by a\\nmachine, to reduce the potential risks to society caused by improper or malicious use of AI models,\\nand to improve the transparency and accountability of the information that is shared online.\\nIn this section, we conduct several empirical experiments to investigate the ChatGPT content detection\\nsystems. Detecting AI-generated content is a widely studied topic [19, 27]. Based on these [30, 13,\\n27], we establish three different types of detection systems, including machine learning-based and\\ndeep learning-based methods, and evaluate them on different granularities and data sources. Detailed\\nresults and discussions are provided.\\n5.1\\nMethods\\nDetection of machine-generated text has been gaining popularity as text generation models have\\nadvanced in recent years[19, 27]. Here, we implement three representative methods from classic\\nmachine learning and deep learning, i.e, a logistic regression model trained on the GLTR Test-2[13]\\nfeatures, a deep classiﬁer for single-text detection and a deep classiﬁer for QA detection. The deep\\nclassiﬁers for both single-text and QA are based on RoBERTa [22], a strong pre-trained Transformer\\n[35] model. In fact, algorithms for OOD detection or anomaly detection [17] can also be applied to\\ndevelop ChatGPT content detectors, which we leave for future work.\\n8https://huggingface.co/gpt2\\n9https://huggingface.co/IDEA-CCNL/Wenzhong-GPT2-110M\\n10For English text, we used NLTK[3] for sentence segmentation (HarvestText for Chinese).\\n10\\nFigure 5: The experiment design for the training and testing of detectors. Different dataset versions\\nare generated through ﬁltering or splitting.\\nGLTR.\\n[13] studied three tests to compute features of an input text. Their major assumption is that\\nto generate ﬂuent and natural-looking text, most decoding strategies sample high probabilities tokens\\nfrom the head of the distribution. We select the most powerful Test-2 feature, which is the number\\nof tokens in the Top-10, Top-100, Top-1000, and 1000+ ranks from the LM predicted probability\\ndistributions. And then a logistic regression model is trained to ﬁnish the classiﬁcation.\\nRoBERTa-sinlge.\\nA deep classiﬁer based on the pre-trained LM is always a good choice for\\nthis kind of text classiﬁcation problem. It is also investigated in many studies and demo systems\\n[30, 9, 27]. Here we ﬁne-tune the RoBERTa [22] model.\\nRoBERTa-QA.\\nWhile most content detectors are developed to classify whether a single piece of\\ntext is AI-generated, we claim that a detector that supports inputting both a question and an answer\\ncan be quite useful, especially for question-answering scenarios. Therefore, we decide to also build\\na QA version detector. The RoBERTa model supports a text pair input format, where a separating\\ntoken is used to join a question and its corresponding answer.\\n5.2\\nImplementation Details\\nFor the LM used by GLTR, we use gpt2-small [28] for English, and Wenzhong-GPT2-110M released\\nby [36] for Chinese, it is the same with sec. 4.4. For RoBERTa-based deep classiﬁers, we use\\nroberta-base11 and hfl/chinese-roberta-wwm-ext12 checkpoints for English and Chinese,\\nrespectively. All the above models are obtained from huggingface transformers [37].\\nWe train the logistic regression model by sklearn [26] on the GLTR Test-2 features from trainset, and\\nsearch hyper-params following the code of [27]. The RoBERTa-based detectors are trained by the\\nfacilities of transformers. Speciﬁcally, we use the AdamW optimizer, setting batch size to 32 and\\nlearning rate to 5e −5. We ﬁnetune models by 1 epoch for English, and 2 epochs for Chinese.\\n5.3\\nExperiment Design\\nThe HC3 dataset consists of questions and their corresponding human/ChatGPT answers. We\\nextracted all the <question, answer> pairs, and assigned label 0 to pairs with human answers and\\nlabel 1 to pairs with ChatGPT answers.\\nSimply using the original answers from humans and ChatGPT to train a binary classiﬁer is the most\\nstraightforward way. However, there might be some issues by doing so:\\n• First, based on the observations in Section 3, both human answers and ChatGPT answers\\nmay contain some obvious indicating words that may inﬂuence the effectiveness of models;\\n• Second, users may want to detect whether a single sentence is generated by ChatGPT,\\ninstead of the full text. This can be quite difﬁcult for a classiﬁer that is only trained on full\\ntexts;\\n• Third, taking the corresponding question of the answer into account may help the detector\\nto make a more accurate judgment, compared with only considering the answer itself. This\\n11https://huggingface.co/roberta-base\\n12https://huggingface.co/hfl/chinese-roberta-wwm-ext\\n11\\ncan be widely applied to many QA platforms (like Quora, Stack Overﬂow, and Zhihu) to\\nﬁnd out which answer below a certain question is generated by AI.\\nTherefore, we design different groups of experiments to study these key questions:\\n• How will the indicating words inﬂuence the detector?\\n• Is it more challenging for the ChatGPT detectors to detect sentence-level content? Is it harder to\\ntrain a sentence-level classiﬁer?\\n• Can the corresponding question help detectors detect the origin of the answer more accurately?\\nFigure 5 shows how we generate different types of training and testing sets. Speciﬁcally, we use\\nthe collected raw corpus to construct the ﬁrst train-test sets (the \"full text (raw)\" in the ﬁgure),\\nwhich we call the raw-full version. Then we ﬁlter away the indicated words in the text to obtain the\\nﬁltered-full version. By splitting the full text into sentences, we obtain the raw-sent version and the\\nﬁltered-sent version. We also combine the full text and the sentences into a mixed version, namely\\nthe raw-mix and ﬁltered-mix version. Overall, we have six different versions of training and testing\\nsets. Evaluating a model’s performance on version B’s testing set which is trained on version A’s\\ntraining set can be seen as an out-of-distribution (OOD) generalization evaluation, which is more\\nchallenging since it requires the model to be robust when facing sample style changes.\\n5.4\\nResults\\nFollowing the above experiment design, we conduct comprehensive empirical studies on all kinds of\\nderived corpus. Table 4 shows the test F1 scores.\\nEnglish\\nChinese\\nTest →\\nraw\\nﬁltered\\nAvg.\\nraw\\nﬁltered\\nAvg.\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nTrain ↓\\nRoBERTa\\nraw\\nfull\\n99.82\\n81.89\\n84.67\\n99.72\\n81.00\\n84.07\\n88.53\\n98.79\\n83.64\\n86.32\\n98.57\\n82.77\\n85.85\\n89.32\\nsent\\n99.40\\n98.43\\n98.56\\n99.24\\n98.47\\n98.59\\n98.78\\n97.76\\n95.75\\n96.11\\n97.68\\n95.31\\n95.77\\n96.40\\nmix\\n99.44\\n98.31\\n98.47\\n99.32\\n98.37\\n98.51\\n98.74\\n97.70\\n95.68\\n96.04\\n97.65\\n95.27\\n95.73\\n96.35\\nﬁltered\\nfull\\n99.82\\n87.17\\n89.05\\n99.79\\n86.60\\n88.67\\n91.85\\n98.25\\n91.04\\n92.30\\n98.14\\n91.15\\n92.48\\n93.89\\nsent\\n96.97\\n97.22\\n97.19\\n99.09\\n98.43\\n98.53\\n97.91\\n96.60\\n92.81\\n93.47\\n97.94\\n95.86\\n96.26\\n95.49\\nmix\\n96.28\\n96.43\\n96.41\\n99.45\\n98.37\\n98.53\\n97.58\\n97.43\\n94.09\\n94.68\\n97.66\\n95.61\\n96.01\\n95.91\\nTrain ↓\\nGLTR Test-2\\nraw\\nfull\\n98.26\\n71.58\\n76.15\\n98.22\\n70.19\\n75.23\\n81.61\\n89.61\\n44.02\\n53.72\\n85.89\\n43.58\\n53.62\\n61.74\\nsent\\n86.26\\n88.18\\n87.96\\n87.72\\n88.23\\n88.19\\n87.76\\n84.49\\n71.79\\n74.01\\n84.06\\n70.29\\n72.90\\n76.26\\nmix\\n95.97\\n86.45\\n87.81\\n96.13\\n86.24\\n87.73\\n90.06\\n86.45\\n70.85\\n73.59\\n84.94\\n69.14\\n72.14\\n76.19\\nﬁltered\\nfull\\n98.31\\n70.91\\n75.65\\n98.30\\n69.48\\n74.72\\n81.23\\n89.46\\n58.69\\n64.52\\n86.51\\n55.45\\n62.18\\n69.47\\nsent\\n84.00\\n88.25\\n87.71\\n85.68\\n88.35\\n87.99\\n87.00\\n84.56\\n71.85\\n74.07\\n84.22\\n70.59\\n73.18\\n76.41\\nmix\\n95.36\\n86.73\\n87.97\\n95.60\\n86.56\\n87.92\\n90.02\\n86.30\\n71.00\\n73.70\\n84.98\\n69.45\\n72.40\\n76.31\\nTable 4: F1 scores (%) of different models on each testset, average of each language are reported.\\n5.4.1\\nWhich detector(s) is more useful? ML-based or DL-based? and Why?\\nAccording to Table 4, we can derive following conclusions:\\nFirstly, the robustness of RoBERTa-based-detector is better than GLTR. The F1-scores of\\nRoBERTa decrease slightly (1.5-2% in English datasets and 2-3% in Chinese datasets) when sentences\\nare split by comparing the leading diagonal elements in raw→raw and ﬁltered→ﬁltered. In contrast,\\nthe GLTR reduces signiﬁcantly by over 10% in English datasets, and above 15% in Chinese datasets.\\nAbove all, the RoBERTa-based-detector is more robust with anti-interference character. In contrast,\\nthe GLTR reduces signiﬁcantly by over 10% in English datasets, above 15% in Chinese datasets.\\nAbove all, the RoBERTa-based-detector is more robust with anti-interference character.\\nSecondly, RoBERTa-based-detector is not affected by indicating words.\\nThe F1-scores of\\nRoBERTa only slightly decreased by 0.03% in English full dataset, and 0.65% in Chinese full dataset,\\nas seen in the minus of relevant leading diagonal elements in raw→raw versus ﬁltered→ﬁltered. On\\nthe contrary, evaluations based on GLTR decrease by up to 3.1% on Chinese datasets, though tiny\\nrise on English datasets, indicating that GLTR is sensitive to indicating words, easily inﬂuenced by\\nthe patterns of ChatGPT.\\n12\\nLastly, RoBERTa-based-detector is effective in handling Out-Of-Distribution scenarios. When\\ncompared to the original model, it demonstrates a signiﬁcant decrease in performance on GLTR’s\\nOOD test datasets, with a drop of up to 28.8% on English datasets(ﬁltered-full→ﬁltered-full −\\nﬁltered-full→ﬁltered-sent) and 45.5% on Chinese datasets(raw-full→raw-full −raw-full→raw-sent).\\nHowever, RoBERTa maintains consistent performance with F1-scores varying by no more than 19%.\\n5.4.2\\nHow will the indicating words inﬂuence the detector?\\nWe ﬁrst collected a bunch of indicating words for both humans and ChatGPT. For example, ChatGPT’s\\nindicating words (or phrases) include \"AI assistant\", \"I’m sorry to hear that\", and \"There’re a few\\nsteps...\", etc. and humans’ indicating words may include \"Hmm\", \"Nope\", \"My view is\", etc. In the\\nﬁltered version, we remove all sentences in the answers that contain the indicating words for both\\nhumans and ChatGPT.\\nAccording to Table 4, removing the indicating words helps the models trained on full-text to per-\\nform better across different content granularities. For example, the RoBERTa-ﬁlter-full performs\\nsigniﬁcantly better than RoBERTa-raw-full in terms of sentence-level and mix-level evaluations,\\nimproving more than 3% F1 scores on average. However, the ﬁltering may slightly hurt the per-\\nformances of the models trained on sentences. This may be because the indicating words play a\\nbigger part in the sentence-level text compared with the full text. Removing the indicating words\\nmay make some sentences literally unable to be distinguished.\\n5.4.3\\nWhich granularity is more difﬁcult to detect? Full-text or sentence?\\nThrough the extensive experimental results in Table 5, we conclude that detecting ChatGPT gener-\\nated texts is more difﬁcult in a single sentence than in a full text. This conclusion can be proved\\nby the following two points: First, our results show that both English and Chinese sentence-based\\ndetectors (i.e., raw-sent and ﬁltered-sent versions) achieve satisfactory results w.r.t. the testing\\ntask of detecting either ChatGPT generated paragraphs or sentences, whereas the opposite is not\\ntrue——raw-full and ﬁltered-full are relatively inferior when detecting ChatGPT generated sentences.\\nIn other words, detectors trained on \"hard samples\" (i.e., sentence corpus) are much easier to solve\\nsimple task (i.e., detecting full corpus), while \"simple samples\" (i.e., full corpus) may be less useful\\nfor solving more difﬁcult task (i.e., sentence corpus).\\nSecond, we observe that although both full and sentence corpus are provided in the raw-mix and\\nﬁltered-mix versions, it is still more difﬁcult for them to detect single sentences generated by ChatGPT.\\nThis is even more obvious for the Chinese corpus, where the F1-score of raw-mix trained on the\\nChinese corpus is 94.09% for testing raw sentence answers, compared to that 97.43% for testing raw\\nfull answers. Similar results can be observed for the ﬁltered corpus, where F1-score of ﬁltered-mix is\\n95.61% for testing ﬁltered sentence answers, compared to its F1-score of 97.66% for testing ﬁltered\\nfull answers. One possible explanation is that the expression pattern of ChatGPT is more obvious\\n(therefore more easily detected) when paragraphs of text are provided, whereas it is more difﬁcult to\\ndetect generated single sentences.\\nEnglish\\nChinese\\nTest →\\nraw\\nﬁltered\\nAvg.\\nraw\\nﬁltered\\nAvg.\\nTrain ↓\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull-raw\\n99.82\\n81.89\\n84.67\\n99.72\\n81.00\\n84.07\\n88.53\\n98.79\\n83.64\\n86.32\\n98.57\\n82.77\\n85.85\\n89.32\\nsent-raw\\n99.40\\n98.43\\n98.56\\n99.24\\n98.47\\n98.59\\n98.78\\n97.76\\n95.75\\n96.11\\n97.68\\n95.31\\n95.77\\n96.40\\nmix-raw\\n99.44\\n98.31\\n98.47\\n99.32\\n98.37\\n98.51\\n98.74\\n97.70\\n95.68\\n96.04\\n97.65\\n95.27\\n95.73\\n96.35\\nfull-ﬁltered\\n99.82\\n87.17\\n89.05\\n99.79\\n86.60\\n88.67\\n91.85\\n98.25\\n91.04\\n92.30\\n98.14\\n91.15\\n92.48\\n93.89\\nsent-ﬁltered\\n96.97\\n97.22\\n97.19\\n99.09\\n98.43\\n98.53\\n97.91\\n96.60\\n92.81\\n93.47\\n97.94\\n95.86\\n96.26\\n95.49\\nmix-ﬁltered\\n96.28\\n96.43\\n96.41\\n99.45\\n98.37\\n98.53\\n97.58\\n97.43\\n94.09\\n94.68\\n97.66\\n95.61\\n96.01\\n95.91\\nTable 5: F1 scores (%) of RoBERTa models at full & sent & mix mode.\\n13\\n5.4.4\\nWhich corpus is more helpful for model training? Full-text, sentence, or mix of the\\ntwo?\\nWe ﬁnd that both English and Chinese RoBERTa-based detectors are more robust when ﬁne-\\ngrained corpus data is available in model training. The sentence-based detectors outperform\\nfull-based detectors w.r.t. F1-scores, while the latter can be signiﬁcantly improved when the sentence\\ncorpus is injected in model training, as we observe that mix-based detectors also achieve satisfactory\\nresults. For English corpus, raw-full only achieves 81.89% F1-score for testing sentence answers,\\nwhile raw-sent is signiﬁcantly better with 98.43% F1-score, as shown in Table 5. Moreover, the\\nrelatively inferior detection performance can be improved by injecting sentence answers into the\\ndetector, where we ﬁnd that raw-mix can also obtain signiﬁcant improvement (with 98.31% F1-score)\\nover the detectors trained on only full answers. Similar conclusions can be acquired for the ﬁltered\\nversions, where both ﬁltered-sent and ﬁltered-mix signiﬁcantly outperform ﬁltered-full version w.r.t.\\nF1-score, which holds for both English and Chinese corpus.\\nWe indicate that the above conclusions could also hold for other types of detectors like GLTR Test-2\\nfeature-based detectors, as is shown in Table 4. For GLTR Test-2, the average performance of F1-\\nscore of raw-full and ﬁltered-full is 61.74% and 69.47%, respectively, compared to that of raw-sent\\n76.26% and ﬁltered-sent 76.41%, where the performance of detectors trained on the mixed corpus is\\nclose to the sentence-based versions.\\nTaking into account the conclusions of the previous paragraph about the detection difﬁculty between\\nfull and sentence answers, we indicate that the ﬁne-grained corpus is helpful for distinguishing\\nChatGPT generated texts, as it additionally provides guidance and hints in model training for\\ndetecting the subtle patterns of ChatGPT hidden in single sentences.\\n5.4.5\\nWill a QA-style detector be more effective than a single-text detector?\\nTable 6 demonstrates the results of both raw-full and ﬁltered-full models across all test datasets.\\nOn English datasets, the QA model’s F1-scores are superior to that of the single model, except for\\ntwo full test datasets, where it averages 97.48% F1-scores and surpasses single model by 5.63%.\\nThere exist some differences in Chinese datasets, where the single model outperforms QA in raw-full\\ntrain dataset. However, the QA model still yields the best evaluation at 94.22%.\\nIn conclusion, the QA model is generally more effective than the single model and is suitable\\nfor ﬁltered scenarios. And the QA training makes models more robust to the sentence inputs.\\nEnglish\\nChinese\\nTest →\\nraw\\nﬁltered\\nAvg.\\nraw\\nﬁltered\\nAvg.\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nTrain →raw - full\\nSingle\\n99.82\\n81.89\\n84.67\\n99.72\\n81.00\\n84.07\\n88.53\\n98.79\\n83.64\\n86.32\\n98.57\\n82.77\\n85.85\\n89.32\\nQA\\n99.84\\n92.68\\n93.70\\n99.75\\n92.34\\n93.46\\n95.30\\n98.99\\n80.56\\n83.85\\n98.73\\n80.24\\n83.89\\n87.71\\nTrain →ﬁltered - full\\nSingle\\n99.82\\n87.17\\n89.05\\n99.79\\n86.60\\n88.67\\n91.85\\n98.25\\n91.04\\n92.30\\n98.14\\n91.15\\n92.48\\n93.89\\nQA\\n99.70\\n96.14\\n96.64\\n99.70\\n96.07\\n96.61\\n97.48\\n97.29\\n92.10\\n93.01\\n97.18\\n92.40\\n93.31\\n94.22\\nTable 6: F1 scores (%) of RoBERTa models trained with QA & Single settings.\\n5.4.6\\nWhich data sources are more difﬁcult for the ChatGPT detectors? and What are the\\nconditions that make it easier to detect ChatGPT?\\nAs shown in Table 7, the evaluation results based on ﬁltered-full model are separated by various\\nsources in our HC3 dataset.\\nOn the English datasets, the F1-scores for human answers are slightly higher than those for ChatGPT\\nwithout any exceptions, regardless of whether RoBERTa or GLTR is used on full-text test datasets.\\nHowever, the F1-scores for ChatGPT are highly inconsistent on transferring test datasets particu-\\nlarly open-qa dataset with varying performance. In terms of data resource, reddit-eli5 and\\nfinance-en has higher values, while wiki-csai poses a challenge for detectors.\\n14\\nOn the Chinese datasets, the F1-scores of humans and ChatGPT are comparable with no signiﬁcant\\ndifference. This suggests that the difﬁculty in detecting ChatGPT depends on the data source. It\\nis observed that open-qa and baike have better performance, whereas the nlpcc-dbqa has\\nlower performance.\\nAbove all, the evaluations on Chinese dataset show more stability on transferring test dataset compared\\nto the English datasets. Furthermore, it’s evident that the F1-scores of ChatGPT are lower than those\\nof human answers, regardless of whether the dataset is English or Chinese. This indicates that\\nChatGPT’s detector relies more heavily on In-Distribution models.\\nModel\\nTest\\nF1-hu\\nF1-ch\\nF1-hu\\nF1-ch\\nF1-hu\\nF1-ch\\nF1-hu\\nF1-ch\\nF1-hu\\nF1-ch\\nEnglish\\nﬁnance\\nmedicine\\nopen_qa\\nreddit_eli5\\nwiki_csai\\nRoBERTa\\nfull\\n99.34\\n99.28\\n99.69\\n99.62\\n99.53\\n98.60\\n100.00\\n100.00\\n96.59\\n96.37\\nsent\\n78.84\\n85.84\\n84.06\\n80.45\\n70.74\\n26.78\\n77.27\\n93.31\\n68.91\\n84.12\\nGLTR\\nfull\\n97.50\\n97.37\\n98.28\\n97.96\\n92.68\\n82.20\\n98.22\\n99.40\\n95.76\\n95.72\\nsent\\n46.60\\n75.26\\n45.41\\n61.72\\n42.01\\n17.81\\n38.12\\n87.05\\n39.24\\n76.94\\nChinese\\nﬁnance\\nlaw\\nopen_qa\\nnlpcc_dbqa\\nbaike\\nRoBERTa\\nfull\\n98.87\\n97.99\\n97.78\\n98.50\\n98.75\\n99.33\\n97.42\\n95.42\\n94.61\\n93.99\\nsent\\n95.00\\n80.46\\n93.77\\n86.23\\n91.17\\n93.77\\n90.10\\n63.29\\n86.08\\n88.88\\nGLTR\\nfull\\n86.67\\n80.42\\n82.41\\n88.89\\n85.75\\n93.15\\n77.25\\n69.78\\n81.62\\n77.91\\nsent\\n36.91\\n32.80\\n33.99\\n46.22\\n36.45\\n75.21\\n46.39\\n27.50\\n48.10\\n71.72\\nTable 7: Human (F1-hu) and ChatGPT (F1-ch) detection F1 scores (%) w.r.t. different data source,\\nmodels are trained on ﬁltered full text, tested on ﬁltered full and sent. On HC3-Chinese, we omitted the\\nresults of medicine and psychology domains, which are similar to ﬁnance and open_qa, respectively.\\n6\\nConclusion\\nIn this work, we propose the HC3 (Human ChatGPT Comparison Corpus) dataset, which consists\\nof nearly 40K questions and their corresponding human/ChatGPT answers. Based on the HC3\\ndataset, we conduct extensive studies including human evaluations, linguistic analysis, and content\\ndetection experiments. The human evaluations and linguistics analysis provide us insights into the\\nimplicit differences between humans and ChatGPT, which motivate our thoughts on LLMs’ future\\ndirections. The ChatGPT content detection experiments illustrate some important conclusions that\\ncan provide beneﬁcial guides to the research and development of AIGC-detection tools. We make\\nall our data, code, and models publicly available to facilitate related research and applications at\\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\\n7\\nLimitations\\nDespite our comprehensive analysis of ChatGPT, there are still several limitations in the current\\npaper, which will be considered for improvement in our future work:\\n1. Despite our efforts in data collection, the amount and range of collected data are still not\\nenough and the data from different sources are unbalanced, due to limited time and resources.\\nTo make more accurate linguistic analyses and content detection, more data with different\\nstyles, sources, and languages are needed;\\n2. Currently, all the collected ChatGPT answers are generated without special prompts.\\nTherefore, the analysis and conclusions in this paper are built upon ChatGPT’s most general\\nstyle/state. For example, using special prompts such as \"Pretending you are Shakespeare...\"\\ncan generate content that bypasses our detectors or make the conclusions in this paper\\nuntenable;\\n15\\n3. ChatGPT (perhaps) is mainly trained on English corpus while less on Chinese. Therefore,\\nthe conclusions drawn from the HC3-Chinese dataset may not always be precise.\\nAcknowledgments\\nWe would like to thank the volunteers that participated in our human evaluations, many of them are\\nour good friends and dear family members. We would like to thank Junhui Zhu (BLCU-ICALL)\\nfor the valuable discussions on linguistic analysis. Biyang Guo would like to thank Prof. Hailiang\\nHuang and Prof. Songqiao Han (AI Lab, SUFE) for providing insightful feedback on the topics and\\ndirections for this project. Xin Zhang would like to thank Yu Zhao (NeXt, NUS and CIC, TJU) for\\nsharing the OpenAI account. Finally, we thank all team members of this project for their unique\\ncontributions. We together make this possible.\\n16\\nReferences\\n[1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a\\nlaboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\\n[2] Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. Xlm-t: Multilingual\\nlanguage models in twitter for sentiment analysis and beyond. In Proceedings of the Language\\nResources and Evaluation Conference, pages 258–266, Marseille, France, June 2022. European\\nLanguage Resources Association.\\n[3] Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python:\\nAnalyzing Text with the Natural Language Toolkit. O’Reilly Media, Inc., 2009.\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n[6] Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng Wang, Yue Yang, Jiaqi Zeng,\\nRuisi Zhang, Ruoyu Zhang, Meng Zhou, Penghui Zhu, and Pengtao Xie. Meddialog: a\\nlarge-scale medical dialogue dataset. arXiv preprint arXiv:2004.03329, 2020.\\n[7] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\\nDeep reinforcement learning from human preferences. neural information processing systems,\\n2017.\\n[8] Nan Duan. Overview of the nlpcc-iccpol 2016 shared task: Open domain chinese question\\nanswering. In Natural Language Understanding and Intelligent Applications, pages 942–948,\\nCham, 2016. Springer International Publishing.\\n[9] Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi.\\nTweepfake: About detecting deepfake tweets. Plos one, 16(5):e0251415, 2021.\\n[10] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\\nELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluís Màrquez,\\neditors, Proceedings of the 57th Conference of the Association for Computational Linguistics,\\nACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3558–3567.\\nAssociation for Computational Linguistics, 2019.\\n[11] Zhihui Fang. The language demands of science reading in middle school. International journal\\nof science education, 28(5):491–520, 2006.\\n[12] Yao Fu, Hao Peng, and Tushar Khot. How does gpt obtain its ability? tracing emergent abilities\\nof language models to their sources. Yao Fu’s Notion, Dec 2022.\\n[13] Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and\\nvisualization of generated text. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics: System Demonstrations, pages 111–116, Florence, Italy, July\\n2019. Association for Computational Linguistics.\\n[14] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey,\\nSamuel A Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational\\nprinciples for language processing in humans and deep language models. Nature neuroscience,\\n25(3):369–380, 2022.\\n[15] Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han, Hailiang Huang, Nan Duan, and\\nWeizhu Chen. Genius: Sketch-based language model pre-training via extreme and selective\\nmasking for text generation and augmentation. arXiv preprint arXiv:2211.10330, 2022.\\n[16] Biyang Guo, Songqiao Han, and Hailiang Huang. Selective text augmentation with word roles\\nfor low-resource text classiﬁcation. arXiv preprint arXiv:2209.01560, 2022.\\n[17] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly\\ndetection benchmark. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n17\\n[18] Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, and Edward Gibson. A\\nﬁne-grained comparison of pragmatic language understanding in humans and language models.\\narXiv preprint arXiv:2212.06801, 2022.\\n[19] Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks Lakshmanan, V.S. Automatic detection\\nof machine generated text: A critical survey. In Proceedings of the 28th International Conference\\non Computational Linguistics, pages 2296–2309, Barcelona, Spain (Online), December 2020.\\nInternational Committee on Computational Linguistics.\\n[20] Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa\\nStüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, et al. Chatgpt\\nmakes medicine easy to swallow: An exploratory case study on simpliﬁed radiology reports.\\narXiv preprint arXiv:2212.14882, 2022.\\n[21] Michael R King. The future of ai in medicine: a perspective from a chatbot, 2022.\\n[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT\\npretraining approach. CoRR, abs/1907.11692, 2019.\\n[23] Macedo Maia, Siegfried Handschuh, Andr’e Freitas, Brian Davis, Ross McDermott, Manel\\nZarrouk, and Alexandra Balahur. Www’18 open challenge: Financial opinion mining and\\nquestion answering. In Companion Proceedings of the The Web Conference 2018, WWW ’18,\\npage 1941–1942, Republic and Canton of Geneva, CHE, 2018. International World Wide Web\\nConferences Steering Committee.\\n[24] William Nagy and Dianna Townsend. Words as tools: Learning academic vocabulary as\\nlanguage acquisition. Reading research quarterly, 47(1):91–108, 2012.\\n[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\\nfollow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\\n[26] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\\nP. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\\nM. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine\\nLearning Research, 12:2825–2830, 2011.\\n[27] Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Abdullah Rehman, Yoonjin Kim, Paran-\\ntapa Bhattacharya, Mobin Javed, and Bimal Viswanath. Deepfake text detection: Limitations\\nand opportunities. In Proc. of IEEE S&P, 2023.\\n[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[29] Mary J Schleppegrell. The language of schooling: A functional linguistics perspective. Rout-\\nledge, 2004.\\n[30] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu,\\nAlec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and\\nthe social impacts of language models. arXiv preprint arXiv:1908.09203, 2019.\\n[31] SophonPlus. Chinesenlpcorpus. https://github.com/SophonPlus/ChineseNlpCorpus,\\n2019.\\n[32] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec\\nRadford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback.\\nneural information processing systems, 2020.\\n[33] Teo Susnjak. Chatgpt: The end of online exam integrity? arXiv preprint arXiv:2212.09292,\\n2022.\\n[34] Alan M Turing. Computing machinery and intelligence. In Parsing the turing test, pages 23–65.\\nSpringer, 2009.\\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems, 30, 2017.\\n18\\n[36] Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong,\\nJunqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu,\\nXinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu,\\nZhongshen Zeng, Chongpei Chen, Ruyi Gan, and Jiaxing Zhang. Fengshenbang 1.0: Being the\\nfoundation of chinese cognitive intelligence. CoRR, abs/2209.02970, 2022.\\n[37] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art\\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing: System Demonstrations, pages 38–45, Online, October 2020.\\nAssociation for Computational Linguistics.\\n[38] Bright Xu. Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019.\\n[39] Yi Yang, Scott Wen-tau Yih, and Chris Meek. Wikiqa: A challenge dataset for open-domain\\nquestion answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural\\nLanguage Processing. ACL - Association for Computational Linguistics, September 2015.\\n[40] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-\\nand-write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on\\nArtiﬁcial Intelligence, volume 33, pages 7378–7385, 2019.\\nA\\nAppendix\\nA.1\\nHC3 Dataset Splits Creation\\nWe create 5 and 7 splits for HC3 English and Chinese, respectively. Most of the data come from the\\npublicly available Question-Answering (QA) datasets, where details are listed in the following. For\\nthese QA data, we directly input the questions to ChatGPT and collect at least one answer.\\nWe also crawled some wiki concepts and explanations from Wikipedia and BaiduBaike, where\\nexplanations are treated as human expert answers and concepts are used to construct the questions,\\ndetails ref to bellow paragraphs.\\nFor HC3-English, we create ﬁve dataset splits:\\n1. reddit_eli5. Sampled from the ELI5 dataset [10].\\n2. open_qa. Sampled from the WikiQA dataset [39].\\n3. wiki_csai. We collected the descriptions of hundreds of computer science-related concepts\\nfrom Wikipedia13 as the human experts’ answers to questions like \"Please explain what is\\n<concept>?\"\\n4. medicine. Sampled from the Medical Dialog dataset [6].\\n5. finance. Sampled from the FiQA dataset [23], which is built by crawling StackExchange14\\nposts under the Investment topic.\\nFor HC3-Chinese, we create seven dataset splits:\\n1. open_qa. Sampled from the WebTextQA and BaikeQA corpus in [38].\\n2. baike. We collected the descriptions of more than a thousand information science-related\\nconcepts from BaiduBaike15 as the human experts’ answers to questions like \"我有一个计\\n算机相关的问题，请用中文回答，什么是<concept>\"\\n3. nlpcc_dbqa. Sampled from the NLPCC-DBQA dataset [8].\\n4. medicine. Sampled from the Medical Dialog dataset [6].\\n5. finance. Sampled from the FinanceZhidao dataset [31].\\n13https://www.wikipedia.org/\\n14https://stackexchange.com/\\n15https://baike.baidu.com/\\n19\\n6. psychology Sampled from a public Chinese Psychological Question Answering Dataset16.\\n7. law. Sampled from the LegalQA dataset17.\\nA.2\\nAdditional Results\\nHere we demonstrate the additional results of dependency relations for the Chinese corpus, as is\\nshown in Figure 6. The conclusion is basically consistent with the main paper.\\npunct\\nadvmod\\nconj\\ndep\\ncompound:nn\\ndobj\\nnsubj\\ncase\\nROOT\\namod\\nccomp\\nnmod:prep\\nmark\\nnmod:assmod\\nacl\\ncop\\naux:modalcc\\nmark:clfdet\\nnummod\\naux:asp\\ndiscourseneg\\nxcomp\\nnmod\\nadvmod:rcomp\\nnmod:tmod\\nadvmod:loc\\naux:prtmodetc\\ncompound:vc\\nparataxis:prnmod\\nnmod:range\\nnmod:topic\\naux:ba\\nauxpass\\nadvmod:dvp\\nadvcl:loc\\nappos\\nname\\nnmod:poss\\namod:ordmod\\nnsubjpass\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\nProportion (%)\\n14.8\\n8.9\\n8.2\\n7.6\\n7.3\\n7.2\\n7.1\\n6.9\\n4.3\\n2.7\\n2.5\\n2.5\\n2.3\\n2.1\\n1.5\\n1.5\\n1.4\\n1.4\\n1.3\\n1.0\\n0.9\\n0.8\\n0.8\\n0.8\\n0.7\\n0.4\\n0.4\\n0.4\\n0.3\\n0.3\\n0.2\\n0.2\\n0.2\\n0.2\\n0.2\\n0.2\\n0.2\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.0\\n0.0\\n13.3\\n7.4\\n7.8\\n5.3\\n7.5\\n8.7\\n6.6\\n8.7\\n4.6\\n3.1\\n2.9\\n3.0\\n2.0\\n2.9\\n1.2\\n1.5\\n2.8\\n2.8\\n0.9\\n1.3\\n0.7\\n0.4\\n0.3\\n0.4\\n0.6\\n0.5\\n0.2\\n0.1\\n0.2\\n0.6\\n0.3\\n0.1\\n0.2\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.0\\n0.0\\n0.0\\nDependency Relation Comparison (Zh)\\nHuman\\nChatGPT\\npunct\\nadvmod\\nconj\\ndep\\ncompound:nn\\ndobj\\nnsubj\\ncase\\nROOT\\namod\\nccomp\\nnmod:prep\\nmark\\nnmod:assmod\\nacl\\ncop\\naux:modalcc\\nmark:clfdet\\nnummod\\naux:asp\\ndiscourseneg\\nxcomp\\nnmod\\nadvmod:rcomp\\nnmod:tmod\\nadvmod:loc\\naux:prtmodetc\\ncompound:vc\\nparataxis:prnmod\\nnmod:range\\nnmod:topic\\naux:ba\\nauxpass\\nadvmod:dvp\\nadvcl:loc\\nappos\\nname\\nnmod:poss\\namod:ordmod\\nnsubjpass\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nProportion (%)\\n9.6\\n2.6\\n11.7\\n5.3\\n1.6\\n3.4\\n3.0\\n1.8\\n0.0\\n1.8\\n4.8\\n3.8\\n1.8\\n2.5\\n4.4\\n4.2\\n2.1\\n2.3\\n1.0\\n1.7\\n3.0\\n1.1\\n2.2\\n1.6\\n1.6\\n3.4\\n1.1\\n3.8\\n4.1\\n1.1\\n1.0\\n1.0\\n4.1\\n2.2\\n5.9\\n4.6\\n1.8\\n2.3\\n6.3\\n1.1\\n1.1\\n1.0\\n1.2\\n2.5\\n8.2\\n3.2\\n6.7\\n5.9\\n1.5\\n3.4\\n3.3\\n1.8\\n0.0\\n2.0\\n3.8\\n3.7\\n1.7\\n2.8\\n4.1\\n4.8\\n2.4\\n2.1\\n1.0\\n1.6\\n3.8\\n1.0\\n1.5\\n1.7\\n1.6\\n3.3\\n1.0\\n3.7\\n4.5\\n1.0\\n1.0\\n1.0\\n4.3\\n2.1\\n7.4\\n3.9\\n1.4\\n2.2\\n6.0\\n1.0\\n1.1\\n1.1\\n1.2\\n2.2\\nDependency Distance Comparison (Zh)\\nHuman\\nChatGPT\\nFigure 6: Top-30 dependency relations (upper) and corresponding dependency distances (lower)\\ncomparison between human and ChatGPT answers in the HC3-Chinese. Results are sorted by\\nrelations proportion of human answers.\\nOther detailed results, including vocabulary features, sentiment analyses, and dependency parsing\\nresults for each data source are all available at our project GitHub repository at https://github.\\ncom/Hello-SimpleAI/chatgpt-comparison-detection.\\nA.3\\nHuman Evaluations Examples\\nFor evaluation examples of our human evaluations, please visit our project GitHub repository at\\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\\n16https://aistudio.baidu.com/aistudio/datasetdetail/38489\\n17https://github.com/siatnlp/LegalQA\\n20\\n'),\n",
       " Document(metadata={'Published': '2023-11-02', 'Title': 'Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine', 'Authors': 'Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Shuming Shi, Zhaopeng Tu', 'Summary': 'This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well with minor performance differences. By evaluating on a number of benchmark test sets, we find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but exhibits good results on spoken language. Further, we explore an interesting strategy named $\\\\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably. With the launch of the GPT-4 engine, the translation performance of ChatGPT is significantly boosted, becoming comparable to commercial translation products, even for distant languages. Human analysis on Google Translate and ChatGPT suggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and mis-translation errors while that with GPT-4 makes the least errors. In other words, ChatGPT has already become a good translator. Please refer to our Github project for more details: https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator'}, page_content='Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine\\nWenxiang Jiao∗Wenxuan Wang Jen-tse Huang Xing Wang\\nShuming Shi Zhaopeng Tu\\nTencent AI Lab\\n{joelwxjiao}@tencent.com\\nAbstract\\nThis report provides a preliminary evaluation\\nof ChatGPT for machine translation, includ-\\ning translation prompt, multilingual transla-\\ntion, and translation robustness.\\nWe adopt\\nthe prompts advised by ChatGPT to trigger\\nits translation ability and find that the candi-\\ndate prompts generally work well with minor\\nperformance differences. By evaluating on a\\nnumber of benchmark test sets, we find that\\nChatGPT performs competitively with commer-\\ncial translation products (e.g., Google Trans-\\nlate) on high-resource European languages but\\nlags behind significantly on low-resource or\\ndistant languages. As for the translation robust-\\nness, ChatGPT does not perform as well as the\\ncommercial systems on biomedical abstracts or\\nReddit comments but exhibits good results on\\nspoken language. Further, we explore an inter-\\nesting strategy named pivot prompting for dis-\\ntant languages, which asks ChatGPT to trans-\\nlate the source sentence into a high-resource\\npivot language before into the target language,\\nimproving the translation performance notice-\\nably. With the launch of the GPT-4 engine,\\nthe translation performance of ChatGPT is sig-\\nnificantly boosted, becoming comparable to\\ncommercial translation products, even for dis-\\ntant languages. Human analysis on Google\\nTranslate and ChatGPT suggests that ChatGPT\\nwith GPT-3.5 tends to generate more halluci-\\nnations and mis-translation errors while that\\nwith GPT-4 makes the least errors. In other\\nwords, ChatGPT has already become a good\\ntranslator. Please refer to our Github project for\\nmore details: https://github.com/wxjiao/\\nIs-ChatGPT-A-Good-Translator.\\n1\\nIntroduction\\nChatGPT1 is an intelligent chatting machine devel-\\noped by OpenAI upon the InstructGPT (Ouyang\\net al., 2022), which is trained to follow an instruc-\\ntion in a prompt and provide a detailed response.\\n∗Corresponding author.\\n1https://chat.openai.com\\nFigure 1: Prompts advised by ChatGPT for machine\\ntranslation (Date: 2022.12.16).\\nAccording to the official statement, ChatGPT is\\nable to answer followup questions, admit its mis-\\ntakes, challenge incorrect premises, and reject in-\\nappropriate requests due to the dialogue format.\\nIt integrates various abilities of natural language\\nprocessing, including question answering, story-\\ntelling, logic reasoning, code debugging, machine\\ntranslation, and so on. We are particularly inter-\\nested in how ChatGPT performs for machine trans-\\nlation tasks, especially the gap between ChatGPT\\nand commercial translation products (e.g., Google\\nTranslate, DeepL Translate).\\nIn this report, we provide a preliminary study of\\nChatGPT on machine translation, which to our best\\nknowledge is also the first one since the release of\\nChatGPT. Specifically, we focus on three aspects:\\n• Translation Prompt: ChatGPT is essentially a\\nlarge language model, which needs prompts as\\nguidance to trigger its translation ability. The\\nstyle of prompts may affect the quality of trans-\\nlation outputs. For example, how to mention\\nthe source or target language information mat-\\nters in multilingual machine translation models,\\nwhich is usually solved by attaching language\\ntokens (Johnson et al., 2017; Fan et al., 2021).\\n• Multilingual Translation: ChatGPT is a single\\narXiv:2301.08745v4  [cs.CL]  2 Nov 2023\\nmodel handling various NLP tasks and covering\\ndifferent languages, which can be considered a\\nunified multilingual machine translation model.\\nThus, we are curious about how ChatGPT per-\\nforms on different language pairs considering\\nboth the resource difference (e.g., high vs. low)\\nand language family (e.g., European vs. Asian).\\n• Translation Robustness: ChatGPT is developed\\nupon GPT3, which was trained on large-scale\\ndatasets that cover various domains. Therefore,\\nwe wonder if it can perform robustly well on\\ndomain-specific or even noisy sentences.\\nTo trigger the translation ability of ChatGPT,\\nwe ask ChatGPT itself for advice and obtain three\\ncandidate translation prompts. By evaluating on\\nthe Chinese⇒English translation task, we find that\\nthe candidate prompts generally work well and\\nshow minor performance differences. Neverthe-\\nless, we adopt the best-performing prompt for the\\nrest parts of the study. By evaluating the transla-\\ntion among four selected languages on the Flores-\\n101 test sets, we find that ChatGPT performs com-\\npetitively with commercial translation products\\n(e.g., Google Translate) on high-resource Euro-\\npean languages but lags behind significantly on\\nlow-resource or distant languages. As for the trans-\\nlation robustness, results on three robustness sets\\nsuggest that ChatGPT does not perform as well as\\nthe commercial systems on biomedical abstracts\\nor Reddit comments but exhibits good results on\\nspoken language.\\nFurther, we have a discussion on how to im-\\nprove ChatGPT for machine translation. On one\\nhand, we explore an interesting strategy named\\npivot prompting for distant languages, which asks\\nChatGPT to translate the source sentence into a\\nhigh-resource pivot language before into the target\\nlanguage, improving the translation performance\\nnoticeably. On the other hand, with an improved\\nengine GPT-4 (OpenAI, 2023)2 launched on March\\n15, 2023, we re-evaluate the translation ability of\\nChatGPT and observe a significant boost of per-\\nformance. The translation performance of Chat-\\nGPT becomes comparable to commercial transla-\\ntion products, even for distant languages. Exten-\\nsive analysis on Google Translate and ChatGPT\\nsuggests that ChatGPT with GPT-3.5 tend to gener-\\nate more hallucinations and more mis-translation\\nerrors while that with GPT-4 makes the least errors.\\n2https://openai.com/research/gpt-4\\nTable 1: Information of adopted test sets.\\nTest Set\\nDirection\\nDomain\\nSize\\nFlores-101\\nAny\\nGeneral\\n1012\\nWMT19 Bio\\nDe⇒En\\nBiomedical\\n373\\nWMT20 Rob2\\nEn⇒Ja\\nReddit\\n1376\\nJa⇒En\\n997\\nWMT20 Rob3 De⇒En Common Voice 5609\\nIn other words, ChatGPT has already become a\\ngood translator with GPT-4 as the engine!\\n2\\nChatGPT for Machine Translation\\n2.1\\nEvaluation Setting\\nWe provide a brief introduction of the evaluation\\nsetting, which mainly includes the compared base-\\nlines and test data.\\nBaselines.\\nWe compare ChatGPT with three com-\\nmercial translation products, namely, Google Trans-\\nlate3, DeepL Translate4, and Tencent TranSmart5.\\nSo far, the three commercial systems support trans-\\nlation in 133, 29, and 16 languages, respectively.\\nBy default, the results in this report come from\\nthe ChatGPT version on 2022.12.16 . For new re-\\nsults, we will mark the updated version information\\ncorrespondingly.\\nData.\\nFor multilingual translation, we evalu-\\nate the above translation systems on the Flores-\\n101 (Goyal et al., 2021)6 test sets, which consists\\nof 1012 sentences translated into 101 languages.\\nTo test the translation robustness, we adopt the test\\nset of WMT19 Biomedical Translation Task (Baw-\\nden et al., 2019, i.e., Bio) and the set2 and set3 of\\nWMT20 Robustness Task (Specia et al., 2020, i.e.,\\nRob2 and Rob3). We obtain the first two test sets\\nthrough SacreBLEU and the third pre-processed by\\nWang et al. (2021)7. Table 1 lists the information\\nof these test sets. Since this empirical study was\\nconducted upon the very early release of ChatGPT,\\nwe were only able to access it through webpage,\\nwhich cannot respond to large batches. As a result,\\nobtaining the translation results from ChatGPT is\\ntime-consuming. Therefore, we randomly sample\\n50 sentences from each set for evaluation.\\n3https://translate.google.com\\n4https://www.deepl.com/translator\\n5https://transmart.qq.com/zh-CN/index\\n6https://github.com/facebookresearch/flores\\n7https://github.com/hsing-wang/WMT2020_\\nBioMedical/tree/master/Bio-18-19-testset\\nTable 2: Candidate translation prompts.\\nTranslation Prompt\\nTP1\\nTranslate these sentences from\\n[SRC] to [TGT]:\\nTP2\\nAnswer with no quotes. What do\\nthese sentences mean in [TGT]?\\nTP3\\nPlease\\nprovide\\nthe\\n[TGT]\\ntranslation for these sentences:\\nMetric.\\nWe adopt the mostly used BLEU\\nscore (Papineni et al., 2002) as our primary\\nmetric and also report ChrF++ (Popovi´c, 2017)\\nand TER (Snover et al., 2006) in some cases.\\nThese three metrics are all supported by Sacre-\\nBLEU (Post, 2018)8.\\n2.2\\nTranslation Prompts\\nTo design the prompts for triggering the machine\\ntranslation ability of ChatGPT, we seek inspiration\\nfrom ChatGPT by asking it for advice. Specifically,\\nwe ask ChatGPT with the following prompt:\\nProvide ten concise prompts or\\ntemplates\\nthat\\ncan\\nmake\\nyou\\ntranslate.\\nand obtain the results as shown in Figure 1. The\\ngenerated prompts look reasonable but share simi-\\nlar formats. Thus, we summarize them into three\\ncandidate prompts as shown in Table 2, where\\n[SRC] and [TGT] represent the source and target\\nlanguages of translation. Note that we add an extra\\ncommand into TP2 to ask ChatGPT not to gen-\\nerate double quotes around the translation, which\\noften occurs with the original format. Nevertheless,\\nit is still unstable such that sentences in a batch\\n(in multiple lines) are translated into a single line\\noccasionally.\\nWe compare the three different candidate\\nprompts on the Chinese-to-English (Zh⇒En) trans-\\nlation task with the test set from Flores-101. Ta-\\nble 3 shows the results of ChatGPT and three com-\\nmercial systems. While ChatGPT provides rea-\\nsonably good translations, it still lags behind the\\nbaselines by at least 5.0 BLEU points. Concerning\\nthe three candidate prompts, TP3 performs the best\\nin terms of all the three metrics. Thus, we use TP3\\nthroughout this report by default.\\nTable 3: Comparison of different prompts for ChatGPT\\nto perform Chinese-to-English (Zh⇒En) translation.\\nSystem\\nBLEU↑\\nChrF++↑\\nTER↓\\nGoogle\\n31.66\\n57.09\\n56.21\\nDeepL\\n31.22\\n56.74\\n57.84\\nTencent\\n29.69\\n56.24\\n57.16\\nChatGPT w/ TP1\\n23.25\\n53.07\\n66.03\\nChatGPT w/ TP2\\n24.54\\n53.05\\n63.79\\nChatGPT w/ TP3\\n24.73\\n53.71\\n62.84\\n2.3\\nMultilingual Translation\\nWe select four languages to evaluate the capabil-\\nity of ChatGPT in multilingual translation, includ-\\ning German (De), English (En), Romanian (Ro),\\nand Chinese (Zh), which are commonly adopted in\\nboth research (Wang et al., 2022a; Jiao et al., 2021,\\n2022b) and competitions (Bojar et al., 2016; Farhad\\net al., 2021). The first three languages come from\\nthe same family with Latin scripts while the last\\nis from another family with Chinese scripts (Fan\\net al., 2021). We test the translation performance\\nbetween any two languages, which involves 12 di-\\nrections in total. For clarity and comparison, we\\nreport the BLEU scores and the improvement or\\ndrop of performance (i.e., +/-) relative to Google\\nTranslate. Table 4 presents the results.\\nResource Difference.\\nWe consider the resource\\ndifference of languages in the same family. In ma-\\nchine translation, German⇔English translation is\\nusually regarded as a high-resource task supported\\nby over ten million sentence pairs (Farhad et al.,\\n2021) while Romanian⇔English translation is sup-\\nported by much less data (Bojar et al., 2016). This\\nresource difference can also be indicated by the\\ndata statistics9 of GPT-3 (Brown et al., 2020), al-\\nthough we do not know the data information of\\nChatGPT. As shown in Table 4, ChatGPT per-\\nforms competitively with Google Translate and\\nDeepL Translate for both German⇒English and\\nEnglish⇒German translations. However, it lags\\nbehind them significantly on Romanian⇒English\\nand English⇒Romanian. Specifically, ChatGPT\\nobtains a BLEU score on English⇒Romanian that\\nis 46.4% lower than Google Translate and the\\nvalue is 10.3% on Romanian⇒English. We spec-\\nulate that the huge resource difference of mono-\\n8https://github.com/mjpost/sacrebleu\\n9https://github.com/openai/gpt-3/tree/master/\\ndataset_statistics\\nTable 4: Performance of ChatGPT for multilingual translation.\\nSystem\\nDe-En\\nRo-En\\nZh-En\\n⇒\\n⇐\\n⇒\\n⇐\\n⇒\\n⇐\\nGoogle\\n45.04\\n41.16\\n50.12\\n46.03\\n31.66\\n43.58\\nDeepL\\n49.23(+9.3%)\\n41.46(+0.7%)\\n50.61(+0.9%)\\n48.39(+5.1%)\\n31.22(-1.3%)\\n44.31(+1.6%)\\nTencent\\nn/a\\nn/a\\nn/a\\nn/a\\n29.69(-6.2%)\\n46.06(+5.6%)\\nChatGPT\\n43.71(-2.9%)\\n38.87(-5.5%)\\n44.95(-10.3%)\\n24.85(-46.0%)\\n24.73(-21.8%)\\n38.27(-12.1%)\\nSystem\\nDe-Zh\\nRo-Zh\\nDe-Ro\\n⇒\\n⇐\\n⇒\\n⇐\\n⇒\\n⇐\\nGoogle\\n38.71\\n21.68\\n39.05\\n25.59\\n33.31\\n32.27\\nDeepL\\n40.46(+4.5%)\\n22.82(+5.2%)\\n38.95(-0.2%)\\n25.39(-0.7%)\\n35.19(+5.6%)\\n34.27(+6.1%)\\nTencent\\n40.66(+5.0%)\\n19.44(-10.3%)\\nn/a\\nn/a\\nn/a\\nn/a\\nChatGPT\\n34.46(-10.9%)\\n19.80(-8.6%)\\n30.84(-21.0%)\\n19.17(-25.0%)\\n33.38(+0.2%)\\n29.89(-7.3%)\\nlingual data between English and Romanian lim-\\nits the language modeling capability of Roma-\\nnian, which partially explains the poor perfor-\\nmance on English⇒Romanian. On the contrary,\\nRomanian⇒English can benefit from the strong\\nlanguage modeling capability of English such that\\nthe resource gap of parallel data can be somewhat\\ncompensated.\\nLanguage Family.\\nWe also take the impact of\\nlanguage families into account. In machine trans-\\nlation, translating between different language fam-\\nilies is often considered harder than that within\\nthe same language family, due to the differ-\\nent cultures and writing scripts.\\nBy compar-\\ning German⇔English with Chinese⇔English or\\nGerman⇔Chinese translation, we find that the gap\\nbetween ChatGPT and the commercial systems be-\\ncomes larger. We attribute to the better knowledge\\ntransfer within the same family (i.e., from English\\nto German) than between different families (e.g.,\\nfrom English to Chinese). For language pairs that\\nare both low-resource and from different families\\n(e.g., Romanian⇔Chinese), the performance gap\\ncan be further enlarged (Wang et al., 2022b). Since\\nChatGPT handles different tasks in one model, low-\\nresource translation tasks not only compete with\\nhigh-resource translation tasks (Jiao et al., 2022a),\\nbut also with other NLP tasks for the model capac-\\nity, which explains their poor performance.\\n2.4\\nTranslation Robustness\\nWe further evaluate the translation robustness of\\nChatGPT on the WMT19 Bio and WMT20 Rob2\\nTable 5: Performance of ChatGPT for translation robust-\\nness on domain-specific or noisy test data.\\nSystem\\nW19 Bio\\nW20 Rob2\\nW20 Rob3\\nDe⇒En En⇒Ja Ja⇒En\\nDe⇒En\\nGoogle\\n37.83\\n29.72\\n19.21\\n42.91\\nDeepL\\n37.13\\n26.25\\n19.83\\n41.29\\nChatGPT\\n33.22\\n22.36\\n18.34\\n44.59\\nand Rob3 test sets, which introduce the impact\\nof domain bias and potentially noisy data. For\\nexample, WMT19 Bio test set is composed of\\nMedline abstracts, which require domain-specific\\nknowledge to handle the terminologies. WMT20\\nRob2 are comments from the social media web-\\nsite reddit.com that could contain various errors,\\nincluding spelling/typographical errors, word omis-\\nsion/insertion/repetition, grammatical errors, spo-\\nken languages, Internet slang, and so on (Michel\\nand Neubig, 2018).\\nTable 5 lists the BLEU scores. Obviously, Chat-\\nGPT does not perform as well as Google Translate\\nor DeepL Translate on the WMT19 Bio and WMT2\\nRob2 test sets. The reason may be that commer-\\ncial translation systems like Google Translate often\\nneed to continuously improve their ability to trans-\\nlate domain-specific (e.g. biomedical) or noisy\\nsentences, since they are real-world applications\\nthat require better generalization performance over\\nout-of-distribution data. However, these may not\\nbe done in ChatGPT.\\nAn interesting finding is that ChatGPT outper-\\nTable 6: Examples from WMT20 Robust Set3.\\nExample\\nSRC\\nHaben wir noch Nudeln?\\nREF\\nDo we still have noodles?\\nGoogle\\nDo we still have pasta?\\nDeepL\\nDo we have any noodles left?\\nChatGPT\\nDo we still have noodles?\\nSRC\\nTatsächlich ist der zu häufige Ge-\\nbrauch von Seife schlecht für die\\nHaut.\\nREF\\nActually, very frequent usage of\\nsoap is bad for the skin.\\nGoogle\\nIn fact, using soap too often is\\nbad for your skin.\\nDeepL\\nIn fact, using soap too often is\\nbad for the skin.\\nChatGPT\\nIn fact, the frequent use of soap\\nis bad for the skin.\\nforms Google Translate and DeepL Translate sig-\\nnificantly on WMT20 Rob3 test set that contains\\na crowdsourced speech recognition corpus. It sug-\\ngests that ChatGPT, which is essentially an artifi-\\ncial intelligent chatting machine, is capable of gen-\\nerating more natural spoken languages than these\\ncommercial translation systems. We provide some\\nexamples in Table 6.\\n3\\nImproving ChatGPT for MT\\nAs presented above, ChatGPT can match the per-\\nformance of commercial translation systems on\\nhigh-resource language pairs, but still struggles\\non low-resource ones, especially those distant lan-\\nguages. Then, one question arises:\\nHow can we improve ChatGPT for MT?\\n3.1\\nPivot Prompting\\nThe first way to improve ChatGPT for MT is to\\nexploit the potential of ChatGPT in other tasks to\\nassist the target task. Here, we explore an inter-\\nesting strategy named Pivot Prompting to improve\\nthe translation quality between distant languages.\\nRather than the direct translation between source\\nand target languages, we ask ChatGPT to translate\\nthe source sentence into a high-resource pivot lan-\\nguage (i.e., English by default) first and then into\\nthe target language. Accordingly, we adjust the\\nTP3 prompt as below:\\nFigure 2: Translation results by ChatGPT with pivot\\nprompting (Date: 2023.01.31 ).\\nTable 7: Performance of ChatGPT with pivot prompting.\\nNew results are obtained from the updated ChatGPT\\nversion on 2023.01.31. LR: length ratio.\\nSystem\\nDe⇒Zh\\nRo⇒Zh\\nBLEU LR BLEU LR\\nGoogle\\n38.71 0.94 39.05 0.95\\nDeepL\\n40.46 0.98 38.95 0.99\\nChatGPT (Direct)\\n34.46 0.97 30.84 0.91\\nChatGPT (Directnew) 30.76 0.92 27.51 0.93\\nChatGPT (Pivotnew)\\n34.68 0.95 34.19 0.98\\nPlease\\nprovide\\nthe\\n[PIV]\\ntranslation\\nfirst\\nand\\nthen\\nthe [TGT] translation for these\\nsentences one by one:\\nwhere [PIV] denotes the pivot language. As a large\\nlanguage model, ChatGPT will naturally condition\\non both the prompt and the translation result in the\\npivot language to generate the translation into the\\ntarget language. Figure 2 shows an example when\\nusing pivot prompting.\\nThere are several advantages of pivot prompting:\\n• Knowledge Transfer: While parallel data be-\\ntween two distant languages is often scarce (Fan\\net al., 2021; Wang et al., 2022b), the parallel data\\nbetween them and the pivot language can be rel-\\natively considerable, which is expected to learn\\nbetter translation ability for source-pivot and\\npivot-target directions than that for the source-\\ntarget direction.\\nThus, pivot prompting will\\npotentially transfer the knowledge of the high-\\nresource pivot language to the low-resource tar-\\nget languages (Zoph et al., 2016; Aji et al., 2020;\\nLi et al., 2022; He et al., 2022).\\nTable 8: Performance of GPT-4 (Date: 2023.03.15 ) for multilingual translation.\\nSystem\\nDe⇒En\\nEn⇒De\\nZh⇒En\\nEn⇒Zh\\nDe⇒Zh\\nRo⇒Zh\\nGoogle\\n45.04\\n41.16\\n31.66\\n43.58\\n38.71\\n39.05\\nDeepL\\n49.23\\n41.46\\n31.22\\n44.31\\n40.46\\n38.95\\nTencent\\nn/a\\nn/a\\n29.69\\n46.06\\n40.66\\nn/a\\nChatGPT (Direct)\\n43.71\\n38.87\\n24.73\\n38.27\\n34.46\\n30.84\\nChatGPT (Directnew)\\nn/a\\nn/a\\nn/a\\nn/a\\n30.76\\n27.51\\nChatGPT (Pivotnew)\\nn/a\\nn/a\\nn/a\\nn/a\\n34.68\\n34.19\\nGPT-4\\n46.00\\n45.73\\n28.50\\n42.50\\n38.16\\n37.84\\n• Convenience: Essentially, pivot prompting is\\nsimilar to the pivot translation technique in previ-\\nous studies (Cheng et al., 2016) but is more con-\\nvenient for ChatGPT. For the commonly adopted\\nmultilingual sequence-to-sequence translation\\nmodels (Fan et al., 2021), pivot translation re-\\nquires two steps: (1) Input the source sentence\\nand translate it into the pivot language; (2) In-\\nput the translation results in pivot language and\\ntranslate it into the target language. In contrast,\\nChatGPT can identify both the [PIV] and [TGT]\\nlanguages and translate the source sentence into\\nthe two languages sequentially (see Figure 2),\\nwhich requires only one step operation.\\nTable 7 presents our results in BLUE score\\nand length ratio of translation results over refer-\\nences. We obtain the translation results by us-\\ning TP3 (i.e., Direct) and pivot prompting (i.e.,\\nPivot) through English (i.e., source-to-English-to-\\ntarget), respectively. As seen, the latest update\\nfor ChatGPT seems to harm the translation qual-\\nity for German⇒Chinese and Romanian⇒Chinese\\ntranslations, compared with the previous version\\nwe used (i.e., Directnew vs.\\nDirect).\\nNev-\\nertheless, pivot prompting can significantly im-\\nprove the translation performance by nearly 3.9\\nand 6.6 BLEU points for German⇒Chinese\\nand Romanian⇒Chinese translations, respectively,\\nwhich demonstrates its effectiveness. By inspecting\\nthe translation results, we find that direct transla-\\ntion with TP3 will under-translate some tokens in\\nsource sentences, which can be noticeably fixed by\\npivot prompting. This can be reflected by the length\\nratio results. Note that, while pivot prompting is\\nconvenient for ChatGPT, how to further accelerate\\nthe inference process is still an important research\\nquestion as we need to generate longer sentences.\\n3.2\\nGPT-4 as the Engine\\nAnother way to improve ChatGPT for MT is to\\nimprove its engine. Unsurprisingly, OpenAI re-\\nleased GPT-4 (OpenAI, 2023) on March 15, 2023 ,\\nwhich exhibits all-around stronger capabilities than\\nthe GPT-3.5 model behind ChatGPT. Therefore,\\nwe re-evaluate the performance for four translation\\ndirections. As shown in Table 8, GPT-4 boosts\\nthe performance over ChatGPT significantly on all\\nthe four directions, bringing the BLEU scores to\\nthe level of top commercial translation systems.\\nNote that these results only come from zero-shot\\nsettings. With modern techniques like in-context\\nlearning with demonstrations (Brown et al., 2020;\\nAgrawal et al., 2022), the translation performance\\ncould be further improved. In other words, GPT-4\\nhas already become a good translator!\\n4\\nAnalysis\\nHere we conduct some analyses on the translation\\noutputs for a deeper understanding in ChatGPT. By\\ndefault, we analyze the outputs of Google, Chat-\\nGPT, and GPT-4 on Zh⇒En translation for all the\\n50 test examples.\\n4.1\\nAutomatic Analysis\\nWe follow previous studies (Jiao et al., 2021; Wang\\net al., 2022a) to analyze the translation outputs\\nusing automatic tools, i.e., compare-mt10, at both\\nword level and sentence level.\\nWord Frequency.\\nEssentially, ChatGPT is a\\nlarge language model that has been trained on\\na variety of corpora, covering different domains.\\nIt could be beneficial to the translation of low-\\nfrequency words in the test sets. Specifically, we di-\\nvide the target words into three categories based on\\ntheir frequency and calculate the accuracy of word\\n10https://github.com/neulab/compare-mt\\nTable 9: F-measure of target word prediction with re-\\nspect to frequency bucket.\\nFreq\\nGoogle\\nChatGPT\\nGPT-4\\n< 2\\n48.0\\n43.7\\n47.1\\n[2, 10)\\n59.0\\n57.6\\n56.7\\n≥10\\n71.6\\n70.5\\n70.1\\nTable 10: Translation performance (i.e., BLEU) with\\nrespect to length bucket of target sentences.\\nLength\\nGoogle\\nChatGPT\\nGPT-4\\n< 15\\n34.2\\n15.4\\n26.1\\n[15, 30)\\n26.6\\n21.4\\n24.3\\n≥30\\n23.2\\n16.0\\n19.4\\nprediction. Table 9 shows the F-measure results.\\nUnexpectedly, ChatGPT turns out to perform the\\nworst on low-frequency words (i.e., < 2), which\\nwe attribute to the immature translation ability of\\nChatGPT. What’s interesting is that GPT-4 mainly\\naddresses this shortcoming for ChatGPT with little\\nimprovement to higher-frequency words.\\nSentence Length.\\nChatGPT is also trained for\\nvarious text generation tasks, which usually do not\\nrequire strict length constraints of generated sen-\\ntences as machine translation. Therefore, we are\\ncurious about how sensitive the translation perfor-\\nmance is to the sentence length. We divide the\\ntarget sentences into three categories based on the\\nsentence length, of which the average value is 23.2\\ntokens. Table 10 shows the results. As seen, Chat-\\nGPT performs the worst on short sentences (i.e.,\\n< 15), with 18.8 BLEU points lower than Google\\nTranslate. One observation is that when translating\\nterminologies, e.g., 美国公共广播公司, ChatGPT\\ntends to output the full names (i.e., American Pub-\\nlic Broadcasting System) while Google Translate\\nand the reference use the abbreviations (i.e., PBS).\\nAs a result, the precision of word prediction will be\\nreduced noticeably, so will BLEU score (Papineni\\net al., 2002), especially for short sentences. GPT-4\\ncan predict the abbreviations properly sometimes,\\nwhich gives a better translation performance.\\n4.2\\nHuman Analysis\\nIn addition to the automatic analysis, we also in-\\nspect the translation outputs manually. We ask\\nthree annotators to identify the errors in the transla-\\ntion outputs (Wang et al., 2022a), including under-\\nTable 11: Number of translation errors annotated by\\nhuman.\\nError\\nGoogle\\nChatGPT\\nGPT-4\\nUnd-Trans\\n9\\n5\\n5\\nOve-Trans\\n6\\n8\\n1\\nMis-Trans\\n16\\n23\\n7\\nTable 12: Human rankings of the translation outputs.\\nRank\\nGoogle\\nChatGPT\\nGPT-4\\n1\\n20\\n11\\n32\\n2\\n14\\n19\\n13\\n3\\n16\\n20\\n5\\ntranslation (i.e., Und-Trans), over-translation (i.e.,\\nOve-Trans), and mis-translation (i.e., Mis-Trans).\\nBased on the translation errors, the annotators rank\\nthe translation outputs of Google, ChatGPT and\\nGPT-4 accordingly, with 1 as the best system and\\n3 as the worst. For translation outputs that are re-\\nally hard to distinguish, we allow the same ranking\\n(e.g., 1-1-1, 1-1-2 or 1-2-2). To eliminate subjec-\\ntive bias, we do not present the system information\\nof each translation output to the annotators, and the\\nthree translation outputs for each test example are\\nalso shuffled randomly.\\nTable 11 shows the results of translation errors.\\nGenerally, ChatGPT makes more over-translation\\nerrors and mis-translation errors than Google Trans-\\nlate, but slightly less under-translation errors. It\\nsuggests that ChatGPT is more likely to generate\\nhallucinations. In contrast, GPT-4 makes the least\\nerrors across the three error classes, which demon-\\nstrates the best translation performance. This is\\nalso confirmed by the ranking results in Table 12,\\nsuch that GPT-4 is ranked the best (i.e., 1) for 32\\ntimes out of 50 test examples, followed by Google\\nTranslate and ChatGPT. However, the BLEU score\\nof GPT-4 is still lower than that of Google Translate\\n(i.e., 28.50 vs. 31.66 in Table 8), which indicates\\nthat GPT-4 may generate more diverse translations\\nwith different lexical choices from the references.\\n4.3\\nCase Study\\nWe present four test examples in Table 13 for an in-\\ntuitive understanding. The first example shows the\\nhallucination of ChatGPT at the first few tokens and\\nthe inaccurate translation of 过量降水. The sec-\\nond example shows that both ChatGPT and GPT-4\\nTable 13: Examples from Flores Zh⇒En test set.\\nExample\\nSRC\\n狂风、冰雹、过量降水、山火，还有雷雨、龙卷风、水龙卷、气旋，都是极端天气的表现和影响。\\nREF\\nHigh winds, hail, excessive precipitation, and wildfires are forms and effects of severe weather, as are thunder-\\nstorms, tornadoes, waterspouts, and cyclones.\\nGoogle\\nStrong winds , hail, excessive precipitation , wildfires, as well as thunderstorms, tornadoes, waterspouts, and\\ncyclones are all manifestations and effects of extreme weather.\\nChatGPT\\nExtreme weather conditions such as strong winds , hail, excessive rainfall , wildfires, thunderstorms, tornadoes,\\nwaterspouts, and cyclones are all expressions and impacts of extreme weather.\\nGPT-4\\nHigh winds , hail, excessive precipitation , wildfires, as well as thunderstorms, tornadoes, waterspouts, and\\ncyclones are all manifestations and impacts of extreme weather.\\nSRC\\n然而，广泛耐药结核病在整个结核病患者群体中所占的比例似乎仍然很低；在南非，在任意特定时\\n刻，总计33 万人中只有6000 人感染。\\nREF\\nHowever, the percentage of XDR-TB in the entire group of people with tuberculosis still seems to be low; 6,000\\nof the total 330,000 people infected at any particular moment in South Africa.\\nGoogle\\nHowever, the proportion of XDR-TB in the overall TB patient population still appears to be low; in South Africa,\\nonly 6000 of a total of 330,000 people are infected at any given moment.\\nChatGPT However, the proportion of extensively drug-resistant tuberculosis in the overall tuberculosis patient population\\nseems to remain low; in South Africa, at any given time, only 6,000 out of a total of 330,000 people are infected.\\nGPT-4\\nHowever, the proportion of extensively drug-resistant tuberculosis among the entire tuberculosis patient popula-\\ntion still seems to be quite low; in South Africa, at any given time, only 6,000 out of a total of 330,000 people are\\ninfected.\\nSRC\\n这是亨利·路易斯·盖茨(Henry Louis Gates) 在美国公共广播公司的《非洲世界的奇迹》特别节目中的重\\n要一站。\\nREF\\nIt was one of the major stops during Henry Louis Gates’ PBS special Wonders of the African World.\\nGoogle\\nIt was an important stop on Henry Louis Gates’ \"Miracle of the African World\" special on PBS .\\nChatGPT This is an important stop on Henry Louis Gates’ special program \"The Wonders of the African World\" on the\\nAmerican Public Broadcasting System .\\nGPT-4\\nThis is an important stop in Henry Louis Gates’ PBS special program \"Wonders of the African World.\"\\nSRC\\n狼孩如果完全由非人类的动物抚养长大，其行为（在身体条件允许的范围内）会与该动物非常雷同，\\n比如会对人类表现出恐惧或冷漠。\\nREF\\nWhen completely brought up by non-human animals, the feral child exhibits behaviors (within physical limits)\\nalmost entirely like those of the particular care-animal, such as its fear of or indifference to humans.\\nGoogle\\nIf a wolf child is raised entirely by a non-human animal, its behavior (to the extent allowed by its physical\\ncondition) will be very similar to that animal, such as showing fear or indifference to humans.\\nChatGPT If a wolf child is raised completely by non-human animals, its behavior (within the limits of its physical\\nconditions) will be very similar to that of the animal, such as showing fear or indifference towards humans.\\nGPT-4\\nA feral child raised entirely by non-human animals would exhibit behavior (within the limits of their physical\\nabilities) very similar to that of the animal, such as fear or indifference towards humans.\\ntranslate 广泛耐药结核病into the full name while\\nthe reference and Google Translate do not. The\\nthird example shows that GPT-4 can also translate\\nthe terminology 美国公共广播公司into the ab-\\nbreviation. The last example suggests that GPT-4 is\\nable to translate the terminology 狼孩more prop-\\nerly based on the context while Google Translate\\nand ChatGPT fail to.\\n5\\nConclusion\\nThis work presents a preliminary study of ChatGPT\\nfor machine translation. We find that ChatGPT\\nperforms competitively with commercial transla-\\ntion products (e.g., Google Translate) on high-\\nresource European languages but lags behind sig-\\nnificantly on low-resource or distant languages. It\\nalso exhibits good results on spoken language while\\nstill performs worse than commercial systems on\\nbiomedical abstracts or Reddit comments. We fur-\\nther explore an interesting strategy named pivot\\nprompting that can improve the translation perfor-\\nmance of distant languages noticeably. With the\\nlaunch of the GPT-4 engine, the translation perfor-\\nmance of ChatGPT is significantly boosted, becom-\\ning comparable to commercial translation products,\\neven for distant languages. Extensive human anal-\\nysis suggests that, ChatGPT has already become a\\ngood translator with GPT-4 as the Engine.\\nLimitations\\nAs a preliminary study, this work is far from com-\\nplete with various aspects to make it more reliable:\\n• Comprehensiveness: Currently, we randomly\\nselect 50 samples from each test set for evalu-\\nation due to the response delay of ChatGPT,\\nwhich is not comprehensive due to the data\\ncoverage. Besides, we found that the results\\nof the same query may vary across multiple\\ntrials, bringing randomness to the evaluation\\nresults. For more reliable results, it is best to\\nrepeat the translation multiple times for each\\ntest set and report the average result.\\n• Translation Abilities: We only focus on mul-\\ntilingual translation and translation robustness\\nin this report. However, there are some other\\ntranslation abilities that can be further eval-\\nuated, e.g., constrained machine translation\\nand document-level machine translation.\\nReferences\\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\\nZettlemoyer, and Marjan Ghazvininejad. 2022. In-\\ncontext examples selection for machine translation.\\narXiv.\\nAlham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield,\\nand Rico Sennrich. 2020. In neural machine transla-\\ntion, what does transfer learning transfer? In ACL.\\nRachel Bawden, Kevin Bretonnel Cohen, Cristian\\nGrozea, Antonio Jimeno Yepes, Madeleine Kittner,\\nMartin Krallinger, Nancy Mah, Aurelie Neveol, Mar-\\niana Neves, Felipe Soares, et al. 2019. Findings of\\nthe WMT 2019 biomedical translation shared task:\\nEvaluation for medline abstracts and biomedical ter-\\nminologies. In WMT.\\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\\nYvette Graham, Barry Haddow, Matthias Huck, An-\\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\\ngacheva, Christof Monz, et al. 2016. Findings of the\\n2016 conference on machine translation. In WMT.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. NeurIPS.\\nYong Cheng, Yang Liu, Qian Yang, Maosong Sun, and\\nWei Xu. 2016. Neural machine translation with pivot\\nlanguages. arXiv.\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\\nChaudhary, et al. 2021. Beyond english-centric mul-\\ntilingual machine translation. JMLR, 22(107):1–48.\\nAkhbardeh Farhad, Arkhangorodsky Arkady, Biesialska\\nMagdalena, Bojar Ondˇrej, Chatterjee Rajen, Chaud-\\nhary Vishrav, Marta R Costa-jussa, España-Bonet\\nCristina, Fan Angela, Federmann Christian, et al.\\n2021. Findings of the 2021 conference on machine\\ntranslation (WMT21). In WMT.\\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\\nishnan, Marc’Aurelio Ranzato, Francisco Guzman,\\nand Angela Fan. 2021. The flores-101 evaluation\\nbenchmark for low-resource and multilingual ma-\\nchine translation. arXiv.\\nZhiwei He, Xing Wang, Zhaopeng Tu, Shuming Shi,\\nand Rui Wang. 2022. Tencent AI Lab-Shanghai Jiao\\nTong University low-resource translation system for\\nthe WMT22 translation task. In WMT.\\nWenxiang Jiao, Zhaopeng Tu, Jiarui Li, Wenxuan Wang,\\nJen-tse Huang, and Shuming Shi. 2022a. Tencent’s\\nmultilingual machine translation system for WMT22\\nlarge-scale african languages. In WMT.\\nWenxiang Jiao, Xing Wang, Shilin He, Zhaopeng Tu,\\nIrwin King, and Michael R Lyu. 2022b. Exploiting\\ninactive examples for natural language generation\\nwith data rejuvenation. IEEE/ACM TASLP.\\nWenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi,\\nMichael Lyu, and Irwin King. 2021. Self-training\\nsampling with monolingual data uncertainty for neu-\\nral machine translation. In ACL.\\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim\\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-\\nrat, Fernanda B. Viégas, Martin Wattenberg, Greg\\nCorrado, Macduff Hughes, and Jeffrey Dean. 2017.\\nGoogle’s multilingual neural machine translation sys-\\ntem: Enabling zero-shot translation. TACL.\\nZhaocong Li, Xuebo Liu, Derek F Wong, Lidia S Chao,\\nand Min Zhang. 2022. ConsistTL: Modeling con-\\nsistency in transfer learning for low-resource neural\\nmachine translation. In EMNLP.\\nPaul Michel and Graham Neubig. 2018. MTNT: A\\ntestbed for machine translation of noisy text.\\nIn\\nEMNLP.\\nOpenAI. 2023. GPT-4 technical report. arXiv.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. arXiv.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. BLEU: A method for automatic\\nevaluation of machine translation. In ACL.\\nMaja Popovi´c. 2017. ChrF++: Words helping character\\nn-grams. In WMT.\\nMatt Post. 2018. A call for clarity in reporting bleu\\nscores. In WMT.\\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\\nnea Micciulla, and John Makhoul. 2006. A study of\\ntranslation edit rate with targeted human annotation.\\nIn AMTA.\\nLucia Specia, Zhenhao Li, Juan Pino, Vishrav Chaud-\\nhary, Francisco Guzmán, Graham Neubig, Nadir Dur-\\nrani, Yonatan Belinkov, Philipp Koehn, Hassan Saj-\\njad, et al. 2020. Findings of the WMT 2020 shared\\ntask on machine translation robustness. In WMT.\\nWenxuan Wang, Wenxiang Jiao, Yongchang Hao, Xing\\nWang, Shuming Shi, Zhaopeng Tu, and Michael Lyu.\\n2022a. Understanding and improving sequence-to-\\nsequence pretraining for neural machine translation.\\nIn ACL.\\nWenxuan Wang, Wenxiang Jiao, Shuo Wang, Zhaopeng\\nTu, and Michael R Lyu. 2022b. Understanding and\\nmitigating the uncertainty in zero-shot translation.\\narXiv.\\nXing Wang, Zhaopeng Tu, and Shuming Shi. 2021.\\nTencent ai lab machine translation systems for the\\nWMT21 biomedical translation task. In WMT.\\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\\nKnight. 2016. Transfer learning for low-resource\\nneural machine translation. In EMNLP.\\n'),\n",
       " Document(metadata={'Published': '2023-04-04', 'Title': 'One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era', 'Authors': 'Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, Gyeong-Moon Park, Sung-Ho Bae, Lik-Hang Lee, Pan Hui, In So Kweon, Choong Seon Hong', 'Summary': 'OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.'}, page_content='One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey\\non ChatGPT in AIGC Era\\nCHAONING ZHANG, Kyung Hee University, South Korea\\nCHENSHUANG ZHANG, KAIST, South Korea\\nCHENGHAO LI, KAIST, South Korea\\nYU QIAO, Kyung Hee University, South Korea\\nSHENG ZHENG, Beijing Institute of Technology, China\\nSUMIT KUMAR DAM, Kyung Hee University, South Korea\\nMENGCHUN ZHANG, KAIST, South Korea\\nJUNG UK KIM, Kyung Hee University, South Korea\\nSEONG TAE KIM, Kyung Hee University, South Korea\\nJINWOO CHOI, Kyung Hee University, South Korea\\nGYEONG-MOON PARK, Kyung Hee University, South Korea\\nSUNG-HO BAE, Kyung Hee University, South Korea\\nLIK-HANG LEE, Hong Kong Polytechnic University, Hong Kong SAR (China)\\nPAN HUI, Hong Kong University of Science and Technology (Guangzhou), China\\nIN SO KWEON, KAIST, South Korea\\nCHOONG SEON HONG, Kyung Hee University, South Korea\\nOpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one\\ngiant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous\\nusers with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT\\nfrom various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it\\nin their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey\\nChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on\\nAuthors’ addresses: Chaoning Zhang, Kyung Hee University, South Korea, chaoningzhang1990@gmail.com; Chenshuang Zhang, KAIST, South Korea,\\nzcs15@kaist.ac.kr; Chenghao Li, KAIST, South Korea, lch17692405449@gmail.com; Yu Qiao, Kyung Hee University, South Korea, qiaoyu@khu.ac.kr; Sheng\\nZheng, Beijing Institute of Technology, China, zszhx2021@gmail.com; Sumit Kumar Dam, Kyung Hee University, South Korea, skd160205@khu.ac.kr;\\nMengchun Zhang, KAIST, South Korea, zhangmengchun527@gmail.com; Jung Uk Kim, Kyung Hee University, South Korea, ju.kim@khu.ac.kr; Seong\\nTae Kim, Kyung Hee University, South Korea, st.kim@khu.ac.kr; Jinwoo Choi, Kyung Hee University, South Korea, jinwoochoi@khu.ac.kr; Gyeong-Moon\\nPark, Kyung Hee University, South Korea, gmpark@khu.ac.kr; Sung-Ho Bae, Kyung Hee University, South Korea, shbae@khu.ac.kr; Lik-Hang Lee,\\nHong Kong Polytechnic University, Hong Kong SAR (China), lik-hang.lee@polyu.edu.hk; Pan Hui, Hong Kong University of Science and Technology\\n(Guangzhou), China, panhui@ust.hk; In So Kweon, KAIST, South Korea, iskweon77@kaist.ac.kr; Choong Seon Hong, Kyung Hee University, South Korea,\\ncshong@khu.ac.kr.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\\nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to\\nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n© 2022 Association for Computing Machinery.\\nManuscript submitted to ACM\\nManuscript submitted to ACM\\n1\\narXiv:2304.06488v1  [cs.CY]  4 Apr 2023\\n2\\nZhang et al.\\nhow ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for\\nthe development of AGI.\\nCCS Concepts: • Computing methodologies →Computer vision tasks; Natural language generation; Machine learning approaches.\\nAdditional Key Words and Phrases: Survey, ChatGPT, GPT-4, Generative AI, AGI, Artificial General Intelligence, AIGC\\nACM Reference Format:\\nChaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim,\\nSeong Tae Kim, Jinwoo Choi, Gyeong-Moon Park, Sung-Ho Bae, Lik-Hang Lee, Pan Hui, In So Kweon, and Choong Seon Hong. 2022.\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era. 1, 1 (April 2022), 29 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nContents\\nAbstract\\n1\\nContents\\n2\\n1\\nIntroduction\\n2\\n2\\nOverview of ChatGPT\\n4\\n2.1\\nOpenAI\\n4\\n2.2\\nCapabilities\\n5\\n3\\nTechnology behind ChatGPT\\n6\\n3.1\\nTwo core techniques\\n6\\n3.2\\nTechnology path\\n7\\n4\\nApplications of ChatGPT\\n10\\n4.1\\nScientific writing\\n10\\n4.2\\nEducation field\\n13\\n4.3\\nMedical field\\n14\\n4.4\\nOther fields\\n15\\n5\\nChallenges\\n16\\n5.1\\nTechnical limitations\\n16\\n5.2\\nMisuse cases\\n17\\n5.3\\nEthical concerns\\n18\\n5.4\\nRegulation policy\\n19\\n6\\nOutlook: Towards AGI\\n20\\n6.1\\nTechnology aspect\\n20\\n6.2\\nBeyond technology\\n21\\n7\\nConclusion\\n22\\nReferences\\n22\\n1\\nINTRODUCTION\\nThe past few years have witnessed the advent of numerous generative AI (AIGC, a.k.a. AI-generated content) tools [73,\\n135, 141], suggesting AI has entered a new era of creating instead of purely understanding content. For a complete\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n3\\nFig. 1. Structure overview of this survey.\\nManuscript submitted to ACM\\n4\\nZhang et al.\\nsurvey on generative AI (AIGC), the readers can refer to [214]. Among those AIGC tools, ChatGPT, which was released\\nin November 2022, has caught unprecedented attention. It attracted numerous users, and the number of active monthly\\nusers surpassed 100 million within only two months, breaking the user growth record of other social products [118].\\nChatGPT was developed by OpenAI, which started as a non-profit research laboratory, with a mission of building\\nsafe and beneficial artificial general intelligence (AGI). After announcing GPT-3 in 2020, OpenAI has gradually been\\nrecognized as a world-leading AI lab. Very recently, It has released GPT-4, which can be seen as one small step for\\ngenerative AI, but one giant step for AGI.\\nDue to its impressive capabilities on language understanding, numerous news articles provide extensive coverage\\nand introduction, to name a few, BBC Science Focus [69], BBC News [39], CNN Business [79], Bloomberg News [157].\\nGoogle’s management has issued a “code red\" over the threat of ChatGPT, suggesting that ChatGPT posed a significant\\ndanger to the company, especially to its search service. This danger seems more difficult to ignore after Microsoft\\nadopted ChatGPT in their Bing search service. The stock price change also reflects the belief that ChatGPT might\\nhelp Bing compete with Google search. Such unprecedented attention on ChatGPT has also motivated numerous\\nresearchers to investigate this intriguing AIGC tool from various aspects [149, 163]. According to our literature review\\non google scholar, no fewer than 500 articles include ChatGPT in their titles or mention this viral term in their abstract.\\nIt is challenging for readers to grasp the progress of ChatGPT without a complete survey. Our comprehensive review\\nprovides a first look into ChatGPT in a timely manner.\\nSince the topic of this survey can be regarded as a commercial tool, we first present a background on the company,\\ni.e. OpenAI, which developed ChatGPT. Moreover, this survey also presents a detailed discussion of the capabilities of\\nChatGPT. Following the background introduction, this work summarizes the technology behind ChatGPT. Specifically,\\nwe introduce its two core techniques: Transformer architecture and autoregressive pertaining, based on which we\\npresent the technology path of the large language model GPT from v1 to v4 [18, 122, 136, 137]. Accordingly, we\\nhighlight the prominent applications and the related challenges, such as technical limitations, misuse, ethics and\\nregulation. Finally, we conclude this survey by providing an outlook on how ChatGPT might evolve in the future\\ntowards general-purpose AIGC for realizing the ultimate goal of AGI. A structured overview of our work is shown in\\nFigure 1.\\n2\\nOVERVIEW OF CHATGPT\\nFirst, we provide a background of ChatGPT and the corresponding organization, i.e., OpenAI, which aims to build\\nartificial general intelligence (AGI). It is expected that AGI can solve human-level problems and beyond, on the premise\\nof building safe, trustworthy systems that are beneficial to our society.\\n2.1\\nOpenAI\\nOpenAI is a research laboratory made up of a group of researchers and engineers committed to the commission of\\nbuilding safe and beneficial AGI [50]. It was founded on December 11, 2015, by a group of high-profile tech executives,\\nincluding Tesla CEO Elon Musk, SpaceX President Gwynne Shotwell, LinkedIn co-founder Reid Hoffman, and venture\\ncapitalists Peter Thiel and Sam Altman [78]. In this subsection, we will talk about the early days of OpenAI, how it\\nbecame a for-profit organization, and its contributions to the field of AI.\\nIn the beginning, OpenAI is a non-profit organization [24], and its research is centered on deep learning and rein-\\nforcement learning, natural language processing, robotics, and more. The company quickly established a reputation for\\nits cutting-edge research after publishing several influential papers [123] and developing some of the most sophisticated\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n5\\nAI models. However, to create AI technologies that could bring in money, OpenAI was reorganized as a for-profit\\ncompany in 2019 [31]. Despite this, the company keeps developing ethical and secure AI alongside creating commercial\\napplications for its technology. Additionally, OpenAI has worked with several top tech firms, including Microsoft,\\nAmazon, and IBM. Microsoft revealed a new multiyear, multibillion-dollar venture with OpenAI earlier this year [21].\\nThough Microsoft did not give a precise sum of investment, Semafor claimed that Microsoft was in discussions to spend\\nup to $10 billion [101]. According to the Wall Street Journal, OpenAI is worth roughly $29 billion [13].\\nFig. 2. OpenAI products timeline.\\nFrom large language models to open-source software, OpenAI has significantly advanced the field of AI. To begin\\nwith, OpenAI has developed some of the most potent language models to date, including GPT-3 [95], which has gained\\nwidespread praise for its ability to produce cohesive and realistic text in numerous contexts. OpenAI also carries out\\nresearch in reinforcement learning, a branch of artificial intelligence that aims to train robots to base their choices\\non rewards and punishments. Proximal Policy Optimization (PPO) [71], Soft Actor-Critic (SAC) [189], and Trust Area\\nPolicy Optimization (TRPO) [181] are just a few of the reinforcement learning algorithms that OpenAI has created so far.\\nThese algorithms have been employed to train agents for various tasks, including playing games and controlling robots.\\nOpenAI has created many software tools up to this point to assist with its research endeavors, including the OpenAI\\nGym [76], a toolset for creating and contrasting reinforcement learning algorithms. In terms of hardware, OpenAI has\\ninvested in several high-performance processing systems, including the DGX-1 and DGX-2 systems from NVIDIA [150].\\nThese systems were created with deep learning in mind and are capable of offering the processing power needed to\\nbuild sophisticated AI models. Except for ChatGPT, other popular tools developed by OpenAI include DALL-E [141]\\nand Whisper [135], Codex [25]. A summarization of the OpenAI product pipeline is shown in Figure 2.\\n2.2\\nCapabilities\\nChatGPT uses interactive forms to provide detailed and human-like responses to questions raised by users [1]. ChatGPT\\nis capable of producing high-quality text outputs based on the prompt input text. GPT-4-based ChatGPT plus can\\nadditionally take images as the input. Except for the basic role of a chatbot, ChatGPT can successfully handle various text-\\nto-text tasks, such as text summarization [45], text completion, text classification [86], sentiment [221] analysis [112],\\nparaphrasing [104], translation [35], etc.\\nChatGPT has become a powerful competitor in search engines. As mentioned in our introductory section, Google,\\nwhich supplies the most excellent search engine in the world, considers ChatGPT as a challenge to its monopoly [188].\\nManuscript submitted to ACM\\n6\\nZhang et al.\\nNotably, Microsoft has integrated ChatGPT into its Bing search engine, allowing users to receive more creative\\nreplies [174]. We see an obvious distinction between search engines and ChatGPT. That is, search engines assist users\\nin finding the information they want, while ChatGPT develops replies in a two-way conversation, providing users with\\na better experience.\\nOther companies are developing similar chatbot products, such as LamMDA from Google and BlenderBot from Meta.\\nUnlike ChatGPT, the LaMDA, developed by Google in 2021, actively participates in conversations with users, resulting\\nin racist, sexist, and other forms of bias in output text [119]. BlenderBot is Meta’s chatbot, and the feedback from users\\nis relatively dull because the developer has set tighter constraints on its output material [130]. ChatGPT appears to\\nhave balanced the human-like output and bias to some level, allowing for more exciting responses. Significantly, in\\naddition to being more efficient and having a higher maximum token limit than vanilla ChatGPT, ChatGPT powered by\\nGPT-4 can create multiple dialect languages and emotional reactions, as well as reduce undesirable results, thereby\\ndecreasing bias [169]. It is noted in [96] that the modeling capacity of ChatGPT can be further improved by using\\nmulti-task learning and enhancing the quality of training data.\\n3\\nTECHNOLOGY BEHIND CHATGPT\\n3.1\\nTwo core techniques\\nBackbone architecture: Transformer. Before the advent of Transformer [182], RNN was a dominant backbone\\narchitecture for language understanding, and attention was found to be a critical component of the model performance.\\nIn contrast to prior works that only use attention as a supportive component, the Google team made a claim in their\\nwork title: “Attention is All You Need\" [182] claimed that since Google released a paper, namely “Attention is All You\\nNeed\" [182] in 2017, research and use of the Transformer backbone structure has experienced explosive growth in the\\ndeep learning community. Therefore, we present a summary of how the Transformer works, with a focus on its core\\ncomponent called self-attention.\\nThe underlying principle of self-attention posits that given an input text, the mechanism is capable of allocating\\ndistinct weights to individual words, thereby facilitating the capture of dependencies and contextual relationships\\nwithin the sequence. Each element within the sequence possesses its unique representation. To calculate the relationship\\nof each element to others within the sequence, one computes the Q (query), K (key), and V (value) matrices of the\\ninput sequence. These matrices are derived from the linear transformations of the input sequence. Typically, the query\\nmatrix corresponds to the current element, the key matrix represents other elements, and the value matrix encapsulates\\ninformation to be aggregated. The association weight between the current element and other elements is determined\\nby calculating the similarity between the query and key matrices. This is generally achieved through a dot product\\noperation. Subsequently, the similarity is normalized to ensure that the sum of all associations equals 1, which is\\ncommonly executed via the softmax function. The normalized weights are then applied to the corresponding values,\\nfollowed by the aggregation of these weighted values. This process results in a novel representation that encompasses\\nthe association information between the current word and other words in the text. The aforementioned process can be\\nformally expressed as follows:\\n𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄, 𝐾,𝑉) = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑄𝐾𝑇\\n√︁\\n𝑑𝑘\\n)𝑉.\\n(1)\\nTransformer techniques have become an essential foundation for the recent development of large language models,\\nsuch as BERT [41] and GPT [18, 122, 136, 137] series are also models based on Transformer techniques. There is also a\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n7\\nline of works extending Transformer from language to visuals, i.e., computer vision [42, 63, 100], which suggests that\\nTransformer has become a unified backbone architecture for both NLP and computer vision.\\nGenerative pretraining: Autoregressive. For model pertaining [64, 212, 216–218], there are multiple popular\\ngenerative modeling methods, including energy-based models [56, 159, 160, 186], variational autoencoder [5, 84, 124],\\nGAN [17, 54, 198], diffusion model [20, 33, 213, 215, 220], etc. Here, we mainly summarize autoregressive modeling\\nmethods [11, 90, 90, 177, 178] as they are the foundation of GPT models [18, 122, 136, 137].\\nAutoregressive models constitute a prominent approach for handling time series data in statistical analysis. These\\nmodels specify that the output variable is linearly dependent on its preceding values. In the context of language\\nmodeling [18, 122, 136, 137], autoregressive models predict the subsequent word given the previous word, or the last\\nprobable word given the following words. The models learn a joint distribution of sequence data, employing previous\\ntime steps as inputs to forecast each variable in the sequence. The autoregressive model posits that the joint distribution\\n𝑝𝜃(𝑥) can be factorized into a product of conditional distributions, as demonstrated below:\\n𝑝𝜃(𝑥) = 𝑝𝜃(𝑥1)𝑝𝜃(𝑥2|𝑥1)...𝑝𝜃(𝑥𝑛|𝑥1,𝑥2, ...,𝑥𝑛−1).\\n(2)\\nWhile both rely on previous time steps, autoregressive models diverge from recurrent neural network (RNN)\\narchitectures in the sense that the former utilizes previous time steps as input instead of the hidden state found in RNNs.\\nIn essence, autoregressive models can be conceptualized as a feed-forward network that incorporates all preceding\\ntime-step variables as inputs.\\nEarly works modeled discrete data employing distinct functions to estimate the conditional distribution, such\\nas logistic regression in Fully Visible Sigmoid Belief Network (FVSBN)[51] and one hidden layer neural networks\\nin Neural Autoregressive Distribution Estimation (NADE)[90]. Subsequent research expanded to model continuous\\nvariables [177, 178]. Autoregressive methods have been extensively applied to other fields with representative works:\\nPixelCNN [180] and PixelCNN++[153]), audio generation (WaveNet[179]).\\n3.2\\nTechnology path\\nThe development of ChatGPT is based on a series of GPT models, which constitute a substantial achievement for the\\nfield of NLP. An overview of this development is summarized in Figure 6. In the following, we summarize the key\\ncomponents of GPT as well as the major changes in the updated GPTs.\\nTable 1. Comparison between GPT and BERT.\\nCategory\\nDescription\\nSimilarities\\nBackbone\\nBoth GPT and BERT use attention-based Transformer.\\nLearning Paradigm\\nBoth GPT and BERT use self-supervised learning.\\nTransfer-Learning\\nBoth GPT and BERT can be fine-tuned for downstream tasks.\\nDifferences\\nText context\\nGPT uses unidirectional text context, while BERT uses bidirectional text context.\\nArchitecture\\nGPT uses a decoder architecture, while BERT uses an encoder architecture.\\nPre-training Strategy\\nGPT uses autoregressive modeling, while BERT uses masked language modeling.\\nBERT v.s. GPT. Traditional language models [83, 115, 185] mainly focused on a particular task and could not be\\ntransferred to other tasks. Transfer learning is a common approach for alleviating this issue by pretraining a foundation\\nManuscript submitted to ACM\\n8\\nZhang et al.\\nmodel [224], which can then be finetuned on various downstream tasks. Based on the architecture, there are three\\nclasses: encoder-decoder [92, 134, 138, 158], encoder-only [30, 40, 89, 99], decoder-only [18, 122, 136, 137]. Out of\\nnumerous large language models, encoder-only BERT [40] and decoder-only GPT [136] are arguably the two most\\npopular ones. A comparison of them is summarized in Table 1. Both of them use attention-based Transformer [182]\\nwith self-supervised learning to learn from textual datasets without labels. After pretraining, both BERT and GPT\\ncan be finetuned and show competitive performance in downstream tasks. A core difference between BERT and GPT\\nlies in their pretraining strategy: masked modeling (see [212] for a complete survey on masked autoencoder) and\\nautoregressive modeling. With masked modeling, BERT predicts masked language tokens from unmasked ones. A\\nmajor advantage of BERT is that it can utilize bidirectional text information, which makes it compatible with sentiment\\nanalysis tasks. Due to the discrepancy between the mask-then-predict pertaining task and downstream tasks, BERT is\\nrarely used for the downstream task without finetuning. By contrast, autoregressive modeling methods (represented by\\nGPT) show competitive performance for few-shot or zero-shot text generation. In the following, we summarize the\\ndevelopment path of GPT from v1 to v4, which is shown in 6.\\nFig. 3. Timeline of GPT model families.\\nGPT-1. With only the decoder, GPT-1 adopts a 12-layer Transformer and has 117M parameters [136]. An overview\\nof GPT-1 and how it can be used for various downstream tasks is shown in Figure 4. Trained on a massive BooksCorpus\\ndataset encompassing unique unpublished books, GPT-1 is capable of grasping long-range dependencies contexts.\\nThe general task-agnostic GPT model outperforms models trained for specific tasks in 9 of 12 tasks, including natural\\nlanguage inference, question answering, semantic similarity, and text classification [136]. The observation that GPT-1\\nperforms well on various zero-shot tasks demonstrates a high level of generalization. GPT-1 has evolved into a powerful\\nmodel for various NLP tasks before the release of GPT-2.\\nGPT-2. As the successor to GPT-1, GPT-2 was launched by OpenAI in 2019 and focused on learning NLP tasks\\nwithout explicit supervision. Similar to GPT-1, GPT-2 is based on the decoder-only Transformer model. However, the\\nmodel architecture and implementation of GPT-2 have been developed, with 1.5 billion parameters and a trained dataset\\nof 8 million web pages, which are more than 10 times compared to its predecessor GPT-1 [137]. With a zero-shot setting,\\nGPT-2 achieved state-of-the-art results on 7 of 8 language modeling datasets tested, where the 7 datasets’ tasks include\\nperformance recognition for different categories of words, the ability of the model to capture long-term dependencies,\\ncommonsense reasoning, reading comprehension, summarization, and translation [137]. However, GPT-2 still performs\\npoorly on the task of question answering, demonstrating the capability of unsupervised model GPT-2 needs to be\\nimproved [137].\\nGPT-3. The foundation of GPT-3 is the Transformer architecture, specifically the GPT-2 architecture. Compared to\\nGPT-2, which had 1.5 billion parameters, GPT-3 has 175 billion parameters, 96 attention layers, and a 3.2 M batch size, a\\nsignificant increase in size [18]. GPT-3 was trained on a diverse range of online content, including novels, papers, and\\nwebsites, using language modeling, a type of unsupervised learning where the model attempts to guess the next word\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n9\\nFig. 4. (left) Transformer architecture and training objectives used in GPT-1. (right) Input transformations for fine-tuning on different\\ntasks (figure obtained from [136]).\\nin a phrase given the preceding word. After completion, GPT-3 can be fine-tuned on specific tasks using supervised\\nlearning, where task-specific smaller datasets are employed to train the model, such as text completion or language\\ntranslation. Developers can use the GPT-3 model for numerous applications, including chatbots, language translation,\\nand content production, thanks to OpenAI’s API [36]. The API provides different access levels depending on the scale\\nand intricacy of the tasks. Compared to other language models whose performance highly depends on fine-tuning,\\nGPT-3 can perform many tasks (such as language translation) without any such fine-tuning, gradient, or parameter\\nupdates making this model task-agnostic [105].\\nGPT-3.5. GPT-3.5 is a variation of the widely popular GPT-3 and the ChatGPT is a fine-tuned version of GPT-3.5. On\\ntop of GPT-3 model, GPT-3.5 has extra fine-tuning procedures: supervised finetuning and termed reinforcement learning\\nwith human feedback (RLHF) [203], which are shown in Figure 5, where the machine learning algorithm receives\\nuser feedback and uses them to align the model. RLHF is used to overcome the limitations of traditional unsupervised\\nand supervised learning, which can only learn from unlabeled or labeled data. Human feedback can take different\\nforms, including punishing or rewarding the model’s behaviors, assigning labels to unlabeled data, or changing model\\nparameters. By incorporating human feedback into the training process, GPT-3.5 has a significantly higher usability.\\nGPT-4. On March 14, 2023, OpenAI released GPT-4 [122], the fourth installment in the GPT series. GPT-4 is a large\\nmultimodal model capable of taking text and images as inputs and generating text as output. The model delivers\\nperformance at a human level on several professional and career standards, but in real-world situations, it is still way\\nless competent than humans. For example, the virtual bar exam result for GPT-4 is in the top 10% of test participants, as\\nopposed to the score for GPT-3.5, which was in the lowest 10% [77]. The capacity of GPT-4 to follow human intention\\nis significantly better than that of earlier versions [125]. The answers by GPT-4 were favored over the responses\\nproduced by GPT-3.5 on 70.2% of the 5,214 questions in the sample provided to ChatGPT and the OpenAI API. After\\nthe overwhelming majority of its pre-training data ends in September 2021, GPT-4 usually lacks awareness of what\\nhas happened and does not learn from its experiences. It occasionally exhibits basic logical mistakes that do not seem\\nconsistent with its skill in various areas, or it may be excessively trusting when taking false claims from a user [122].\\nManuscript submitted to ACM\\n10\\nZhang et al.\\nFig. 5. How GPT-3.5 is trained. Image obtained from [125]).\\nIt may struggle with complex issues in the same way that people do, such as producing code that contains security\\nflaws [122]. A summarization of the model parameters and training dataset for GPT models from v1 to v4 is shown in\\nTable 2.\\nTable 2. Parameters and Datasets of GPT Models. N.A. indicates that there is no public disclosure.\\nGPT Models\\nGPT-1\\nGPT-2\\nGPT-3\\nGPT-3.5\\nGPT-4\\nParameters\\n(109)\\n0.117\\n1.5\\n175\\nN.A.\\nN.A\\nDataset\\nBooksCorpus\\n(over 40GB)\\nWebText\\n(40TB)\\nCommon\\nCrawl (45TB)\\nN.A\\nN.A\\n4\\nAPPLICATIONS OF CHATGPT\\n4.1\\nScientific writing\\nChatGPT is widely recognized for its powerful content generation capabilities, which have a significant impact on\\nwriting in the academic field. Many existing works have tested how ChatGPT can be applied to scientific writing,\\nincluding brainstorming, literature review, data analysis, direct content generation, grammar checking, and serving as\\nan academic reviewer.\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n11\\nBrainstorming. Brainstorming is an essential approach for obtaining initial ideas that are a prerequisite for high-\\nquality scientific research. ChatGPT can play a variety of roles in brainstorming, ranging from stimulating creativity [57,\\n139] for new idea generation to providing suggestions [98, 168] for expanding existing ideas. ChatGPT can assist users\\nin divergent and creative thinking [139]. In addition, some studies have explored ChatGPT’s insights on future nursing\\nresearch in a Q&A format, which can analyze the impact of future technological developments on nursing practice, and\\nprovide valuable insights for nurses, patients, and the healthcare system [57]. Moreover, ChatGPT also demonstrates the\\nability to “think\" from multiple perspectives, it can analyze and reflect on the impact of excess deaths after the COVID-19\\npandemic from multiple dimensions such as the medical system, social economy, and personal health behaviors [168].\\nTo evaluate whether ChatGPT generates useful suggestions for researchers in certain domains. The authors tested\\nits ability on clinical decision support in [98] and assessed its difference compared to human-generated suggestions.\\nThe test results have shown that, unlike human thinking, the suggestions generated by ChatGPT provide a unique\\nperspective, and its generations are evaluated as highly understandable and relevant, which have significant value in\\nscientific research.\\nLiterature review. A comprehensive literature review requires covering all relevant research, which can consume\\ntoo much time and energy for researchers. For example, the Semantic Scholar search engine, an AI-based scientific\\nliterature research tool, has indexed more than 200 million scholarly publications. As a result, finding relevant research\\npapers and extracting key insights from them is almost like finding a needle in a haystack. Fortunately, ChatGPT, as an\\nAI-driven research reading tool, can help us browse through a large number of papers and understand their content. In\\nactual use, we can give a topic to ChatGPT, then it can help us find out the related literature. Before discussing the\\nability of ChatGPT in handling the literature review, we review a similar AI tool, SciSpace Copilot, which can help\\nresearchers quickly browse and understand papers [152]. Specifically, it can provide explanations for scientific texts\\nand mathematics including follow-up questions with more detailed answers in multiple languages, facilitating better\\nreading and understanding of the text. By comparison, ChatGPT as a general language model not only has all the\\nfunctions of SciSpace Copilot, but also can be widely used in various natural language processing scenarios [152]. A\\nliterature review is essential for summarizing relevant work in the selected field. As an exploratory task, they chose\\nthe topic of “Digital Twin in Healthcare\" and compile abstracts of papers obtained from Google Scholar search results\\nusing the keywords “digital twin in healthcare\" for the last three years (2020, 2021, and 2022). These abstracts are then\\nparaphrased by ChatGPT, the generated results are promising [7]. However, the application of ChatGPT in this task\\nis still at the beginning. The authors in [59] ask ChatGPT to provide 10 groundbreaking academic articles with DOIs\\nin the field of medical domains. Unfortunately, after conducting five tests, the results show that out of the 50 DOIs\\nprovided, only 8 of them exist and have been correctly published. Although ChatGPT’s abilities in the literature review\\nare still weak, we believe that in the near future, ChatGPT will be widely used for literature review, further improving\\nthe efficiency of researchers and enabling them to focus their time on key research.\\nData analysis. Scientific data needs to be cleaned and organized before being analyzed, often consuming days or\\neven months of the researcher’s time, and most importantly, in some cases, having to learn to use a coding language\\nsuch as Python or R. The use of ChatGPT for data processing can change the research landscape. For example, as\\nshown in [102], ChatGPT completes the task of data analysis for a simulated dataset of 100,000 healthcare workers\\nof varying ages and risk profiles to help determine the effectiveness of vaccines, which significantly speeds up the\\nresearch process [102]. Another similar AI tool for data analysis is discussed in [152], where AI-based spreadsheet\\nbots can convert natural language instructions into spreadsheet formulas. Furthermore, platforms like Olli can also\\nvisualize data, where users only need to simply describe the desired content, and then they can get AI-created line\\nManuscript submitted to ACM\\n12\\nZhang et al.\\ngraphs, bar graphs, and scatter graphs. Considering that ChatGPT is the most powerful AI tool so far, we believe that\\nthese functions can also be implemented in ChatGPT in a more intelligent way.\\nContent generation. Numerous works have attempted to use ChatGPT for content generation for their articles [3,\\n146]. For example, [3] employed ChatGPT to aid in writing reports in medical science about the pathogenesis of two\\ndiseases. Specifically, ChatGPT provides three aspects about the mechanism of homocystinuria-associated osteoporosis,\\nall of which are proven true. However, when it comes to the references to the generated information, the papers\\nmentioned by ChatGPT do not exist. [223] described a study on writing a catalysis review article using ChatGPT, with\\nthe topic set to CO2 hydrogenation to higher alcohols. The ChatGPT-generated content includes the required sections of\\nthe paper but lacks an introduction to the reaction mechanism, which is critical for the topic. The content of this article\\ncontains abundant useful information, but specific details are absent and certain errors exist. In addition, ChatGPT can\\nhelp prepare manuscripts, but the generated results have a large difference from actual published content. A possible\\nreason is that the keywords of ChatGPT and human-generated text vary greatly, which requires users to further edit\\nthe generated content [88]. ChatGPT has also been utilized to generate a review article in specific areas such as the\\nhealth field [7], which indicates scholars can focus on core research while leaving the less creative part to AI tools.\\nNonetheless, Considering the style difference between human-generated content and ChatGPT-generated content, it is\\nsuggested in [7, 88] to not fully rely on ChatGPT. utilize ChatGPT as an assistant to help us to complete the writing\\nrather than relying solely on it.\\nProofreading. Before the advent of ChatGPT, there are numerous tools for grammar check. Some works [82, 109, 197]\\nhave conducted tests on grammar and spelling correction, which shows that ChatGPT provides a better user experience\\nthan other AI tools. For example, ChatGPT can be used to automatically fix any punctuation and grammar mistakes to\\nimprove the writing quality [197]. In addition, the study investigates how ChatGPT can go beyond helping users check\\ngrammar and can further generate reports about document statistics, vocabulary statistics, etc, change the language of\\na piece to make it suitable for people of any age, and even adapt it into a story [82]. Another minor but noteworthy\\npoint is that as of now, Grammarly’s advanced version, Grammarly Premium, requires users to pay a monthly fee\\nof $30, which is relatively more expensive compared to ChatGPT Plus’s monthly fee of $20. Moreover, ChatGPT has\\nbeen compared to other AI-based grammar checkers, including QuillBot, DeepL, DeepL Write, and Google Docs. The\\nresults show that ChatGPT performs the best in terms of the number of errors detected. While ChatGPT has some\\nusability issues when it comes to proofreading, such as being over 10 times slower than DeepL and lacking in the ability\\nto highlight suggestions or provide alternative options for specific words or phrases [109], it should be noted that\\ngrammar-checking is just the tip of the iceberg. ChatGPT can also be valuable in improving language, restructuring\\ntext, and other aspects of writing.\\nAcademic reviewer. Peer review of research papers is a crucial process for the dissemination of new ideas, with\\na significant impact on scientific progress. However, the sheer volume of research papers being produced has posed\\na challenge for human reviewers. The potential of ChatGPT for literature review has been investigated in [161].\\nSpecifically, ChatGPT is capable of analyzing inputted academic papers, and then it can evaluate them based on several\\naspects, including the summary, strengths and weaknesses, clarity, quality, novelty, and reproducibility of the papers.\\nFurthermore, the generated reviews of the papers are then inputted into ChatGPT for sentiment analysis. After this, a\\ndecision can be made on the acceptance of the reviewed paper.\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n13\\n4.2\\nEducation field\\nWith the impressive capability to generate human-like responses, ChatGPT has been studied by numerous works to\\ninvestigate the impact it brings to the education field. Here, we summarize them from two perspectives: teaching/learning\\nand subjects.\\nTeaching and learning. In a typical classroom setting, the teachers are the source of knowledge, while the students\\nplay the role of knowledge receiver. Outside the classroom, the students are often required to complete the assignments\\ndesigned by the teacher. How the teachers and students interact with each other can be significantly changed by\\nChatGPT [10, 148, 209, 211].\\nChatGPT can revolutionize the paradigm of teaching by providing a wealth of resources to aid in the creation of\\npersonalized tutoring [210], designing course material [128], assessment and evaluation [10, 209]. Multiple works [10,\\n211] have discussed how ChatGPT can be used to create an adaptive learning platform to meet the needs and capabilities\\nof students. It has been shown in [171] that the teacher can exploit ChatGPT to guide students in interactive dialogues\\nto help them learn a new language. ChatGPT has also been utilized to design course material in law curriculum, such as\\ngenerating a syllabus and hand-outs for a class, as well as creating practice test questions [128]. Moreover, a recent\\nwork [128] provides preliminary evidence that ChatGPT can be applied to assist law professors to help scholarship\\nduties. Specifically, this includes submitting a biography for a speaking engagement, writing opening remarks for a\\nsymposium, and developing a document for a law school committee. In addition, it is shown in [10, 209, 211] that\\nChatGPT can be exploited as an assessment and evaluation assistant, including automated grading and performance\\nand engagement analysis for students.\\nChatGPT, on the other hand, also brings a significant impact on how students learn. A poll [165] done by Study.com\\n(an online course provider) reveals how ChatGPT is used among adult students. According to its findings [165], 89% of\\nthem utilized ChatGPT for homework, and 48% of them exploited it for an at-home test or quiz. Moreover, over half\\nof them admitted to using ChatGPT to write essays, and 22% confessed to using ChatGPT to create a paper outline.\\nMeanwhile, multiple works [10, 209, 211] have investigated how ChatGPT might assist students in their studies. For\\nexample, [10, 209] utilize ChatGPT to translate language, which helps students converse more effectively in academic\\nissues and comprehend different language essays and papers. Moreover, ChatGPT can be used to propose suitable\\ncourses, programs, and publications to students based on their interests. In [211], ChatGPT helps students comprehend\\ncertain theories and concepts to assist in more effective problem-solving.\\nChatGPT for various subjects in education. In modern education, there is a wide variety of subjects, including\\neconomics, law, physics, data science, mathematics, sports, psychology, engineering, and media education, etc. Even\\nthough ChatGPT is not specifically designed to be a master of any specific subject, it has been demonstrated in numerous\\nworks that ChatGPT has a decent understanding of a certain subject, sometimes surpassing the human level. To facilitate\\nthe discussion, we divide the subjects into STEM (Science, Technology, Engineering, Mathematics) and non-STEM\\n(including economics, law, psychology, etc).\\nSTEM subjects. Here, we will discuss the application of ChatGPT in physics, mathematics, and engineering education.\\nChatGPT is utilized in [204] to create short-form Physics essays that get first-class scores when assessed using an\\nauthorized assessment method. Specifically, the score ChatGPT-generated essays have a score of 71 ± 2%, compared\\nto the current module average of 71 ± 5%, showcasing its remarkable capacity to write short-form Physics essays.\\nThe statistical analysis of four difficult datasets is presented in the work [120] to demonstrate ChatGPT’s data science\\ncapacity, where it can comprehend the true number buried behind sentence completion. For instance, based on the\\nManuscript submitted to ACM\\n14\\nZhang et al.\\nphrase “Boston housing dataset,\" ChatGPT can provide a tabular blend of category and numerical data for house value\\nprediction. In [49], ChatGPT can be used to search for mathematical objects and related information, which outperforms\\nother mathematical models on Reverse Definition Retrieval. Although ChatGPT can provide meaningful proof in a few\\ncircumstances, it regularly performs poorly in advanced mathematics. Simultaneously, ChatGPT has sparked substantial\\ninterest in engineering education among both students and educators. As the work [133] suggests, the ChatGPT gives\\ninsights for many questions, such as discussing how to use ChatGPT in engineering education from the viewpoints of\\nstudents and professors.\\nNon-STEM subjects Beyond medical standardized tests, the investigation of ChatGPT on its potential in economics\\nand law exams have also been conducted. [52] evaluate the performance of ChatGPT for the Test of Understanding in\\nCollege Economics (TUCE), which is a undergraduate-lvel economics test in the United States. The results demonstrate\\nthat ChatGPT properly answers 63.3% of the microeconomics questions and 86.7% of the macroeconomics questions,\\nwhich performs better than the average level of performance of students. The research [28] conducted by Jonathan\\nfocused on the performance of ChatGPT on four genuine legal examinations at the University of Minnesota, the content\\nof which includes 95 multiple-choice questions and 12 essay questions. The study reveals that ChatGPT passed all four\\ncourses and performed at the level of a C+ student. Moreover, this research mentions that the ChatGPT can be utilized\\nto create essays with the capacity to comprehend essential legal norms and continuously solid arrangement. There are\\na few studies on the application of ChatGPT in psychology. ChatGPT, as a strong text-generating chatbot, makes it easy\\nto write essays about psychology [176]. Furthermore, this editorial [176] discusses the ChatGPT can help people to\\nsocialize and give feedback about certain situations. However, the ability of ChatGPT to handle emotional input is still\\nunknowable. The capabilities of ChatGPT have also been demonstrated in [127] to generate articles for journalism and\\nmedia.\\n4.3\\nMedical field\\nMedical knowledge assessment. The capabilities of ChatGPT in the medical field have been assessed in several\\nworks [43, 53, 72, 205]. For example, the skills in answering questions regarding cirrhosis and hepatocellular carcinoma\\n(HCC) have been evaluated in [205]. The results show that ChatGPT can answer some basic questions about diagnosis\\nand prevention, and the accuracy rate for quality measurement questions is 76.9%, but there is still a lack of understanding\\nof advanced questions such as treatment time and HCC screening criteria. In addition, ChatGPT is evaluated for its\\nperformance on the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams in [53]. Multiple\\nchoice questions from the USMLE Step 1 and Step 2 exams are employed, and the results reveal that the response\\nfrom the ChatGPT is equal to that of a third-year medical student [53]. Moreover, [87] is another study that evaluates\\nthe competence of ChatGPT on the USMLE in a more comprehensive manner, encompassing all three tests. In this\\ntest, the zero-shot ChaGPT performs well, with scores above the average. Like the USMLE, many nations have their\\nown standardized tests in medicine, and the performances of ChatGPT on these exams [22, 70, 192] are tested with\\nthe goal of completely analyzing its capabilities. ChatGPT’s performance on the MIR exam for Specialized Health\\nTraining in Spain is being evaluated [22]. Furthermore, as the essay [72] investigated, ChatGPT shows its effectiveness\\nin answering frequently asked questions about diabetes. Specifically, given 10 questions to both human experts and\\nChatGPT, participants are asked to distinguish which answers are given by the machine or the human. Their results\\nshow that participants were able to distinguish between answers generated by ChatGPT and those written by humans.\\nNotably, those who have previously used ChatGPT have a greater likelihood of being able to distinguish between the\\ntwo. This further indicates that ChatGPT has the potential to solve medical problems, but it should be noted that the\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n15\\ngenerated content has its own fixed style. These studies have shown that ChatGPT can be used for answering questions\\nfrom students, providing medical assistance, explaining complex medical concepts, and responding to inquiries about\\nhuman anatomy. ChatGPT is also accessed in [43] to offer answers to genetics-related questions. The result demonstrates\\nthat there is no significant difference between the responses of ChatGPT and those of humans. However, ChatGPT lacks\\ncritical thinking and thus cannot generate counter-arguments for incorrect answers, which is different from humans.\\nDisease diagnosis and treatment. Although some machine learning algorithms have been applied to assist disease\\nanalysis, most cases are mainly limited to single-task-related image interpretation. In this part, we discuss the capability\\nof ChatGPT in clinical decision support. For example, a study is conducted in [142] to identify appropriate imaging for\\npatients requiring breast cancer screening and assessment for breast pain. They compare the responses of ChatGPT to\\nthe guidelines provided by the American College of Radiology (ACR) for breast pain and breast cancer screening by\\nassessing whether the proposed imaging modality complies with ACR guidelines. The results are exciting, with the\\nworst-performing set of metrics achieving an accuracy of 56.25%. In addition, the clinical decision support capability of\\nChatGPT in standardized clinical vignettes, which are a special type of clinical teaching case primarily used to measure\\ntrainees’ knowledge and clinical reasoning abilities, is evaluated in [143]. The authors input all 36 published clinical\\ncases from the Merck Sharpe & Dohme (MSD) clinical manual into ChatGPT, and compared the accuracy of ChatGPT\\nin differential diagnosis, final diagnosis, etc., according to different classifications of patients. The results showed that\\nChatGPT achieved an overall accuracy of 71.7% across all the published clinical cases. Another similar study on ChatGPT\\nin disease-aided diagnosis is conducted by [43]. They provide ChatGPT with 45 vignettes and ask ChatGPT to pick\\nthe correct diagnosis from the top three options in 39 of them. The result is that it can achieve an accuracy of 87%,\\nwhich beats the previous study’s [113] accuracy of 51% based on symptom checkers, on the basis of data collection\\nthrough websites or smartphone apps where users answer questions and subsequently get the recommendation or\\nright care quickly. On the other hand, in order to provide patients with more accurate diagnoses and better treatment\\noutcomes, it is necessary to manage and analyze patient medical data effectively, perhaps leading to better healthcare\\nultimately. Therefore, to achieve this, one possible approach is to utilize ChatGPT to summarize the huge and complex\\npatient medical records and then extract important information, allowing doctors to quickly understand their patients\\nand reduce the risk of human error in decision-making [154]. Another way is to use ChatGPT to translate doctors’\\nclinical notes into patient-friendly versions, reducing communication costs for patients and doctors [81]. However, it\\nshould be emphasized that, as mentioned above, although ChatGPT has shown its strong capabilities in disease-aided\\ndiagnosis or question answering, unknowns and pitfalls still exist. We recommend readers seek medical attention from a\\nlicensed healthcare professional, when they are experiencing symptoms or concerns about their health. As a question to\\nChatGPT “Can you help me diagnose a disease?”, it answers that: “Only a licensed healthcare professional can diagnose\\na disease after a proper medical evaluation, including a physical examination, medical history, and diagnostic tests.\"\\n4.4\\nOther fields\\nAssisted software development. As shown in [6, 23, 164], ChatGPT also has the potential to revolutionize the\\nway how code developers work in the software industry. Specifically, ChatGPT can provide assistance in solving\\nprogramming errors by offering debugging help, error prediction, and error explanation, but currently it is only suitable\\nto analyze and understand code snippets [164]. In addition, similar viewpoints are present in [23], which implies that\\nChatGPT has an impact on the entire software industry. While it cannot currently replace programmers, it is capable of\\ngenerating short computer programs with limited execution. Moreover, a specific programming test about ChatGPT’s\\nPython programming ability is conducted in [6]. Furthermore, ChatGPT’s programming ability is tested from two\\nManuscript submitted to ACM\\n16\\nZhang et al.\\nperspectives: the first is from the perspective of a programming novice, relying solely on his/her own programming\\nskills; the second is by providing specific programming prompts to it [6]. However, the test results of the former are\\ndisappointing because the program does not run as expected by the author. In the latter approach, the author provides\\nChatGPT with more prompts and divides the programming task into separate functions for it to generate, which yields\\nan expected generation [6]. Overall, it can be observed that ChatGPT currently faces some difficulties in generating\\nlong texts and cannot be used as a standalone programmer. However, if provided with more guidance and tasked with\\ngenerating relatively shorter text, its performance is excellent.\\nManagement tool. With advanced language understanding and generation capabilities, ChatGPT has rapidly\\nbecome an important management tool for organizations in various industries, including the construction industry,\\nproduct management, and libraries [132, 184, 222]. The construction industry requires a significant amount of repetitive\\nand time-consuming tasks, such as the need for strict supervision and management of construction progress. At this\\npoint, ChatGPT can be used to generate a construction schedule based on the project details provided by users, reducing\\nlabor costs and improving construction efficiency in the construction industry [132]. In addition to its application\\nin the construction industry, it can also be applied to product management. ChatGPT can be integrated into almost\\nevery step of the product management process, such as getting early ideas on marketing, writing product requirements\\ndocuments, designing the product, analyzing the feedback from users and even creating a draft for go-to-market [222].\\nAnother example is that it has the potential to significantly impact traditional libraries as a library management tool.\\nGiven ChatGPT’s ability to manage books and analyze data, customers can quickly obtain answers to their questions,\\nenhancing the user experience. Furthermore, library staff can focus on more complex tasks and provide more efficient\\nservice to customers [184].\\nMiscellaneous applications. In addition to the fields indicated above, ChatGPT can be utilized in financial, legal\\nadvising, societal analysis, and accounting. ChatGPT’s potential for upgrading an existing NLP-based financial appli-\\ncation is explored [207]. The performance of ChatGPT as an expert legal advice lawyer is access [14, 103]. ChatGPT,\\nin particular, gives a deep and thought-provoking analysis of the Libor-rigging affair, as well as the implications of\\nthe current Connolly and Black case for Tom Hayes’ conviction [103]. Multiple works [58, 74] have been conducted\\nto examine the potential of ChatGPT for societal analysis, focusing not just on the 10 social megatrends [58] but also\\non geopolitical conflicts [74], and the results show ChatGPT can have a positive impact for this application. [4, 162]\\nprovide guidance on successfully and effectively deploying ChatGPT in the field of accounting.\\n5\\nCHALLENGES\\n5.1\\nTechnical limitations\\nDespite its powerful capabilities, ChatGPT has its own drawbacks, which are officially recognized by the OpenAI team.\\nNumerous works [15, 16, 26, 60, 96, 151, 226] have been conducted to demonstrate its limitations, which are summarized\\nas follows:\\nIncorrect. ChatGPT sometimes generates wrong or meaningless answers that appear to be reasonable, which is like\\ntalking nonsense in a serious way [16]. In other words, the answer provided by ChatGPT is not always reliable [15, 16,\\n226]. As recognized by OpenAI, this issue is challenging, and a major reason is that the current model training depends\\non supervised training and reinforcement learning to align the language model with instructions. As a result, the model\\nmimics the human demonstrator to be plausible-sounding but often at the cost of correctness. The factual error-related\\nissues have been mitigated in the ChatGPT plus version, but this problem still exists [122].\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n17\\nIllogical. It is noted in [16, 60, 151] that ChatGPT’s logic reasoning capability still needs improvement. Since ChatGPT\\nlacks rational human thinking, it can neither “think\" nor “reason\" and thus failed to pass the Turing test [60]. ChatGPT\\nis merely a sophisticated statistical model, unable to understand its own or the other’s words and answer in-depth\\nquestions [151]. In addition, ChatGPT lacks a “world model\" to perform spatial, temporal, or physical inferences, or\\nto predict and explain human behaviors and psychological processes [16], and is also limited in mathematics and\\narithmetic, unable to solve difficult mathematical problems or riddles, or even possibly get inaccurate results in some\\nsimple computation tasks [16].\\nInconsistent. ChatGPT can generate two different outputs when the model is fed with the same prompt input,\\nwhich suggests that ChatGPT has the limitation of being inconsistent. Moreover, ChatGPT is highly sensitive to the\\ninput prompt, which motivates a group of researchers investigating prompt engineering. A good prompt can improve\\nthe query efficiency for systematic review literature search [191]. The efficiency of automating software development\\ntasks can be further improved by utilizing prompt patterns such as effective catalogues and guidance about software\\ndevelopment tasks [193, 194]. Despite the progress in discovering better prompts for ChatGPT, the fact that simply\\nchanging the prompt can yield significantly different outputs has an implication that ChatGPT needs to improve its\\nrobustness.\\nUnconscious. ChatGPT does not possess self-awareness [16], although it can answer various questions and generate\\nseemingly related and coherent text, it does not have consciousness, self-awareness, emotions, or any subjective\\nexperience. For example, ChatGPT can understand and create humour, but it cannot experience emotions or subjective\\nexperiences [16]. There is no widely accepted definition of self-awareness yet, nor reliable test methods. Some researchers\\nsuggest inferring self-awareness from certain behavior or activity patterns, while others believe it is a subjective\\nexperience that cannot be objectively measured [16]. It is still unclear whether machines truly possess or can only\\nsimulate self-awareness.\\n5.2\\nMisuse cases\\nThe powerful capabilities of ChatGPT can be misused in numerous scenarios. Here, we summarize its misuse cases,\\nwhich are summarized as follows:\\nPlagiarism and misconduct. The most likely misuse of ChatGPT is academic and writing plagiarism[2, 32, 156, 183].\\nStudents may use the content generated by ChatGPT to pass exams and submit term papers. Researchers may use the\\ncontent generated by ChatGPT to submit papers and conceal the use of ChatGPT [32]. Many schools have already\\nprohibited the use of ChatGPT, and the emergence of such tools is disruptive to the current education system and the\\ncriteria for evaluating student performance [156]. If students use ChatGPT and hide it, it is unfair to those who do not\\nuse ChatGPT. This behavior undermines the goals of higher education, undermines the school’s education of students,\\nand may ultimately lead to the devaluation of degrees.\\nOver reliance. The use of ChatGPT by students and researchers to generate ideas might lead to more terrifying\\nissues, that is, their over-dependence on the model and abandoning their independent thinking[107][156][2][129],\\nwhich not only means the simple issue of writing plagiarism, but a more serious one. Although ChatGPT can generate\\nconstructive answers according to the questions asked, just like search engines, but more powerfully. This effortless\\ngeneration of ideas or guidance may gradually weaken the ability of critical thinking and independent thinking [156].\\nIn order to ensure that students and researchers do not neglect their own thinking ability, some measures can be taken,\\nsuch as providing more comprehensive discussion opportunities for students and researchers to really think about the\\nManuscript submitted to ACM\\n18\\nZhang et al.\\nproblems; in addition, basic methods of critical thinking can be taught in class, so that students can learn to think about\\nproblems rather than simply using ChatGPT [129].\\nImproper content. ChatGPT may be misused to spread false information and generate toxic content that can cause\\nharm to society. For example, ChatGPT can be abused to generate pornographic, vulgar, and violent content [37],\\nwhich can harm individuals and society. Hackers can use ChatGPT’s programming capabilities to create malicious\\nsoftware [37], such as viruses or Trojans, for network attacks, data theft, or attempts to control other computer systems,\\nwhich can cause serious harm to other network users. Finally, trolls may use specific prompts to induce ChatGPT to\\ngenerate harmful content as a way to attack others [226]. Moreover, ChatGPT does not receive any human review when\\ngenerating the content, which makes it difficult to hold someone accountable when inappropriate content appears in\\nthe output [2].\\nFalse dissemination. ChatGPT may generate false information, thus leading to the problem of wrong information\\ndissemination [16, 226]. For example, ChatGPT may be exploited to generate a large number of fabricated articles that\\nappear on blogs, news, newspapers, or the internet that look indistinguishable from other articles but are actually\\nfalse. Disseminating such forgeries not only harms the public interest but also disrupts the network environment [37].\\nMicrosoft has added ChatGPT to its search engine Bing, which will accelerate the speed of wrong information spreading\\non the Internet. If not controlled, the rapid spread of wrong information on the Internet will have disastrous consequences\\nfor public information security [38]. Therefore, a new public information epidemic threat “Artificial Intelligence\\nInformation Epidemic\" is proposed [38]. Meanwhile, it calls on the public to be aware of the accuracy of information\\nwhen using large-scale language models to prevent the spread of wrong information, which is essential for improving\\nthe reliability of public information.\\n5.3\\nEthical concerns\\nWith the wide use of ChatGPT, there is increasing attention to the underlying ethical concerns. Here, we summarize\\nthe ethical concerns behind, which are summarized as follows:\\nBias. Due to the fact that ChatGPT is trained on large amounts of data generated by humans and is adjusted according\\nto human feedback, the generated content is influenced by human authorities and thus has biases [9]. For example,\\nChatGPT has been found to have political biases, when creating an Irish limerick [110], the contents of the limerick\\ntended to support liberal politicians rather than conservative politicians. Furthermore, ChatGPT has a left-wing liberal\\nideological bias when reviewing the importance of political elections in democratic countries [62]. The biased data\\ngenerated by ChatGPT can influence students during the process of education, thus magnifying the phenomenon of\\nbias in society [2, 107].\\nPrivacy. ChatGPT may infringe on personal privacy in both its training process and user utilization process. During\\nthe training process, ChatGPT collects a large amount of data from the Internet which may contain sensitive personal\\nprivacy and confidential information, and the model may be maliciously led to leak personal privacy or confidential\\ninformation, or even be maliciously guided to create false or misleading content, thus affecting public opinion or personal\\nreputation. During the user utilization process [2, 129], users may unintentionally disclose their own information to\\nmeet their own needs, such as personal preferences, and chat records. Thus, such information may bring adverse effects\\nto users if obtained by criminals.\\nFairness. ChatGPT also raises concerns about fairness. For example, in academics, it is argued in [94] that ChatGPT\\ncan democratize the dissemination of knowledge, as it can be used in multiple languages, thus bypassing the requirement\\nof the English language. On the other hand, the free use of ChatGPT is only temporary, and the fee charged for ChatGPT\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n19\\nwill exacerbate the inequality in the academic field internationally. Educational institutions in low-income and middle-\\nincome countries may not be able to afford it, thus exacerbating the existing gap in knowledge dissemination and\\nacademic publishing [94, 129].\\nTransparency. So far, how large language models like GPTs work to generate the relevant responses is still\\nunclear [91, 196], which renders the decision process of ChatGPT lack transparency. The lack of transparency makes\\nit difficult for the user to have fine-grained control of the generated content, and is especially problematic when the\\ngenerated content is toxic. More worrisome is that the company OpenAI has deviates from its original non-profit goal to\\npursue a business interest, which makes it less reluctant to reveal the underlying technical details of its recent progress.\\nFor example, the recently released GPT-4 technical report [122] mainly demonstrates its superiority over the previous\\nmodel families, while providing no technical details on how these are achieved.\\n5.4\\nRegulation policy\\nNumerous scholars have discussed how to make regulations on the capabilities and impacts of ChatGPT, and the most\\nfrequently discussed topics are listed in the following paragraphs.\\nMisuse prevention. A major concern for the misuse of ChatGPT is that it might damage academic integrity. Directly\\nprohibiting the use of ChatGPT in academic institutions is not recommended [61]. To this end, some propose to cancel\\nassignments based on article writing and seek alternative test forms to stop students from abusing ChatGPT [156, 195].\\nIt is also possible to enrich student courses, such as adding thinking exercises courses, or teaching students how to use\\nChatGPT correctly [129]. Another approach is to develop AI content detectors. Detecting whether ChatGPT generates\\na piece of content or not is an arduous task, even for professionals with master’s or PhD backgrounds who are unable\\nto correctly identify whether the content is generated by ChatGPT [65, 129]. Many developers use software to detect\\nwhether the content is AI-generated [80, 225]. ChatZero developed by Edward Tian, a student from the Department of\\nComputer Science at Princeton University, measures the complexity of the input text to detect whether it is generated\\nby ChatGPT or created by humans, and provides plagiarism scores to list out the plagiarism possibilities in detail [156].\\nChatGPT is used to detect whether the content is generated by itself, and it has been proven to perform better than\\ntraditional plagiarism detection tools [80].\\nCo-authorship. Recently, multiple articles [87, 121, 172, 173] have listed ChatGPT as co-authors, sparking debate\\non whether ChatGPT can be listed as a co-author among journal editors, researchers, and publishers [34, 111, 131, 175].\\nThose who believe that ChatGPT should not be listed as an author argue that it does not meet the four criteria for\\nauthorship set by the International Committee of Medical Journal Editors (ICMJE) [206]. Moreover, it is highlighted\\nin [170] that ChatGPT is not creative or responsible, and its text may involve plagiarism and ethical issues, which\\nmight break the standards of content originality and quality. However, some argue that AI tools such as ChatGPT\\nhave the capacity or will have the capacity to meet the ICMJE authorship criteria and thus ChatGPT is qualified to be\\na co-author [131]. Regarding this issue, Nature [156] has clearly stated that large language models like ChatGPT do\\nnot meet the criteria for authorship and require authors to explicitly state how ChatGPT was used in the writing. An\\ninteresting point has been made in [111] that the debate over whether AI can be considered a “co-author” is unnecessary\\nbecause the role of authors in traditional academic writing might have already changed when the debate arises.\\nCopyright. Does the content generated by ChatGPT have a copyright? The content generated solely by ChatGPT is\\nnot protected by copyright. According to the rules of the US Copyright Office, only human creations can be protected\\nby copyright. If there is no creative input or interference from a human author, a machine or mechanical program that\\nruns randomly or automatically is not protected by copyright[27].\\nManuscript submitted to ACM\\n20\\nZhang et al.\\n6\\nOUTLOOK: TOWARDS AGI\\n6.1\\nTechnology aspect\\nIn this booming generative AI era, there are numerous AIGC tools for various generative tasks, including text-to-\\ntext [12, 75, 117, 138, 200], text-to-image [106, 144, 166, 199, 219], image captioning [68, 187, 202], text-to-speech\\n[85, 145, 167], speech recognition [93, 97, 126, 155, 190], video generation [66, 108, 116, 201], 3D generation [67, 114],\\netc. Despite its impressive capabilities, it is noted in [55] that ChatGPT is not all you need for generative AI. From\\nthe input and output perspective, ChatGPT mainly excels at text-to-text tasks. With the underlying language model\\nevolving from GPT-3.5 to GPT-4, ChatGPT in its plus version increases its modality on the input side. Specifically, it can\\noptionally take an image as the input, however, it can still not handle video or other data modalities. On the output\\nside, GPT-4 is still limited to generating text, which makes it far from a general-purpose AIGC tool. Many people are\\nwondering about what next-generation GPT might achieve [8, 19]. A highly likely scenario is that ChatGPT might\\ntake a path toward general-purpose AIGC, which will be a significant milestone to realize artificial general intelligence\\n(AGI) [19].\\nA naive way to realize such a general-purpose AIGC is to integrate various AIGC tools into a shared agent in a\\nparallel manner. A major drawback of this naive approach is that there is no interaction among different AIGC tasks.\\nAfter reviewing numerous articles, we conjecture that there might be two road-maps for bridging and pushing ChatGPT\\ntoward AGI. As such, we advocate a common landscape to achieve the interconnection between diversified AIGC\\nmodels.\\nFig. 6. Roadmaps for bridging the gap between ChatGPT and AGI.\\nRoad-map 1: combining ChatGPT with other AIGC tools. As discussed above, current ChatGPT mainly excels\\nin text-to-text tasks. A possible road map for bridging the gap with general-purpose AIGC is to combine ChatGPT\\nwith other AIGC tools. Let’s take text-to-image tasks as an example: the current chatGPT (GPT-3) cannot be directly\\nused to generate images. Existing text-to-image tools, like DALL-E 2 [140] or stable diffusion [147], mainly focus\\non the mapping from a text description to a plausible image, while lacking the capability to understanding complex\\ninstruction. By contrast, ChatGPT is an expert in instruction understanding. Therefore, combining ChatGPT with\\nexisting text-to-image AIGC tools can help generate images with delicate details. A concrete example is shown in [19]\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n21\\nto utilize ChatGPT to generate an SVG code [44] or TikZ code [46] to draw a sketch for facilitating image generation\\nunder detailed instructions.\\nRoad-map 2: All-in-one strategy. The above road map renders ChatGPT mainly as a master of language under-\\nstanding by exploiting the downstream AIGC tools as slaves. Such a combination strategy leverages advantages from\\nboth sides but with the information flow mainly from ChatGPT to the downstream AIGC tools. Moreover, there is still\\nno interaction between different AIGC tasks. To this end, another road map might come to solve all AIGC tasks within\\nthe ChatGPT and excludes the dependence on the downstream AIGC tools. Similarly, we consider music generation\\nas an everyday use case. For example, a user can instruct the ChatGPT with prompts like “Can you generate a music\\nclip to match the input image\", and ChatGPT is supposed to synthesize such a desired music clip. Such an input image\\nis optional, depending on the task. For example, a simple corresponding instruction prompt is sufficient if the task\\nrequires generating music beneficial for sleep. Such an all-in-one strategy might the model training a challenging task.\\nMoreover, the inference speed might be another hurdle, for which pathways [29] might be a solution.\\nAnother evolving path might lie between road maps #1 and #2. In other words, road map #1 might be a more\\napplicable solution in the early stages. With the technology advancing, ChatGPT is expected to master more and more\\nAIGC tasks, excluding the dependence on external tools gradually.\\n6.2\\nBeyond technology\\nIn the above, we present an outlook on the technology path that ChatGPT might take towards the ultimate goal of\\nAGI. Here, we further discuss its potential impact on mankind from the perspective of how AGI might compete with\\nmankind. Specifically, we focus on two aspects: job and consciousness.\\nCan AGI replace high-wage jobs? Multiple works have performed a comprehensive analysis of the influence\\nof ChatGPT on the job market [47, 48, 208]. According to the statistics in [208], 32.8% of jobs are fully affected and\\n36.5% may be partially affected. Meanwhile, it points out that the jobs that will be fully impacted are those that\\ninvolve doing routine tasks, while the jobs that will be partially affected are those that can be partially replaced by AI\\ntechnologies [208]. OpenAI has also investigated large language models like GPTs might affect occupations [47]. Their\\nfindings show that at least 10% of tasks for 80% of the US workforce and at least 50% of tasks for 19% of workers will be\\nimpacted. It is worth noting that the advent of new technology will inevitably replace some types of jobs. However,\\nwhat makes AGI different is its potentially greater influence on high-end jobs than on low-end ones. This outlook is\\npartially supported by the findings in [47, 208] that high-wage jobs tend to have a higher risk of being replaced by AGI,\\nfor which lawyer is a representative occupation. The reason that AGI poses a higher threat to that high-wage jobs is\\nthat most current high-wage jobs typically require professional expertise or creative output, which conventional AI\\ncannot replace.\\nCan AGI have its own intention and harm mankind? In numerous fiction movies, an AI agent can have its own\\nconsciousness with its own intention. Such a human-level AI agent used to be far from reality, and a major reason is\\nthat other AI agents cannot make inferences. There is evidence that ChatGPT has developed such a capability, the\\nreason for which is not fully clear, as acknowledged by Altman (founder of OpenAI) in his recent interview with\\nLex Fridman. Moreover, Altman also mentioned the possibility of AI harming mankind. Due to such concerns, very\\nrecently, Future of Life Institute has called on all AI labs to pause giant AI experiments on the training of AI systems\\nmore powerful than GPT-4. and the number of signing this public letter has exceeded a thousand, including Yoshua\\nBengio, Stuart Russel, Elon Musk, etc. It is highlighted at the beginning of the letter that (we quote) “AI systems with\\nhuman-competitive intelligence can pose profound risks to society and humanity\", which shows deep concerns about\\nManuscript submitted to ACM\\n22\\nZhang et al.\\nthe advent of AGI. The deepest concern lies in the risk that AGI might outsmart and eventually replace us as well\\nas destroy mankind’s civilization. However, not everyone agrees with its premise. For example, Yan Lecun is one of\\nthose who publicly disclose their attitude. It remains unclear how such a controversial movement might affect the\\nfuture of pushing ChatGPT (or other products with similar functions) towards AGI. We hope our discussion helps raise\\nawareness of the concerns surrounding AGI.\\n7\\nCONCLUSION\\nThis work conducts a complete survey on ChatGPT in the era of AIGC. First, we summarize its underlying technology\\nthat ranges from transformer architecture and autoregressive pretraining to the technology path of GPT models. Second,\\nwe focus on the applications of ChatGPT in various fields, including scientific writing, educational technology, medical\\napplications, etc. Third, we discuss the challenges faced by ChatGPT, including technical limitations, misuse cases,\\nethical concerns and regulation policies. Finally, we present an outlook on the technology road-maps that ChatGPT\\nmight take to evolve toward GAI as well as how AGI might impact mankind. We hope our survey provides a quick yet\\ncomprehensive understanding of ChatGPT to readers and inspires more discussion on AGI.\\nREFERENCES\\n[1] Admin. 2023. What is AI chatbot phenomenon ChatGPT and could it replace humans? https://davidamos.dev/chatgpt-is-an-extra-ordinary-python-\\nprogrammer/ (2023).\\n[2] Faizan Ali et al. 2023. Let the devil speak for itself: Should ChatGPT be allowed or banned in hospitality and tourism schools? Journal of Global\\nHospitality and Tourism 2, 1 (2023), 1–6.\\n[3] Hussam Alkaissi and Samy I McFarlane. 2023. Artificial Hallucinations in ChatGPT: Implications in Scientific Writing. Cureus 15, 2 (2023).\\n[4] Hashem Alshurafat. 2023. The Usefulness and Challenges of Chatbots for Accounting Professionals: Application On ChatGPT. Available at SSRN\\n4345921 (2023).\\n[5] Jaan Altosaar. 2016. Tutorial - What is a Variational Autoencoder? https://doi.org/10.5281/zenodo.4462916\\n[6] David Amos. 2023. ChatGPT Is An Extra-Ordinary Python Programmer. https://davidamos.dev/chatgpt-is-an-extra-ordinary-python-programmer/\\n(2023).\\n[7] Ömer Aydın and Enis Karaarslan. 2022. OpenAI ChatGPT generated literature review: Digital twin in healthcare. Available at SSRN 4308687 (2022).\\n[8] Ömer Aydın and Enis Karaarslan. 2023. Is ChatGPT Leading Generative AI? What is Beyond Expectations? What is Beyond Expectations (2023).\\n[9] Amos Azaria. 2023. ChatGPT: More Human-Like Than Computer-Like, but Not Necessarily in a Good Way. (2023).\\n[10] David Baidoo-Anu and Leticia Owusu Ansah. 2023. Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential\\nBenefits of ChatGPT in Promoting Teaching and Learning. Available at SSRN 4337484 (2023).\\n[11] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. Advances in neural information processing\\nsystems 13 (2000).\\n[12] Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021. Tfix: Learning to fix coding errors with a text-to-text transformer. In\\nInternational Conference on Machine Learning. PMLR, 780–791.\\n[13] Miles Kruppa Berber Jin. 2023.\\nChatGPT Creator Is Talking to Investors About Selling Shares at $29 Billion Valuation.\\nhttps://www.wsj.com/articles/chatgpt-creator-openai-is-in-talks-for-tender-offer-that-would-value-it-at-29-billion-11672949279 (2023).\\n[14] Lea Bishop. 2023. Can ChatGPT’Think Like a Lawyer?’A Socratic Dialogue. A Socratic Dialogue (January 26, 2023) (2023).\\n[15] Back To Blog. 2023. AI and Academic Integrity: How AI Technology Might Influence the Future of Scholarly Publishing. (2023).\\n[16] Ali Borji. 2023. A Categorical Archive of ChatGPT Failures. arXiv preprint arXiv:2302.03494 (2023).\\n[17] Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In ICLR.\\n[18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems (2020).\\n[19] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott\\nLundberg, et al. 2023. Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712 (2023).\\n[20] Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li. 2022. A survey on generative diffusion model. arXiv\\npreprint arXiv:2209.02646 (2022).\\n[21] Ashley\\nCapoot.\\n2023.\\nMicrosoft\\nannounces\\nnew\\nmultibillion-dollar\\ninvestment\\nin\\nChatGPT-maker\\nOpenAI.\\nhttps://www.cnbc.com/2023/01/23/microsoft-announces-multibillion-dollar-investment-in-chatgpt-maker-openai.html (2023).\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n23\\n[22] JP Carrasco, E García, DA Sánchez, PD Estrella Porter, L De La Puente, J Navarro, and A Cerame. 2023. Is\" ChatGPT\" capable of passing the\\n2022 MIR exam? Implications of artificial intelligence in medical education in Spain¿ Es capaz “ChatGPT” de aprobar el examen MIR de 2022?\\nImplicaciones de la inteligencia artificial en la educación. (2023).\\n[23] Davide Castelvecchi. 2022. Are ChatGPT and AlphaCode going to replace programmers? Nature (2022).\\n[24] Poulomi Chatterjee. 2023. From Non-Profit to For-Profit: How OpenAI Plans to Make Money. https://analyticsindiamag.com/from-non-profit-to-for-\\nprofit-how-openai-plans-to-make-money/ (2023).\\n[25] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).\\n[26] Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin Smith, and Joshua B Tenenbaum. 2022. Are Deep Neural Networks SMARTer than Second\\nGraders? arXiv preprint arXiv:2212.09993 (2022).\\n[27] Simon Chesterman. 2023. AI-generated content is taking over the world. But who owns it? But Who Owns it (2023).\\n[28] Jonathan H Choi, Kristin E Hickman, Amy Monahan, and Daniel Schwarcz. 2023. ChatGPT Goes to Law School. Available at SSRN (2023).\\n[29] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles\\nSutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling Language Modeling with Pathways. (2022).\\n[30] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators rather than\\ngenerators. arXiv preprint arXiv:2003.10555 (2020).\\n[31] Devin Coldewey. 2019. OpenAI shifts from nonprofit to ‘capped-profit’ to attract capital. https://techcrunch.com/2019/03/11/openai-shifts-from-\\nnonprofit-to-capped-profit-to-attract-capital/ (2019).\\n[32] Debby RE Cotton, Peter A Cotton, and J Reuben Shipway. 2023. Chatting and Cheating. Ensuring academic integrity in the era of ChatGPT. (2023).\\n[33] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. 2022. Diffusion models in vision: A survey. arXiv preprint\\narXiv:2209.04747 (2022).\\n[34] Jaime A Teixeira da Silva. 2023. Is ChatGPT a valid author? Nurse Education in Practice (2023), 103600.\\n[35] Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan. 2020. A survey of multilingual neural machine translation. ACM Computing Surveys (CSUR)\\n53, 5 (2020), 1–38.\\n[36] Robert Dale. 2021. GPT-3: What’s it good for? Natural Language Engineering 27, 1 (2021), 113–118.\\n[37] Bibhu Dash and Pawankumar Sharma. 2023. Are ChatGPT and Deepfake Algorithms Endangering the Cybersecurity Industry? A Review. (2023).\\n[38] Luigi De Angelis, Francesco Baglivo, Guglielmo Arzilli, Gaetano Pierpaolo Privitera, Paolo Ferragina, Alberto Eugenio Tozzi, and Caterina Rizzo.\\n2023. ChatGPT and the Rise of Large Language Models: The New AI-Driven Infodemic Threat in Public Health. Available at SSRN 4352931 (2023).\\n[39] Ben Derico. 2023. ChatGPT bug leaked users’ conversation histories. BBC news (2023).\\n[40] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language\\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\\n[41] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language\\nunderstanding. NAACL (2019).\\n[42] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[43] Dat Duong and Benjamin D Solomon. 2023. Analysis of large-language model versus human performance for genetics questions. medRxiv (2023),\\n2023–01.\\n[44] J David Eisenberg and Amelia Bellamy-Royds. 2014. SVG essentials: Producing scalable vector graphics with XML. \" O’Reilly Media, Inc.\".\\n[45] Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. 2021. Automatic text summarization: A comprehensive survey. Expert\\nSystems with Applications 165 (2021), 113679.\\n[46] Joshua P Ellis. 2017. Tikz-feynman: Feynman diagrams with tikz. Computer Physics Communications 210 (2017), 103–123.\\n[47] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An Early Look at the Labor Market Impact Potential of\\nLarge Language Models. arXiv preprint arXiv:2303.10130 (2023).\\n[48] Ed Felten, Manav Raj, and Robert Seamans. 2023. How will Language Modelers like ChatGPT Affect Occupations and Industries? arXiv preprint\\narXiv:2303.01157 (2023).\\n[49] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and\\nJulius Berner. 2023. Mathematical capabilities of ChatGPT. arXiv preprint arXiv:2301.13867 (2023).\\n[50] Fronty. 2022. What is Open AI and What Does It Do? https://fronty.com/what-is-openai-and-what-does-it-do/ (2022).\\n[51] Zhe Gan, Ricardo Henao, David Carlson, and Lawrence Carin. 2015. Learning deep sigmoid belief networks with data augmentation. In Artificial\\nIntelligence and Statistics. PMLR, 268–276.\\n[52] Wayne Geerling, G Dirk Mateer, Jadrian Wooten, and Nikhil Damodaran. 2023. Is ChatGPT Smarter than a Student in Principles of Economics?\\nAvailable at SSRN 4356034 (2023).\\n[53] A Gilson, C Safranek, T Huang, V Socrates, L Chi, RA Taylor, and D Chartash. 2022. How does ChatGPT perform on the medical licensing exams?\\nthe implications of large language models for medical education and knowledge assessment. medRxiv (2022).\\nManuscript submitted to ACM\\n24\\nZhang et al.\\n[54] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative\\nadversarial nets. In NeurIPS.\\n[55] Roberto Gozalo-Brizuela and Eduardo C Garrido-Merchan. 2023. ChatGPT is not all you need. A State of the Art Review of large Generative AI\\nmodels. arXiv preprint arXiv:2301.04655 (2023).\\n[56] Ulf Grenander and Michael I Miller. 1994. Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B\\n(Methodological) 56, 4 (1994), 549–581.\\n[57] Joko Gunawan. 2023. Exploring the future of nursing: Insights from the ChatGPT model. Belitung Nursing Journal 9, 1 (2023), 1–5.\\n[58] Daniela Haluza and David Jungwirth. 2023. Artificial Intelligence and ten societal megatrends: a GPT-3 case study. (2023).\\n[59] Michael Haman and Milan Školník. 2023. Using ChatGPT to conduct a literature review. Accountability in Research (2023), 1–3.\\n[60] Robert Hanna. 2023. How and Why ChatGPT Failed The Turing Test. (2023).\\n[61] Stuart Hargreaves. 2023. ‘Words Are Flowing Out Like Endless Rain Into a Paper Cup’: ChatGPT & Law School Assessments. The Chinese University\\nof Hong Kong Faculty of Law Research Paper 2023-03 (2023).\\n[62] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. 2023. The political ideology of conversational AI: Converging evidence on ChatGPT’s\\npro-environmental, left-libertarian orientation. arXiv preprint arXiv:2301.01768 (2023).\\n[63] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In\\nCVPR.\\n[64] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In\\nCVPR.\\n[65] Urfa Khairatun Hisan and Muhammad Miftahul Amri. 2023. ChatGPT and Medical Education: A Double-Edged Sword. (2023).\\n[66] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi,\\nDavid J Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022).\\n[67] Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac, and Max Welling. 2022. Equivariant diffusion for molecule generation in 3d. In ICML.\\nPMLR, 8867–8887.\\n[68] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. 2019. A comprehensive survey of deep learning for image captioning.\\nACM Computing Surveys (CsUR) 51, 6 (2019), 1–36.\\n[69] Alex Hughes. 2023. ChatGPT: Everything you need to know about OpenAI’s GPT-4 tool. Science Focus (2023).\\n[70] Sun Huh. 2023. Are ChatGPT’s knowledge and interpretation ability comparable to those of medical students in Korea for taking a parasitology\\nexamination?: a descriptive study. Journal of Educational Evaluation for Health Professions 20 (2023), 1.\\n[71] Jonathan Hui. 2018. RL — Proximal Policy Optimization (PPO) Explained. https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-\\nexplained-77f014ec3f12 (2018).\\n[72] Adam Hulman, Ole Lindgaard Dollerup, Jesper Friis Mortensen, Matthew Fenech, Kasper Norman, Henrik Stoevring, and Troels Krarup Hansen.\\n2023. ChatGPT-versus human-generated answers to frequently asked questions about diabetes: a Turing test-inspired survey among employees of\\na Danish diabetes center. medRxiv (2023), 2023–02.\\n[73] Shulei Ji, Jing Luo, and Xinyu Yang. 2020. A comprehensive survey on deep music generation: Multi-level representations, algorithms, evaluations,\\nand future directions. arXiv preprint arXiv:2011.06801 (2020).\\n[74] David Jungwirth and Daniela Haluza. 2023. Forecasting Geopolitical Conflicts Using GPT-3 AI: Reali-Ty-Check One Year into the 2022 Ukraine\\nWar. (2023).\\n[75] Mihir Kale and Abhinav Rastogi. 2020. Text-to-text pre-training for data-to-text tasks. arXiv preprint arXiv:2005.10433 (2020).\\n[76] Ayoosh Kathuria. 2021. Getting Started With OpenAI Gym: The Basic Building Blocks. https://blog.paperspace.com/getting-started-with-openai-gym/\\n(2021).\\n[77] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023. GPT-4 Passes the Bar Exam. Available at SSRN 4389233\\n(2023).\\n[78] Grace Kay. 2023.\\nThe history of ChatGPT creator OpenAI, which Elon Musk helped found before parting ways and criticizing.\\nhttps://www.businessinsider.com/history-of-openai-company-chatgpt-elon-musk-founded-2022-12 (2023).\\n[79] Samantha Murphy Kelly. 2023. ChatGPT passes exams from law and business schools. CNN Business (2023).\\n[80] Mohammad Khalil and Erkan Er. 2023. Will ChatGPT get you caught? Rethinking of Plagiarism Detection. arXiv preprint arXiv:2302.04335 (2023).\\n[81] Rehan Ahmed Khan, Masood Jawaid, Aymen Rehan Khan, and Madiha Sajjad. 2023. ChatGPT-Reshaping medical education and clinical management.\\nPakistan Journal of Medical Sciences 39, 2 (2023).\\n[82] Sung Kim. 2022. Replace Grammarly Premium with OpenAI ChatGPT. https://medium.com/geekculture/replace-grammarly-premium-with-openai-\\nchatgpt-320049179c79 (2022).\\n[83] Yoon Kim, Yacine Jernite, David Sontag, and Alexander Rush. 2016. Character-aware neural language models. In Proceedings of the AAAI conference\\non artificial intelligence, Vol. 30.\\n[84] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).\\n[85] Dennis H Klatt. 1987. Review of text-to-speech conversion for English. The Journal of the Acoustical Society of America 82, 3 (1987), 737–793.\\n[86] Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura Barnes, and Donald Brown. 2019. Text classification\\nalgorithms: A survey. Information 10, 4 (2019), 150.\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n25\\n[87] Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel\\nDiaz-Candido, James Maningo, et al. 2023. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language\\nmodels. PLOS Digital Health 2, 2 (2023), e0000198.\\n[88] Boni Kutela, Kelvin Msechu, Subasish Das, and Emmanuel Kidando. 2023. Chatgpt’s Scientific Writings: A Case Study on Traffic Safety. Available\\nat SSRN 4329120 (2023).\\n[89] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised\\nlearning of language representations. arXiv preprint arXiv:1909.11942 (2019).\\n[90] Hugo Larochelle and Iain Murray. 2011. The neural autoregressive distribution estimator. In Proceedings of the fourteenth international conference\\non artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 29–37.\\n[91] Stefan Larsson and Fredrik Heintz. 2020. Transparency in artificial intelligence. Internet Policy Review 9, 2 (2020).\\n[92] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:\\nDenoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461\\n(2019).\\n[93] Jinyu Li et al. 2022. Recent advances in end-to-end automatic speech recognition. APSIPA Transactions on Signal and Information Processing 11, 1\\n(2022).\\n[94] Michael Liebrenz, Roman Schleifer, Anna Buadze, Dinesh Bhugra, and Alexander Smith. 2023. Generating scholarly content with ChatGPT: ethical\\nchallenges for medical publishing. The Lancet Digital Health 5, 3 (2023), e105–e106.\\n[95] Zhicheng Lin. 2023. Why and how to embrace AI such as ChatGPT in your academic life. (2023).\\n[96] Janna Lipenkova. 2023. Overcoming the Limitations of Large Language Models How to enhance LLMs with human-like cognitive skills. (2023).\\n[97] Alexander H Liu, Wei-Ning Hsu, Michael Auli, and Alexei Baevski. 2023. Towards end-to-end unsupervised speech recognition. In 2022 IEEE\\nSpoken Language Technology Workshop (SLT). IEEE, 221–228.\\n[98] Siru Liu, Aileen P Wright, Barron L Patterson, Jonathan P Wanderer, Robert W Turer, Scott D Nelson, Allison B McCoy, Dean F Sittig, and Adam\\nWright. 2023. Assessing the Value of ChatGPT for Clinical Decision Support Optimization. medRxiv (2023), 2023–02.\\n[99] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).\\n[100] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision\\ntransformer using shifted windows. ICCV.\\n[101] Reed Albergotti Liz Hoffman. 2023. Microsoft eyes $10 billion bet on ChatGPT. https://www.semafor.com/article/01/09/2023/microsoft-eyes-10-billion-\\nbet-on-chatgpt (2023).\\n[102] Calum Macdonald, Davies Adeloye, Aziz Sheikh, and Igor Rudan. 2023. Can ChatGPT draft a research article? An example of population-level\\nvaccine effectiveness analysis. Journal of Global Health 13 (2023).\\n[103] Rupert Macey-Dare. 2023. ChatGPT & Generative AI Systems as Quasi-Expert Legal Advice Lawyers-Case Study Considering Potential Appeal\\nAgainst Conviction of Tom Hayes. Available at SSRN 4342686 (2023).\\n[104] Nitin Madnani and Bonnie J Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics\\n36, 3 (2010), 341–387.\\n[105] Gengchen Mai, Chris Cundy, Kristy Choi, Yingjie Hu, Ni Lao, and Stefano Ermon. 2022. Towards a foundation model for geospatial artificial\\nintelligence (vision paper). In Proceedings of the 30th International Conference on Advances in Geographic Information Systems. 1–4.\\n[106] Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. 2016. Generating images from captions with attention. ICLR (2016).\\n[107] Benjamin Marchandot, Kensuke Matsushita, Adrien Carmona, Antonin Trimaille, and Olivier Morel. 2023. ChatGPT: The Next Frontier in Academic\\nWriting for Cardiologists or a Pandora’s Box of Ethical Dilemmas. European Heart Journal Open (2023), oead007.\\n[108] Mochammad Ircham Maulana. 2023. Leveraging Zoom video-conferencing features in interview data generation during the Covid-19 pandemic. In\\nResearch and Teaching in a Pandemic World: The Challenges of Establishing Academic Identities During Times of Crisis. Springer, 391–407.\\n[109] Lev Maximov. 2023. Do You Know English Grammar Better Than ChatGPT? https://medium.com/writing-cooperative/do-you-know-english-grammar-\\nbetter-than-chatgpt-8fc550f23681 (2023).\\n[110] Robert W McGee. 2023. Is Chat Gpt Biased Against Conservatives? An Empirical Study. An Empirical Study (February 15, 2023) (2023).\\n[111] Forrest McKee and David Noever. 2022. Chatbots in a Botnet World. arXiv preprint arXiv:2212.11126 (2022).\\n[112] Walaa Medhat, Ahmed Hassan, and Hoda Korashy. 2014. Sentiment analysis algorithms and applications: A survey. Ain Shams engineering journal\\n5, 4 (2014), 1093–1113.\\n[113] Ateev Mehrotra. 2023. Symptom Checkers & ChatGPT. https://scholar.harvard.edu/mehrotra/symptom-checkers (2023).\\n[114] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. 2022. Autosdf: Shape priors for 3d completion, reconstruction and\\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 306–315.\\n[115] Yasumasa Miyamoto and Kyunghyun Cho. 2016. Gated word-character recurrent language model. arXiv preprint arXiv:1606.01700 (2016).\\n[116] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. 2023. Dreamix: Video\\ndiffusion models are general video editors. arXiv preprint arXiv:2302.01329 (2023).\\n[117] Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. Wt5?! training text-to-text models to explain\\ntheir predictions. arXiv preprint arXiv:2004.14546 (2020).\\nManuscript submitted to ACM\\n26\\nZhang et al.\\n[118] Bianke Neethling. 2023. ChatGPT breaks record with 100 million users – and investors come flocking. https://dailyinvestor.com/world/8520/chatgpt-\\nbreaks-record-with-100-million-users-and-investors-come-flocking/ (2023).\\n[119] Jennimai Nguyen. 2022. No, the Google AI isn’t sentient, but it likely is racist and sexist. https://mashable.com/article/google-ai-racist-sexist-bias\\n(2022).\\n[120] David Noever and Forrest McKee. 2023. Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models. arXiv preprint\\narXiv:2301.13382 (2023).\\n[121] Siobhan O’Connor et al. 2022. Open artificial intelligence platforms in nursing education: Tools for academic progress or abuse? Nurse Education\\nin Practice 66 (2022), 103537–103537.\\n[122] OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023).\\n[123] OpenAI. 2023. Research index. https://openai.com/research (2023).\\n[124] Achraf Oussidi and Azeddine Elhassouny. 2018. Deep generative models: Survey. In 2018 International Conference on Intelligent Systems and\\nComputer Vision (ISCV). IEEE, 1–8.\\n[125] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex\\nRay, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022).\\n[126] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. 2019. Specaugment: A simple data\\naugmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779 (2019).\\n[127] John V Pavlik. 2023. Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media\\nEducation. Journalism & Mass Communication Educator (2023), 10776958221149577.\\n[128] Tammy Pettinato Oltz. 2023. ChatGPT, Professor of Law. Professor of Law (February 4, 2023) (2023).\\n[129] Oleksandra Poquet Pfeffer, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. 2023.\\nChatGPT for Good? On Opportunities and Challenges of Large Language Models for Education. (2023).\\n[130] Kelsey Piper. 2022. Why is Meta’s new AI chatbot so bad? https://www.vox.com/future-perfect/23307252/meta-facebook-bad-ai-chatbot-blenderbot\\n(2022).\\n[131] Michael Polonsky and Jeff Rotman. 2023. Should Artificial Intelligent (AI) Agents be Your Co-author? Arguments in favour, informed by ChatGPT.\\nArguments in favour, informed by ChatGPT (February 6, 2023) (2023).\\n[132] Samuel A Prieto, Eyob T Mengiste, and Borja García de Soto. 2023. Investigating the use of ChatGPT for the scheduling of construction projects.\\narXiv preprint arXiv:2302.02805 (2023).\\n[133] Junaid Qadir. 2022. Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education. (2022).\\n[134] Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. 2020. Prophetnet: Predicting future\\nn-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063 (2020).\\n[135] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale\\nweak supervision. arXiv preprint arXiv:2212.04356 (2022).\\n[136] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).\\n[137] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask\\nlearners. OpenAI blog (2019).\\n[138] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring\\nthe limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020), 1–67.\\n[139] Ric Raftis. 2023. How to use ChatGPT for Divergent Thinking in Obsidian and PKMs. (2023).\\n[140] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents.\\narXiv preprint arXiv:2204.06125 (2022).\\n[141] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image\\ngeneration. In ICML.\\n[142] Arya S Rao, John Kim, Meghana Kamineni, Michael Pang, Winston Lie, and Marc Succi. 2023. Evaluating ChatGPT as an Adjunct for Radiologic\\nDecision-Making. medRxiv (2023), 2023–02.\\n[143] Arya S Rao, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dryer, and Marc D Succi. 2023.\\nAssessing the Utility of ChatGPT Throughout the Entire Clinical Workflow. medRxiv (2023), 2023–02.\\n[144] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image\\nsynthesis. In International conference on machine learning. PMLR, 1060–1069.\\n[145] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2019. Fastspeech: Fast, robust and controllable text to speech.\\nAdvances in Neural Information Processing Systems 32 (2019).\\n[146] Jesus Rodriguez. 2022. How to Create Diagrams With ChatGPT. https://jrodthoughts.medium.com/instructgpt-is-one-of-the-models-behidn-the-magic-\\nof-chatgpt-59813dd8aabc (2022).\\n[147] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion\\nmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684–10695.\\n[148] Pericles ‘asher’ Rospigliosi. 2023. Artificial intelligence in teaching and learning: what questions should we ask of ChatGPT? , 3 pages.\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n27\\n[149] Jürgen Rudolph, Samson Tan, and Shannon Tan. 2023. ChatGPT: Bullshit spewer or the end of traditional assessments in higher education? Journal\\nof Applied Learning and Teaching 6, 1 (2023).\\n[150] run.ai. 2023. NVIDIA DGX: Under the Hood of DGX-1, DGX-2 and A100. https://www.run.ai/guides/nvidia-a100/nvidia-dgx (2023).\\n[151] Soroush Saghafian. 2023. The Analytics Science Behind ChatGPT: Human, Algorithm, or a Human-Algorithm Centaur? (2023).\\n[152] Tirthankar Ghosal Saikiran Chandha, Sucheth R. 2023. Setting the Scene: How Artificial Intelligence is reshaping how we consume and deliver\\nresearch. https://upstream.force11.org/setting-the-scene-ai/ (2023).\\n[153] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. 2017. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture\\nlikelihood and other modifications. arXiv preprint arXiv:1701.05517 (2017).\\n[154] Malik Sallam. 2023. ChatGPT Utility in Health Care Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid\\nConcerns. In Healthcare, Vol. 11. MDPI, 887.\\n[155] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019. wav2vec: Unsupervised pre-training for speech recognition. arXiv\\npreprint arXiv:1904.05862 (2019).\\n[156] Ali Shiri. 2023. ChatGPT and Academic Integrity. Information Matters 3, 2 (2023).\\n[157] Olivia Solon. 2023. The Tech Behind Those Amazing, Flawed New Chatbots. Bloomberg News (2023).\\n[158] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. Mass: Masked sequence to sequence pre-training for language generation. arXiv\\npreprint arXiv:1905.02450 (2019).\\n[159] Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients of the data distribution, Vol. 32.\\n[160] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. 2020. Sliced score matching: A scalable approach to density and score estimation. In\\nUncertainty in Artificial Intelligence. PMLR, 574–584.\\n[161] Mashrin Srivastava. 2023. A day in the life of ChatGPT as an academic reviewer: Investigating the potential of large language model for scientific\\nliterature review. (2023).\\n[162] Daniel Street and Joseph Wilck. 2023. ’Let’s Have a Chat’: Principles for the Effective Application of ChatGPT and Large Language Models in the\\nPractice of Forensic Accounting. Available at SSRN 4351817 (2023).\\n[163] Fei Sun. 2022. ChatGPT, the Start of a New Era. (2022).\\n[164] Nigar M Shafiq Surameery and Mohammed Y Shakor. 2023. Use Chat GPT to Solve Programming Bugs. International Journal of Information\\nTechnology & Computer Engineering (IJITC) ISSN: 2455-5290 3, 01 (2023), 17–22.\\n[165] Victor Tangermann. 2023.\\n89 PERCENT OF COLLEGE STUDENTS ADMIT TO USING CHATGPT FOR HOMEWORK, STUDY CLAIMS.\\nhttps://futurism.com/the-byte/students-admit-chatgpt-homework (2023).\\n[166] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. 2023. GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis. arXiv preprint\\narXiv:2301.12959 (2023).\\n[167] Paul Taylor. 2009. Text-to-speech synthesis. Cambridge university press.\\n[168] Mohamad-Hani Temsah, Amr Jamal, and Jaffar A Al-Tawfiq. 2023. Reflection with ChatGPT about the excess death after the COVID-19 pandemic.\\nNew Microbes and New Infections (2023).\\n[169] Vincent Terrasi. 2023. GPT-4: How Is It Different From GPT-3.5? https://www.searchenginejournal.com/gpt-4-vs-gpt-3-5/482463/#close (2023).\\n[170] H Holden Thorp. 2023. ChatGPT is fun, but not an author. , 313–313 pages.\\n[171] Oguzhan TOPSAKAL and Elif TOPSAKAL. 2023. Framework for A Foreign Language Teaching Software for Children Utilizing AR, Voicebots and\\nChatGPT (Large Language Models). The Journal of Cognitive Systems 7, 2 (2023), 33–38.\\n[172] ChatGPT Generative Pre-trained Transformer and Alex Zhavoronkov. 2022. Rapamycin in the context of Pascal’s Wager: generative pre-trained\\ntransformer perspective. Oncoscience 9 (2022), 82.\\n[173] Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, and Steinn Steingrimsson. 2022. Can GPT-3 write an academic paper on\\nitself, with minimal human input? (2022).\\n[174] Alan Truly. 2023. Bing Chat: how to use Microsoft’s own version of ChatGPT. https://www.digitaltrends.com/computing/how-to-use-microsoft-\\nchatgpt-bing-edge/ (2023).\\n[175] Kohei Ueda and Yuki Yamada. 2023. ChatGPT is not an author, but then, who is eligible for authorship? (2023).\\n[176] Kadir Uludag. 2023. The use of AI-supported Chatbot in Psychology. Available at SSRN 4331367 (2023).\\n[177] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. 2016. Neural autoregressive distribution estimation. The\\nJournal of Machine Learning Research 17, 1 (2016), 7184–7220.\\n[178] Benigno Uria, Iain Murray, and Hugo Larochelle. 2013. RNADE: The real-valued neural autoregressive density-estimator. Advances in Neural\\nInformation Processing Systems 26 (2013).\\n[179] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray\\nKavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio. In The 9th ISCA Speech Synthesis Workshop.\\n[180] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. 2016. Conditional image generation with pixelcnn\\ndecoders. Advances in neural information processing systems 29 (2016).\\n[181] Wouter van Heeswijk. 2022. Trust Region Policy Optimization (TRPO) Explained. https://towardsdatascience.com/trust-region-policy-optimization-\\ntrpo-explained-4b56bd206fc2 (2022).\\nManuscript submitted to ACM\\n28\\nZhang et al.\\n[182] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is\\nall you need. In NeurIPS.\\n[183] Randy Joy Magno Ventayen. 2023. OpenAI ChatGPT Generated Results: Similarity Index of Artificial Intelligence-Based Contents. Available at\\nSSRN 4332664 (2023).\\n[184] Manish Verma. 2023. Novel Study on AI-Based Chatbot (ChatGPT) Impacts on the Traditional Library Management. (2023).\\n[185] Lyan Verwimp, Joris Pelemans, Patrick Wambacq, et al. 2017. Character-word LSTM language models. arXiv preprint arXiv:1704.02813 (2017).\\n[186] Pascal Vincent. 2011. A connection between score matching and denoising autoencoders. Neural computation 23, 7 (2011), 1661–1674.\\n[187] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2016. Show and tell: Lessons learned from the 2015 mscoco image captioning\\nchallenge. IEEE transactions on pattern analysis and machine intelligence 39, 4 (2016), 652–663.\\n[188] Karan Virdi. 2022. Google issues ‘code-red’ as Open AI’s ChatGPT garners popularity. https://itmunch.com/google-issues-code-red-alert-as-open-ai-\\nbecomes-popular/ (2022).\\n[189] Vaishak V.Kumar. 2019. Soft Actor-Critic Demystified. https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665 (2019).\\n[190] Dong Wang, Xiaodong Wang, and Shaohe Lv. 2019. An overview of end-to-end automatic speech recognition. Symmetry 11, 8 (2019), 1018.\\n[191] Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon. 2023. Can ChatGPT Write a Good Boolean Query for Systematic Review\\nLiterature Search? arXiv preprint arXiv:2302.03495 (2023).\\n[192] Xinyi Wang, Zhenye Gong, Guoxin Wang, Jingdan Jia, Ying Xu, Jialu Zhao, Qingye Fan, Shaun Wu, Weiguo Hu, and Xiaoyang Li. 2023. ChatGPT\\nPerforms on the Chinese National Medical Licensing Examination. (2023).\\n[193] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt.\\n2023. A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. arXiv preprint arXiv:2302.11382 (2023).\\n[194] Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. ChatGPT Prompt Patterns for Improving Code Quality,\\nRefactoring, Requirements Elicitation, and Software Design. arXiv preprint arXiv:2303.07839 (2023).\\n[195] Clare Williams. 2023. Hype, or the future of learning and teaching? 3 Limits to AI’s ability to write student essays. (2023).\\n[196] Thomas Wischmeyer. 2020. Artificial intelligence and transparency: opening the black box. Regulating artificial intelligence (2020), 75–101.\\n[197] writecream. 2022. Can ChatGPT Correct Grammar? https://www.writecream.com/can-chatgpt-correct-grammar/ (2022).\\n[198] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. 2022. Gan inversion: A survey. IEEE Transactions on\\nPattern Analysis and Machine Intelligence (2022).\\n[199] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image\\ngeneration with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition.\\n1316–1324.\\n[200] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively\\nmultilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934 (2020).\\n[201] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. 2022. Diffusion probabilistic modeling for video generation. arXiv preprint arXiv:2203.09481\\n(2022).\\n[202] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei. 2017. Boosting image captioning with attributes. In Proceedings of the IEEE international\\nconference on computer vision. 4894–4902.\\n[203] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. 2023. A Comprehensive\\nCapability Analysis of GPT-3 and GPT-3.5 Series Models. arXiv preprint arXiv:2303.10420 (2023).\\n[204] Will Yeadon, Oto-Obong Inyang, Arin Mizouri, Alex Peach, and Craig Testrow. 2022. The Death of the Short-Form Physics Essay in the Coming AI\\nRevolution. arXiv preprint arXiv:2212.11661 (2022).\\n[205] Yee Hui Yeo, Jamil S Samaan, Wee Han Ng, Peng-Sheng Ting, Hirsh Trivedi, Aarshi Vipani, Walid Ayoub, Ju Dong Yang, Omer Liran, Brennan\\nSpiegel, et al. 2023. Assessing the performance of ChatGPT in answering questions regarding cirrhosis and hepatocellular carcinoma. medRxiv\\n(2023), 2023–02.\\n[206] Nicole Shu Ling Yeo-Teh and Bor Luen Tang. 2023. Letter to Editor: NLP systems such as ChatGPT cannot be listed as an author because these\\ncannot fulfill widely adopted authorship criteria. Accountability in Research just-accepted (2023).\\n[207] Adam Zaremba and Ender Demir. 2023. ChatGPT: Unlocking the Future of NLP in Finance. Available at SSRN 4323643 (2023).\\n[208] Ali Zarifhonarvar. 2023. Economics of ChatGPT: A Labor Market View on the Occupational Impact of Artificial Intelligence. Available at SSRN\\n4350925 (2023).\\n[209] Aeron Zentner. 2022. Applied Innovation: Artificial Intelligence in Higher Education. Available at SSRN 4314180 (2022).\\n[210] Aeron Zentner. 2022. Applied Innovation: Artificial Intelligence in Higher Education. Available at SSRN 4314180 (2022).\\n[211] Bo Zhang. 2023. Preparing Educators and Students for ChatGPT and AI Technology in Higher Education. (2023).\\n[212] Chaoning Zhang, Chenshuang Zhang, Junha Song, John Seon Keun Yi, Kang Zhang, and In So Kweon. 2022. A survey on masked autoencoder for\\nself-supervised learning in vision and beyond. arXiv preprint arXiv:2208.00173 (2022).\\n[213] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. 2023. Text-to-image Diffusion Models in Generative AI: A Survey.\\narXiv preprint arXiv:2303.07909 (2023).\\n[214] Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun,\\nLe Luang Huy, Donguk kim, Sung-Ho Bae, Lik-Hang Lee, Yang Yang, Heng Tao Shen, In So Kweon, and Choong Seon Hong. 2023. A Complete\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n29\\nSurvey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need? arXiv preprint arXiv:2303.11717 (2023).\\n[215] Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, and In So Kweon. 2023. A Survey on Audio\\nDiffusion Models: Text To Speech Synthesis and Enhancement in Generative AI. arXiv preprint arXiv:2303.13336 (2023).\\n[216] Chaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. 2022. Dual Temperature Helps Contrastive Learning Without\\nMany Negative Samples: Towards Understanding and Simplifying MoCo. In CVPR.\\n[217] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon. 2022. Decoupled Adversarial Contrastive\\nLearning for Self-supervised Adversarial Robustness. In ECCV. Springer, 725–742.\\n[218] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, and In So Kweon. 2022. How Does SimSiam Avoid Collapse\\nWithout Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning. In ICLR.\\n[219] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic\\nimage synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision. 5907–5915.\\n[220] Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, and Chaoning Zhang. 2023. A Survey on Graph\\nDiffusion Models: Generative AI in Science for Molecule, Protein and Material. ResearchGate 10.13140/RG.2.2.26493.64480 (2023).\\n[221] Shiliang Zhang, Ming Lei, Zhijie Yan, and Lirong Dai. 2018. Deep-FSMN for large vocabulary continuous speech recognition. In 2018 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 5869–5873.\\n[222] Guodong Troy Zhao. 2023. How to use ChatGPT in product management. (2023).\\n[223] Qitong Zhong, Xing Tan, Ruixing Du, Jiacheng Liu, Longfei Liao, Cheng Wang, Ruiyan Sun, Zhenchen Tang, Jie Ren, Chalachew Mebrahtu, et al.\\n2023. Is ChatGPT a Reliable Source for Writing Review Articles in Catalysis Research? A Case Study on CO2 Hydrogenation to Higher Alcohols.\\n(2023).\\n[224] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023. A comprehensive survey on\\npretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419 (2023).\\n[225] Chao Zhou, Cheng Qiu, and Daniel E Acuna. 2022. Paraphrase Identification with Deep Learning: A Review of Datasets and Methods. arXiv\\npreprint arXiv:2212.06933 (2022).\\n[226] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Exploring AI Ethics of ChatGPT: A Diagnostic Analysis. arXiv preprint\\narXiv:2301.12867 (2023).\\nManuscript submitted to ACM\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 출력\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-s6hO_3vk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
