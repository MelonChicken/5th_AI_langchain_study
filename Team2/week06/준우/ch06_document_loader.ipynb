{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 01. 도큐먼트(Document) 의 구조",
   "id": "f3a004598ad018c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "실습에 활용한 문서\n",
    "- 소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "\n",
    "- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- 링크: https://spri.kr/posts/view/23669\n",
    "- 파일명: SPRI_AI_Brief_2023년12월호_F.pdf"
   ],
   "id": "f2093cd75b238cbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Document",
   "id": "bec8f2d854059f2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LangChain 의 기본 문서 객체입니다.\n",
    "\n",
    "속성\n",
    "- `page_content`: 문서의 내용을 나타내는 문열입니다.\n",
    "- `metadata`: 문서의 메타데이터를 나타내는 딕셔너리입니다."
   ],
   "id": "8f3262160b940cc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:01:11.963057Z",
     "start_time": "2025-12-13T05:01:11.408152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document = Document(\"안녕하세요? 이건 랭체인의 도큐먼드 입니다\")"
   ],
   "id": "24c7eb3e13779ea4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-13T05:02:11.180325Z",
     "start_time": "2025-12-13T05:02:11.172178Z"
    }
   },
   "source": [
    "# 도큐먼트의 속성 확인\n",
    "document.__dict__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {},\n",
       " 'page_content': '안녕하세요? 이건 랭체인의 도큐먼드 입니다',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "metadata 에 속성 추가",
   "id": "805bdaa6694791ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:02:22.127785Z",
     "start_time": "2025-12-13T05:02:22.123278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 메타데이터 추가\n",
    "document.metadata[\"source\"] = \"TeddyNote\"\n",
    "document.metadata[\"page\"] = 1\n",
    "document.metadata[\"author\"] = \"Teddy\""
   ],
   "id": "e21bdb54894aa07f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:02:44.818285Z",
     "start_time": "2025-12-13T05:02:44.812500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 도큐먼트의 속성 확인\n",
    "document.metadata"
   ],
   "id": "8ca1777dad6bb300",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'TeddyNote', 'page': 1, 'author': 'Teddy'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Document Loader",
   "id": "87a566a5240e2ddb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "다양한 파일의 형식으로부터 불러온 내용을 문서(Document) 객체로 변환하는 역할을 합니다.\n",
    "\n",
    "주요 Loader\n",
    "- `PyPDFLoader`: PDF 파일을 로드하는 로더입니다.\n",
    "- `CSVLoader`: CSV 파일을 로드하는 로더입니다.\n",
    "- `UnstructuredHTMLLoader`: HTML 파일을 로드하는 로더입니다.\n",
    "- `JSONLoader`: JSON 파일을 로드하는 로더입니다.\n",
    "- `TextLoader`: 텍스트 파일을 로드하는 로더입니다.\n",
    "- `DirectoryLoader`: 디렉토리를 로드하는 로더입니다."
   ],
   "id": "4a0beaa3848756c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:10:40.896462Z",
     "start_time": "2025-12-13T05:10:40.893071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 예제 파일 경로\n",
    "FILE_PATH = (\"./week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\")"
   ],
   "id": "ae0872894f59ed9b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:10:45.840764Z",
     "start_time": "2025-12-13T05:10:45.124238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 로더 설정\n",
    "loader = PyPDFLoader(FILE_PATH)\n"
   ],
   "id": "ace9ee85c43166c3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`load()`\n",
    "\n",
    "문서를 로드하여 반환합니다.\n",
    "\n",
    "반환된 결과는 `List[Document]` 형태입니다."
   ],
   "id": "75ab6930b1cf5715"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:11:59.899837Z",
     "start_time": "2025-12-13T05:11:57.602929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PDF 로더\n",
    "docs = loader.load()\n",
    "\n",
    "# 로드된 문서의 수 확인\n",
    "len(docs)"
   ],
   "id": "dee64383c8537cda",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:12:11.494984Z",
     "start_time": "2025-12-13T05:12:11.489541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 첫번째 문서 확인\n",
    "docs[0]\n"
   ],
   "id": "6bf76c29a7c73914",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1'}, page_content='2023 년 12월호')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`load_and_split()`\n",
    "\n",
    "splitter 를 사용하여 문서를 분할하고 반환합니다.\n",
    "\n",
    "반환된 결과는 `List[Document]` 형태입니다."
   ],
   "id": "b3ba45875b9fbdc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:13:06.488609Z",
     "start_time": "2025-12-13T05:13:04.275683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 문열 분할기 설정\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "# 문서 분할\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# 로드된 문서의 수 확인\n",
    "len(docs)\n",
    "# 첫번째 문서 확인\n",
    "docs[0]"
   ],
   "id": "80b873abd457cc61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1'}, page_content='2023 년 12월호')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`lazy_load()`\n",
    "\n",
    "generator 방식으로 문서를 로드합니다."
   ],
   "id": "8d65a7c42274beed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:13:21.529066Z",
     "start_time": "2025-12-13T05:13:19.978627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generator 방식으로 문서 로드\n",
    "for doc in loader.lazy_load():\n",
    "    print(doc.metadata)"
   ],
   "id": "407622d4b4dbd622",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22'}\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23'}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`aload()`\n",
    "\n",
    "비동기(Async) 방식의 문서 로드"
   ],
   "id": "e6002c6afc4242da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:15:29.423798Z",
     "start_time": "2025-12-13T05:15:29.419769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 문서를 async 방식으로 로드\n",
    "adocs = loader.aload()"
   ],
   "id": "ac2d5af26cca4f2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T05:15:34.520239Z",
     "start_time": "2025-12-13T05:15:33.219955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 문서 로드\n",
    "await adocs"
   ],
   "id": "5b6a7bf476de6608",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1'}, page_content='2023 년 12월호'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2'}, page_content='2023 년 12월호\\nⅠ. 인공지능 산업 동향 브리프\\n 1. 정책/법제 \\n   ▹ 미국, 안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 발표  ························· 1\\n   ▹ G7, 히로시마 AI 프로세스를 통해 AI 기업 대상 국제 행동강령에 합의··························· 2\\n   ▹ 영국 AI 안전성 정상회의에 참가한 28개국, AI 위험에 공동 대응 선언··························· 3\\n   ▹ 미국 법원, 예술가들이 생성 AI 기업에 제기한 저작권 소송 기각····································· 4\\n   ▹ 미국 연방거래위원회 , 저작권청에 소비자 보호와 경쟁 측면의 AI 의견서 제출················· 5\\n   ▹ EU AI 법 3자 협상, 기반모델 규제 관련 견해차로 난항··················································· 6\\n \\n 2. 기업/산업 \\n   ▹ 미국 프런티어 모델 포럼, 1,000 만 달러 규모의 AI 안전 기금 조성································ 7\\n   ▹ 코히어 , 데이터 투명성 확보를 위한 데이터 출처 탐색기 공개  ······································· 8\\n   ▹ 알리바바 클라우드 , 최신 LLM ‘통이치엔원 2.0’ 공개 ······················································ 9\\n   ▹ 삼성전자 , 자체 개발 생성 AI ‘삼성 가우스 ’ 공개 ··························································· 10\\n   ▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································ 11\\n   ▹ IDC, 2027 년 AI 소프트웨어 매출 2,500 억 달러 돌파 전망··········································· 12\\n   ▹ 빌 게이츠 , AI 에이전트로 인한 컴퓨터 사용의 패러다임 변화 전망································ 13\\n   ▹ 유튜브 , 2024 년부터 AI 생성 콘텐츠 표시 의무화 ···························································· 14\\n 3. 기술/연구\\n   ▹ 영국 과학혁신기술부 , AI 안전 연구소 설립 발표······························································ 15\\n   ▹ 구글 딥마인드 , 범용 AI 모델의 기능과 동작에 대한 분류 체계 발표······························ 16\\n   ▹ 갈릴레오의 LLM 환각 지수 평가에서 GPT-4 가 가장 우수 ··········································· 17\\n   \\n 4. 인력/교육     \\n   ▹ 영국 옥스퍼드 인터넷 연구소 , AI 기술자의 임금이 평균 21% 높아······························· 18\\n   \\n   \\n \\nⅡ. 주요 행사\\n   ▹CES 2024 ····························································································································· 19\\n   ▹AIMLA 2024 ························································································································· 19\\n   ▹AAAI Conference on Artificial Intelligence ·································································· 19'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3'}, page_content='Ⅰ. 인공지능 산업 동향 브리프'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n미국, 안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 발표 \\nn미국 바이든 대통령이 ‘안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 ’에 서명하고 \\n광범위한 행정 조치를 명시\\nn행정명령은 △AI의 안전과 보안 기준 마련 △개인정보보호 △형평성과 시민권 향상 △소비자 \\n보호 △노동자 지원 △혁신과 경쟁 촉진 △국제협력을 골자로 함KEY Contents\\n£바이든 대통령 , AI 행정명령 통해 안전하고 신뢰할 수 있는 AI 개발과 활용 추진\\nn미국 바이든 대통령이 2023년 10월 30일 연방정부 차원에서 안전하고 신뢰할 수 있는 AI 개발과 \\n사용을 보장하기 위한 행정명령을 발표\\n∙행정명령은 △AI의 안전과 보안 기준 마련 △개인정보보호 △형평성과 시민권 향상 △소비자 보호 \\n△노동자 지원 △혁신과 경쟁 촉진 △국제협력에 관한 내용을 포괄\\nn(AI 안전과 보안 기준) 강력한 AI 시스템을 개발하는 기업에게 안전 테스트 결과와 시스템에 관한 \\n주요 정보를 미국 정부와 공유할 것을 요구하고 , AI 시스템의 안전성과 신뢰성 확인을 위한 표준 및 \\nAI 생성 콘텐츠 표시를 위한 표준과 모범사례 확립을 추진\\n∙△1026 플롭스 (FLOPS, Floating Point Operation Per Second) 를 초과하는 컴퓨팅 성능 또는 생물학적 \\n서열 데이터를 주로 사용하고 1023플롭스를 초과하는 컴퓨팅 성능을 사용하는 모델 △단일 데이터센터에서 \\n1,000Gbit/s 이상의 네트워킹으로 연결되며 AI 훈련에서 이론상 최대 1020 플롭스를 처리할 수 있는 \\n컴퓨팅 용량을 갖춘 컴퓨팅 클러스터가 정보공유 요구대상\\nn(형평성과 시민권 향상) 법률, 주택, 보건 분야에서 AI의 무책임한 사용으로 인한 차별과 편견 및 기타 \\n문제를 방지하는 조치를 확대\\n∙형사사법 시스템에서 AI 사용 모범사례를 개발하고 , 주택 임대 시 AI 알고리즘 차별을 막기 위한 명확한 \\n지침을 제공하며 , 보건복지 부문에서 책임 있는 AI 배포와 사용을 위한 전략을 마련 \\nn(소비자 보호와 근로자 지원) 의료 분야에서 책임 있는 AI 사용을 촉진하고 맞춤형 개인교습 등 학교 \\n내 AI 교육 도구 관련 자원을 개발하며 , AI로 인한 근로자 피해를 완화하고 이점을 극대화하는 원칙과 \\n모범사례를 마련\\nn(혁신과 경쟁 촉진) 국가AI연구자원 (National Artificial Intelligence Research Resource, NAIRR)* 을 \\n통해 미국 전역의 AI 연구를 촉진하고 , 중소기업과 개발자에 기술과 인프라를 지원\\n* 국가 차원에서 AI 연구 인프라를 확충해 더 많은 AI 연구자에게 인프라를 지원하는 프로그램 \\n∙비자 기준과 인터뷰 절차의 현대화와 간소화로 AI 관련 주요 분야의 전문 지식을 갖춘 외국인들이 미국에서 \\n공부하고 취업할 수 있도록 지원\\n☞ 출처 : The White House, Executive Order on the Safe, Secure, and Trustworthy Development and Use of \\nArtificial Intelligence (E.O. 14110), 2023.10.30.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5'}, page_content='SPRi AI Brief |  \\n2023-12 월호\\n2G7, 히로시마 AI 프로세스를 통해 AI 기업 대상 국제 행동강령에 합의\\nnG7이 첨단 AI 시스템을 개발하는 기업을 대상으로 AI 위험 식별과 완화를 위해 자발적인 \\n채택을 권고하는 AI 국제 행동강령을 마련\\nn행동강령은 AI 수명주기 전반에 걸친 위험 평가와 완화, 투명성과 책임성의 보장, 정보공유와 \\n이해관계자 간 협력, 보안 통제, 콘텐츠 인증과 출처 확인 등의 조치를 요구KEY Contents\\n£G7, 첨단 AI 시스템의 위험 관리를 위한 국제 행동강령 마련\\nn주요 7개국(G7)* 은 2023 년 10월 30일 ‘히로시마 AI 프로세스 ’를 통해 AI 기업 대상의 AI 국제 \\n행동강령 (International Code of Conduct for Advanced AI Systems) 에 합의\\n∙G7은 2023 년 5월 일본 히로시마에서 개최된 정상회의에서 생성 AI에 관한 국제규범 마련과 \\n정보공유를 위해 ‘히로시마 AI 프로세스 ’를 출범**\\n∙기업의 자발적 채택을 위해 마련된 이번 행동강령은 기반모델과 생성 AI를 포함한 첨단 AI 시스템의 \\n위험 식별과 완화에 필요한 조치를 포함\\n* 주요 7개국(G7)은 미국, 일본, 독일, 영국, 프랑스 , 이탈리아 , 캐나다를 의미\\n** 5월 정상회의에는 한국, 호주, 베트남 등을 포함한 8개국이 초청을 받았으나 , AI 국제 행동강령에는 우선 G7 국가만 포함하여 채택\\nnG7은 행동강령을 통해 아래의 조치를 제시했으며 , 빠르게 발전하는 기술에 대응할 수 있도록 \\n이해관계자 협의를 통해 필요에 따라 개정할 예정\\n∙첨단 AI 시스템의 개발 과정에서 AI 수명주기 전반에 걸쳐 위험을 평가 및 완화하는 조치를 채택하고 , \\n첨단 AI 시스템의 출시와 배포 이후 취약점과 오용 사고, 오용 유형을 파악해 완화\\n∙첨단 AI 시스템의 성능과 한계를 공개하고 적절하거나 부적절한 사용영역을 알리는 방법으로 투명성을 \\n보장하고 책임성을 강화\\n∙산업계 , 정부, 시민사회 , 학계를 포함해 첨단 AI 시스템을 개발하는 조직 간 정보공유와 사고 발생 시 \\n신고를 위해 협력하고 , 위험 기반 접근방식을 토대로 개인정보보호 정책과 위험 완화 조치를 포함하는 \\nAI 거버넌스와 위험 관리 정책을 마련\\n∙AI 수명주기 전반에 걸쳐 물리보안 , 사이버보안 , 내부자 위협 보안을 포함한 강력한 보안 통제 구현\\n∙사용자가 AI 생성 콘텐츠를 식별할 수 있도록 워터마크를 비롯하여 기술적으로 가능한 기법으로 \\n신뢰할 수 있는 콘텐츠 인증과 출처 확인 메커니즘을 개발 및 구축 \\n∙사회적 위험과 안전·보안 문제를 완화하는 연구와 효과적인 완화 대책에 우선 투자하고 , 기후 위기 \\n대응, 세계 보건과 교육 등 세계적 난제 해결을 위한 첨단 AI 시스템을 우선 개발\\n∙국제 기술 표준의 개발 및 채택을 가속화하고 , 개인정보와 지식재산권 보호를 위해 데이터 입력과 수집 \\n시 적절한 보호 장치 구현\\n☞ 출처: G7, Hiroshima Process International Code of Conduct for Advanced AI Systems, 2023.10.30.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n영국 AI 안전성 정상회의에 참가한 28개국, AI 위험에 공동 대응 선언\\nn영국 블레츨리 파크에서 개최된 AI 안전성 정상회의에 참가한 28개국들이 AI 안전 보장을 \\n위한 협력 방안을 담은 블레츨리 선언을 발표\\nn첨단 AI를 개발하는 국가와 기업들은 AI 시스템에 대한 안전 테스트 계획에 합의했으며 , \\n영국의 AI 안전 연구소가 전 세계 국가와 협력해 테스트를 주도할 예정 KEY Contents\\n£AI 안전성 정상회의 참가국들 , 블레츨리 선언 통해 AI 안전 보장을 위한 협력에 합의\\nn2023 년 11월 1~2일 영국 블레츨리 파크에서 열린 AI 안전성 정상회의 (AI Safety Summit) 에 \\n참가한 28개국 대표들이  AI 위험 관리를 위한 ‘블레츨리 선언’을 발표 \\n∙선언은 AI 안전 보장을 위해 국가, 국제기구 , 기업, 시민사회 , 학계를 포함한 모든 이해관계자의 협력이 \\n중요하다고 강조했으며 , 특히 최첨단 AI 시스템 개발 기업은 안전 평가를 비롯한 적절한 조치를 취하여 \\nAI 시스템의 안전을 보장할 책임이 있다고 지적\\n∙각국은 AI 안전 보장을 위해 첨단 AI 개발기업의 투명성 향상, 적절한 평가지표와 안전 테스트 도구 \\n개발, 공공부문 역량 구축과 과학 연구개발 등의 분야에서 협력하기로 합의\\n£영국 총리, 정부 주도의 첨단 AI 시스템 안전 테스트 계획 발표\\nn리시 수낙 영국 총리는 AI 안전성 정상회의를 마무리하며 첨단 AI 모델에 대한 안전성 시험 계획 \\n수립과  테스트 수행을 주도할 영국 AI 안전 연구소의 출범을 발표\\n∙첨단 AI 모델의 안전 테스트는 국가 안보와 안전, 사회적 피해를 포함한 여러 잠재적 유해 기능에 대한 \\n시험을 포함하며 , 참석자들은 정부 주도의 외부 안전 테스트에 합의\\n∙각국 정부는 테스트와 기타 안전 연구를 위한 공공부문 역량에 투자하고 , 테스트 결과가 다른 국가와 \\n관련된 경우 해당 국가와 결과를 공유하며 , 적절한 시기에 공동 표준 개발을 위해 노력하기로 합의 \\nn참가국들은 튜링상을 수상한 AI 학자인 요슈아 벤지오 교수가 주도하는 ‘과학의 현황(State of \\nthe Science)’ 보고서 작성에도 합의했으며 , 보고서를 통해 첨단 AI의 위험과 가능성에 관한 \\n기존 연구를 과학적으로 평가하고 향후 AI 안전 연구를 위한 우선순위를 제시할 계획 \\nn한국은 영국 정부와 6개월 뒤에 온라인으로 AI 미니 정상회의를 공동 개최하기로 합의했으며 , \\n프랑스 정부와는 1년 후 대면 정상회의를 개최할 예정\\n☞ 출처: Gov.uk, The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023, 2023.11.01.\\nGov.uk, World leaders, top AI companies set out plan for safety testing of frontier as first global AI Safety Summit \\nconcludes, 2023.11.02.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7'}, page_content='SPRi AI Brief |  \\n2023-12 월호\\n4미국 법원, 예술가들이 생성 AI 기업에 제기한 저작권 소송 기각\\nn미국 캘리포니아 북부지방법원은 미드저니 , 스태빌리티 AI, 디비언트아트를 대상으로 예술가 \\n3인이 제기한 저작권 침해 소송을 기각\\nn법원은 기각 이유로 고소장에 제시된 상당수 작품이 저작권청에 등록되지 않았으며 , AI로 \\n생성된 이미지와 특정 작품 간 유사성을 입증하기 어렵다는 점을 제시 KEY Contents\\n£예술가들의 AI 저작권 침해 소송, 저작권 미등록과 증거불충분으로 기각\\nn미국 캘리포니아 북부지방법원의 윌리엄 오릭(W illiam Orrick) 판사는 2023년 10월 30일 미드저니\\n(M idjourney),  스태빌리티 AI(Stability AI), 디비언트아트 (DeviantArt) 에 제기된 저작권 침해 소송을 기각 \\n∙2023 년 1월 예술가 사라 앤더슨 (Sarah Anderson), 캘리 맥커넌 (Kelly McKernan), 칼라 \\n오르티즈 (Karla Ortiz) 는 이미지 생성 AI 서비스를 개발한 3개 기업을 상대로 저작권 침해 소송을 제기\\n∙예술가들은 3개 기업이 AI 모델을 학습시키기 위해 원작자 동의 없이 작품을 학습 데이터셋에 \\n포함하여 저작권을 침해했다고 주장했으며 , 법원은 지난 4월 피소 기업들이 제출한 기각 신청을 \\n수용해 소송을 기각 \\nn오릭 판사는 판결문에서 소송을 기각한 핵심 이유로 예술가들의 저작권 미등록을 제시\\n∙판결문은 소송을 제기한 캘리 맥커넌과 칼라 오르티즈가 미국 저작권청에 예술 작품에 대한 저작권을 \\n제출하지 않았다는 점을 지적했으며 , 사라 앤더슨은 고소장에 인용된 수백 개의 작품 중 16개 작품에 \\n대해서만 저작권을 보유\\nn판결문은 또한 생성 AI 모델 훈련에 사용된 모든 이미지에 저작권이 있다거나 , 생성 AI로 만든 \\n이미지가 저작물을 이용해 훈련되었으므로 저작물의 파생 이미지라는 주장은 개연성이  부족하다고  \\n지적\\n∙AI는 새로운 이미지를 생성할 때 다양한 예술가의 작품을 참조하므로 , 생성된 이미지와 저작권을 가진 \\n특정 작품과의 실질적 유사성을 입증할 수 없다면 저작권 침해를 인정받기 어려움\\nn오릭 판사는 원고 측에 고소장을 수정하고 저작권이 침해된 특정 이미지를 중심으로 소송 범위를 \\n줄여 소송을 다시 제기할 것을 요청\\n∙단, 사라 앤더슨이 저작권을 보유한 16개 작품을 무단으로 복제한 스태빌리티 AI에 대한 저작권 침해 \\n소송은 인정되어 계속 진행됨\\n☞ 출처: Venturebeat, Midjourney, Stability AI and DeviantArt win a victory in copyright case by artists- but the \\nfight continues, 2023.10.30.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n미국 연방거래위원회 , 저작권청에 소비자 보호와 경쟁 측면의 AI 의견서 제출\\nn미국 FTC는 저작권청이 실시한 저작권과 AI 관련 질의공고에 대하여 소비자 보호와 경쟁 \\n측면의 의견을 제시\\nnFTC는 생성 AI로 인한 창작자와 소비자 피해의 가능성에 우려를 표시하는 한편, 일부 \\n빅테크가 막대한 재원을 활용해 시장 지배력을 더욱 강화할 수 있다는 우려를 제기KEY Contents\\n£FTC, 생성 AI로 인한 소비자와 창작자의 피해 및 빅테크의 시장 지배력 강화 우려\\nn미국 연방거래위원회 (FTC) 가 2023 년 10월 30일 저작권청 (U.S. Copyright Office, USCO) 이 \\n지난 9월 발표한 저작권과 AI 관련 질의공고 (Notice of Inquiry, NOI)에 대한 의견서를 발표\\n∙저작권청은 생성 AI와 관련된 저작권법과 정책 이슈를 조사하고 있으며 , 폭넓은 의견 수렴을 통해 \\n입법과 규제 조치의 필요성을 검토할 계획\\n∙FTC는 생성 AI의 개발과 배포가 소비자 , 근로자 , 중소기업에 피해를 줄 수 있다며 소비자의 개인정보 \\n침해, 차별과 편견의 자동화 , 사기 범죄 등 AI 사용과 관련된 위험에 주목\\nnFTC는 저작권법에 따른 권리와 책임 범위를 넘어서는 저작권 문제에 주목하여 생성 AI로 인해 \\n창작자의 경쟁력이 불공정한 피해를 볼 수 있으며 , 소비자가 특정 창작자의 작품을 생성 AI가 \\n만들었다고 오해할 소지가 있다고 지적\\n∙저작권법에 저촉되는 행위는 불공정 경쟁이나 기만행위에도 해당될 수 있으며 , 창작자의 평판 악화, \\n저작물의 가치 저하나 개인정보 유출로 소비자에 상당한 피해를 초래 가능\\nnFTC는 일부 빅테크가 막대한 재원을 활용해 생성 AI 사용자의 이탈을 막고 저작권이 있는 상용 \\n데이터에 대한 독점 라이선스를 확보해 시장 지배력을 더욱 강화할 수 있다는 우려도 제기\\n∙이와 관련 FTC는 아마존 AI 비서 ‘알렉사 (Alexa)’ 와 스마트홈 보안 기기 ‘링(Ring)’ 이 소비자의 사적 \\n정보를 알고리즘 훈련에 사용하여 프라이버시를 침해한 혐의를 조사하는 등 법적 권한을 활용해 AI \\n관련 불법 행위에 대처하고 있음\\n* FTC는 2023년 5월 31일 동의를 받지 않고 어린이들의 음성과 위치 정보를 활용한 ‘알렉사 ’와 고객의 사적 영상에 대하여 \\n직원에게 무제한 접근 권한을 부여한 ‘링’에 3,080 만 달러(약 420억 원)의 과징금을 부과  \\nnFTC는 빠르게 발전하는 생성 AI가 여러 산업과 비즈니스에 변화를 가져올 수 있지만 , 현행법상 \\nAI에 관한 예외 조항은 없다며 , 모든 권한을 활용해 소비자를 보호하고 개방적이고 공정한 경쟁 \\n시장을 유지하겠다고 강조\\n☞ 출처: FTC, In Comment Submitted to U.S. Copyright Office, FTC Raises AI-related Competition and \\nConsumer Protection Issues, Stressing That It Will Use Its Authority to Protect Competition and \\nConsumers in AI Markets, 2023.10.30.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9'}, page_content='SPRi AI Brief |  \\n2023-12 월호\\n6EU AI 법 3자 협상, 기반모델 규제 관련 견해차로 난항\\nn유럽의회 , EU 집행위원회 , EU 이사회가 진행 중인 AI 법 최종협상에서 프랑스 , 이탈리아 , \\n독일이 기반모델에 대한 규제에 반대하며 협상이 난관에 봉착\\nn프랑스 , 이탈리아 , 독일 3개국은 기반모델 개발기업에 대하여 자율적 행동강령을 도입하고 \\n준수를 의무화하는 방안을 제안KEY Contents\\n£AI 법 3자 협상, 이사회 일부 국가가 기반모델 규제에 반대하며 차질\\nn유럽의회 , EU 집행위원회 , EU 이사회가 ‘AI 법(AI act)’에 대한 최종협상을 진행 중인 가운데 , \\n일부 국가가 기반모델에 대한 규제에 반대하며 협상이 난관에 봉착  \\n∙10월 24일 열린 3자 협상 회의에서는 사회에 더 큰 영향을 미치는 강력한 AI 모델에 더 엄격한 규칙을 \\n적용하는 계층적 접근방식에 따라 기반 모델 규제에 대한 기본적인 합의에 도달\\n∙그러나 11월 10일 열린 통신작업반 회의에서 EU 이사회의 프랑스 , 독일, 이탈리아 대표가 \\n기반모델에 대한 모든 유형의 규제에 반대하며 협상이 중단됨\\nn유럽 정책 미디어 유랙티브 (Euractive) 에 따르면 프랑스 AI 기업 미스트랄 (Mistral) 이 로비를 통해 \\n기반모델에 대한 규제 반대를 주도  \\n∙독일의 대표적인 AI 기업 알레프 알파(Aleph Alpha) 역시 독일 정부에 압력을 행사하고 있으며 , 이들 \\n기업은 EU의 AI 규제로 인해 미국과 중국의 경쟁사보다 뒤처질 것을 우려 \\n£독일, 프랑스 , 이탈리아 3개국, 기반모델에 대한 ‘의무적 자율규제 ’ 제안\\nn통신작업반 회의가 결렬된 이후 독일, 프랑스 , 이탈리아는 2023 년 11월 19일 비공식 문서를 통해 \\n‘의무적 자율규제 (Mandatory Self-regulation)’ 방식의 기반모델 규제를 제안\\n∙3개국은 기반모델 전반에 대한 규제가 기술 중립적이고 위험 기반의 AI 규제 원칙에 어긋난다고 \\n주장하며 기반모델 전반에 대한 규제가 아닌, 특정 용도로 사용될 수 있는 AI 시스템에 대한 규제를 요구  \\n∙3개국은 자발적인 행동강령을 도입하고 준수를 의무화하는 방안을 제안하며 , 기반모델 개발기업에 \\n머신러닝 기술 정보와 모델의 기능과 한계를 요약한 ‘모델 카드’ 작성을 요구하겠다고 설명\\n∙3개국은 AI 감독기관이 모델 카드를 토대로 기반모델 개발기업의 행동강령 준수 여부를 확인하되 , \\n위반 시 곧바로 제재를 가하지 않고 위반행위 분석과 영향 평가를 시행한 후 제재하는 방안을 제안\\n☞ 출처: Euractiv, EU’s AI Act negotiations hit the brakes over foundation models, 2023.11.1.\\n Euractiv, France, Germany, Italy push for ‘mandatory self-regulation’ for foundation models in EU’s AI law, 2023.11.19.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n미국 프런티어 모델 포럼, 1,000 만 달러 규모의 AI 안전 기금 조성\\nn구글, 앤스로픽 , 마이크로소프트 , 오픈AI가 참여하는 프런티어 모델 포럼이 자선단체와 함께 AI \\n안전 연구를 위한 1,000 만 달러 규모의 AI 안전 기금을 조성\\nn프런티어 모델 포럼은 AI 모델의 취약점을 발견하고 검증하는 레드팀 활동을 지원하기 위한 \\n모델 평가 기법 개발에 자금을 중점 지원할 계획KEY Contents\\n£프런티어 모델 포럼, 자선단체와 함께 AI 안전 연구를 위한 기금 조성\\nn구글, 앤스로픽 , 마이크로소프트 , 오픈AI가 출범한 프런티어 모델 포럼이 2023년 10월 25일 AI 안전 \\n연구를 위한 기금을 조성한다고 발표\\n∙참여사들은 맥거번 재단(Patrick J. McGovern Foundation), 데이비드 앤 루실 패커드 재단(The \\nDavid and Lucile Packard Foundation ) 등의 자선단체와 함께 AI 안전 연구를 위한 기금에 \\n1,000 만 달러 이상을 기부 \\n∙또한 신기술의 거버넌스와 안전 분야에서 전문성을 갖춘 브루킹스 연구소 출신의 크리스 메서롤 (Chris \\nMeserole) 을 포럼의 상무이사로 임명\\nn최근 AI 기술이 급속히 발전하면서 AI 안전에 관한 연구가 부족한 시점에 , 포럼은 이러한 격차를 해소\\n하기 위해 AI 안전 기금을 조성\\n∙참여사들은 지난 7월 백악관 주재의 AI 안전 서약에서 외부자의 AI 시스템 취약점 발견과 신고를 \\n촉진하기로 약속했으며 , 약속을 이행하기 위해 기금을 활용해 외부 연구집단의 AI 시스템 평가에 \\n자금을 지원할 계획\\n£AI 안전 기금으로 AI 레드팀을 위한 모델 평가 기법 개발을 중점 지원할 계획\\nn프런티어 모델 포럼은 AI 안전 기금을 통해 AI 레드팀 활동을 위한 새로운 모델 평가 기법의  개발을 \\n중점 지원할 예정\\n∙포럼에 따르면 AI 레드팀에 대한 자금 지원은 AI 모델의 안전과 보안 기준의 개선과 함께 AI 시스템 \\n위험 대응 방안에 관한 산업계와 정부, 시민사회의 통찰력 확보에 도움이 될 전망으로 , 포럼은 향후 몇 \\n달 안에 기금 지원을 위한 제안 요청을 받을 계획\\nn프런티어 모델 포럼은 출범 이후 업계 전반에 걸쳐 AI 레드팀 구성에 관한 모범사례 공유를 추진하는  \\n한편, 첨단 AI 모델의 취약점이나 잠재적으로 위험한 기능 및 위험 완화 관련 정보를 공유할 수 \\n있는 공개 절차도 개발 중\\n☞ 출처: Google, Anthropic, Google, Microsoft and OpenAI announce Executive Director of the Frontier Model \\nForum and over $10 million for a new AI Safety Fund, 2023.10.25.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11'}, page_content='SPRi AI Brief |  \\n2023-12 월호\\n8코히어 , 데이터 투명성 확보를 위한 데이터 출처 탐색기 공개\\nn코히어와 12개 기관이  광범위한 데이터셋에 대한 감사를 통해 원본 데이터 출처, 재라이선스 상태, \\n작성자 등 다양한 정보를 제공하는 ‘데이터 출처 탐색기 ’ 플랫폼을 출시\\nn대화형 플랫폼을 통해 개발자는 데이터셋의 라이선스 상태를 쉽게 파악할 수 있으며 데이터셋의 \\n구성과 계보도 추적 가능KEY Contents\\n£데이터 출처 탐색기 , 광범위한 데이터셋 정보 제공을 통해 데이터 투명성 향상\\nnAI 기업 코히어 (Cohere) 가 매사추세츠 공과⼤(MIT), 하버드 ⼤ 로스쿨 , 카네기멜론 ⼤ 등 12개 기관과  \\n함께 2023 년 10월 25일 ‘데이터 출처 탐색기 (Data Provenance Explorer)’ 플랫폼을 공개\\n∙AI 모델 훈련에 사용되는 데이터셋의 불분명한 출처로 인해 데이터 투명성이 확보되지 않아 다양한 \\n법적·윤리적 문제가 발생\\n∙이에 연구진은 가장 널리 사용되는 2,000 여 개의 미세조정 데이터셋을 감사 및 추적하여 데이터셋에 \\n원본 데이터소스에 대한 태그, 재라이선스 (Relicensing) 상태, 작성자 , 기타 데이터 속성을 지정하고 \\n이러한 정보에 접근할 수 있는 플랫폼을 출시\\n∙대화형 플랫폼 형태의 데이터 출처 탐색기를 통해 데이터셋의 라이선스 상태를 쉽게 파악할 수 있으며 , \\n주요 데이터셋의 구성과 데이터 계보도 추적 가능\\nn연구진은 오픈소스 데이터셋에 대한 광범위한 감사를 통해 데이터 투명성에 영향을 미치는 주요 \\n요인을  발견\\n∙깃허브 (GitHub), 페이퍼위드코드 (Papers with Code) 와 같은 크라우드소싱 플랫폼에서 수집한 \\n데이터로 훈련된 오픈소스 LLM에서는 데이터 라이선스의 누락 비율이 72~83% 에 달함 \\n∙또한 크라우드소싱 플랫폼이 할당한 라이선스는 데이터셋 원저작자의 의도보다 더 광범위한 사용을 \\n허용한 경우가 상당수\\n∙데이터 생태계 분석 결과, 부정확하거나 모호한 라이선스 문서화 등 데이터 출처 입증과 관련된 관행 \\n전반에서 구조적 문제가 드러남\\nn연구진은 데이터 출처 탐색기만으로는 해결이 어려운 법적 이슈도 존재한다며 일관된 법적 프레임\\n워크의 필요성을 제기\\n∙일례로 데이터를 수집한 지역, 모델 훈련 지역, 모델 배포 지역마다 규제가 다르면 어떤 법률을 \\n적용해야 하는지 실무자의 판단이 어려울 수 있으며 , 서로 다른 라이선스를 적용받는 개별 데이터셋을 \\n하나로 통합해 사용하는 경우에도 각각의 라이선스 조건 준수에 어려움이 발생\\n☞ 출처 : Cohere, Data Provenance Explorer Launches to Tackle Data Transparency Crisis, 2023.10.25.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n알리바바 클라우드 , 최신 LLM ‘통이치엔원 2.0’ 공개\\nn알리바바 클라우드가 복잡한 지침 이해, 광고문구 작성, 추론, 암기 등에서 성능이 향상된 최신 \\nLLM ‘통이치엔원 2.0’을 공개\\nn알리바바 클라우드는 산업별로 특화된 생성 AI 모델을 공개하는 한편, 모델 개발과 애플리케이션 \\n구축 절차를 간소화하는 올인원 AI 모델 구축 플랫폼도 출시KEY Contents\\n£알리바바의 통이치엔원 2.0, 주요 벤치마크 테스트에서 여타 LLM 능가\\nn중국의 알리바바 클라우드가 2023 년 10월 31일 열린 연례 기술 컨퍼런스에서 최신 LLM ‘통이\\n치엔원 (Tongyi Qianwen) 2.0’을 공개\\n∙알리바바 클라우드는 통이치엔원 2.0이 2023 년 4월 출시된 1.0 버전보다 복잡한 지침 이해, \\n광고문구 작성, 추론, 암기 등에서 성능이 향상되었다고 설명\\n∙통이치엔원 2.0은 언어 이해 테스트 (MMLU), 수학(GSM8k), 질문 답변(ARC-C) 과 같은 벤치마크 \\n테스트에서 라마(Llama-2-70B) 와 GPT-3.5 를 비롯한 주요 AI 모델을 능가 \\n∙통이치엔원 2.0은 알리바바 클라우드의 웹사이트와 모바일 앱을 통해 대중에 제공되며 개발자는 \\nAPI를 통해 사용 가능 \\nn알리바바 클라우드는 여러 산업 영역에서 생성 AI를 활용해 사업 성과를 개선할 수 있도록 지원\\n하는 산업별 모델도 출시\\n∙산업 영역은 고객지원 , 법률 상담, 의료, 금융, 문서관리 , 오디오와 동영상 관리, 코드 개발, 캐릭터 \\n제작을 포함\\nn알리바바 클라우드는 급증하는 생성 AI 수요에 대응해 모델 개발과 애플리케이션 구축 절차를 \\n간소화하는 올인원 AI 모델 구축 플랫폼 ‘젠AI(GenAI)’ 도 공개\\n∙이 플랫폼은 데이터 관리, 모델 배포와 평가, 신속한 엔지니어링을 위한 종합 도구 모음을 제공하여 \\n다양한 기업들이 맞춤형 AI 모델을 한층 쉽게 개발할 수 있도록 지원\\n∙생성 AI 개발에 필요한 컴퓨팅과 데이터 처리 요구사항을 지원하기 위해 AI 플랫폼 (PAI), \\n데이터베이스 솔루션 , 컨테이너 서비스와 같은 클라우드 신제품도 발표\\nn알리바바 클라우드는 AI 개발을 촉진하기 위해 올해 말까지 720억 개 매개변수를 가진 통이치엔원  \\n모델을 오픈소스화한다는 계획도 공개\\n☞ 출처 : Alibaba Cloud, Alibaba Cloud Launches Tongyi Qianwen 2.0 and Industry-specific Models to Support \\nCustomers Reap Benefits of Generative AI, 2023.10.31.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13'}, page_content='SPRi AI Brief |  \\n2023-12 월호\\n10삼성전자 , 자체 개발 생성 AI ‘삼성 가우스 ’ 공개\\nn삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성 \\nAI 모델 ‘삼성 가우스 ’를 공개\\nn삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로 , 온디바이스 작동이 가능한 \\n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유KEY Contents\\n£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스 , 온디바이스 작동 지원\\nn삼성전자가 2023 년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델 \\n‘삼성 가우스 ’를 최초 공개\\n∙정규분포 이론을 정립한 천재 수학자 가우스 (Gauss) 의 이름을 본뜬 삼성 가우스는 다양한 상황에 \\n최적화된 크기의 모델 선택이 가능\\n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며 , \\n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유\\n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며 , 생성 AI 모델을 다양한 제품에 \\n단계적으로 탑재할 계획\\nn삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는  \\n이미지 모델의 3개 모델로 구성\\n∙언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며 , 메일 작성, 문서 요약, 번역 업무의 \\n처리를 지원\\n∙코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이 (code.i)’ 는 대화형 인터페이스로 서비스를 제공하며 \\n사내 소프트웨어 개발에 최적화\\n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며 \\n저해상도 이미지의 고해상도 전환도 지원\\nnIT 전문지 테크리퍼블릭 (TechRepublic) 은 온디바이스 AI가 주요 기술 트렌드로 부상했다며 , \\n2024 년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2 를 탑재한 퀄컴 기기 및 구글 \\n어시스턴트를 적용한 구글 픽셀(Pixel) 과 경쟁할 것으로 예상\\n☞ 출처 : 삼성전자 , ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스 ’ 공개, 2023.11.08.\\n삼성전자 , ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.\\nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 \\nn구글이 앤스로픽에 최대 20억 달러 투자에 합의하고 5억 달러를 우선 투자했으며 , 앤스로픽은 \\n구글과 클라우드 서비스 사용 계약도 체결\\nn3대 클라우드 사업자인 구글, 마이크로소프트 , 아마존은 차세대 AI 모델의 대표 기업인 \\n앤스로픽 및 오픈AI와 협력을 확대하는 추세KEY Contents\\n£구글, 앤스로픽에 최대 20억 달러 투자 합의 및 클라우드 서비스 제공\\nn구글이 2023 년 10월 27일 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며 , 이 중 5억 \\n달러를 우선 투자하고 향후 15억 달러를 추가로 투자할 방침\\n∙구글은 2023 년 2월 앤스로픽에 이미 5억 5,000 만 달러를 투자한 바 있으며 , 아마존도 지난 9월 \\n앤스로픽에 최대 40억 달러의 투자 계획을 공개\\n∙한편, 2023 년 11월 8일 블룸버그 보도에 따르면 앤스로픽은 구글의 클라우드 서비스 사용을 위해 \\n4년간 30억 달러 규모의 계약을 체결\\n∙오픈AI 창업자 그룹의 일원이었던 다리오 (Dario Amodei) 와 다니엘라 아모데이 (Daniela Amodei) \\n남매가 2021 년 설립한 앤스로픽은 챗GPT의 대항마 ‘클로드 (Claude)’ LLM을 개발\\nn아마존과 구글의 앤스로픽 투자에 앞서, 마이크로소프트는 차세대 AI 모델의 대표 주자인  오픈\\nAI와 협력을 확대\\n∙마이크로소프트는 오픈AI에 앞서 투자한 30억 달러에 더해 2023 년 1월 추가로 100억 달러를 \\n투자하기로 하면서 오픈AI의 지분 49%를 확보했으며 , 오픈AI는 마이크로소프트의 애저(Azure) \\n클라우드 플랫폼을 사용해 AI 모델을 훈련\\n£구글, 클라우드 경쟁력 강화를 위해 생성 AI 투자 확대\\nn구글은 수익률이 높은 클라우드 컴퓨팅 시장에서 아마존과 마이크로소프트를 따라잡고자 생성 AI를 \\n통한 기업 고객의 클라우드 지출 확대를 위해 AI 투자를 지속  \\n∙구글은 앤스로픽 외에도 AI 동영상 제작 도구를 개발하는 런웨이 (Runway) 와 오픈소스 소프트웨어 \\n기업 허깅 페이스 (Hugging Face) 에도 투자\\n∙구글은 챗GPT의 기반 기술과 직접 경쟁할 수 있는 차세대 LLM ‘제미니 (Gemini)’ 를 포함한 자체 AI \\n시스템 개발에도 수십억 달러를 투자했으며 , 2024 년 제미니를 출시할 계획\\n☞ 출처 : The Wall Street Journal, Google Commits $2 Billion in Funding to AI Startup Anthropic, 2023.10.27.\\nBloomberg, AI Startup Anthropic to Use Google Chips in Expanded Partnership, 2023.11.09.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15'}, page_content='SPRi AI Brief |  \\n2023-12 월호\\n12IDC, 2027년 AI 소프트웨어 매출 2,500 억 달러 돌파 전망\\nnIDC의 예측에 의하면 AI 소프트웨어 시장은 2027년 2,510 억 달러로 달할 전망이며 , 생성 \\nAI 플랫폼과 애플리케이션은 2027 년까지 283억 달러의 매출을 창출할 전망 \\nn2023년 기준 AI 소프트웨어 매출의 3분의 1을 차지하는 최대 시장인 AI 애플리케이션은 \\n2027년까지 21.1% 의 연평균 성장률을 기록할 전망KEY Contents\\n£기업들의 AI 투자 증가에 힘입어 AI 소프트웨어 시장 급성장 예상\\nn시장조사기관 IDC는 AI 소프트웨어 시장이 2022년 640억 달러에서 2027년 2,510 억 달러로 연평\\n균 성장률 31.4% 를 기록하며 급성장할 것으로 예상\\n∙AI 소프트웨어 시장은 AI 플랫폼 , AI 애플리케이션 , AI 시스템 인프라 소프트웨어 (SIS), AI 애플리케이션 \\n개발·배포(AI AD&D) 소프트웨어를 포괄\\n∙협업, 콘텐츠 관리, 전사적 자원관리 (ERM), 공급망 관리, 생산 및 운영, 엔지니어링 , 고객관계관리 (CRM) 를 \\n포함하는 AI 애플리케이션은 AI 소프트웨어의 최대 시장으로 2023년 전체 매출의 약 3분의 1을 차지하며 \\n2027년까지 21.1% 의 연평균 성장률을 기록할 전망\\n∙AI 비서를 포함한 AI 모델과 애플리케이션의 개발을 뒷받침하는 AI 플랫폼은 두 번째로 시장 규모가 큰 \\n분야로 , 2027년까지 35.8% 의 연평균 성장률이 예상됨\\n∙분석, 비즈니스 인텔리전스 , 데이터 관리와 통합을 포함하는 AI SIS는 기존 소프트웨어 시스템과 통합되어 \\n방대한 데이터를 활용한 의사결정과 운영 최적화를 지원하며 , 현재 매출 규모는 비교적 작지만 5년간 \\n연평균 성장률은 32.6% 로 시장 전체를 웃돌 전망\\n∙애플리케이션 개발, 소프트웨어 품질과 수명주기 관리 소프트웨어 , 애플리케이션 플랫폼을 포함하는 AI \\nAD&D 는 향후 5년간 카테고리 중 가장 높은 38.7% 의 연평균 성장률이 예상됨\\nnIDC에 따르면 경제적 불확실성과 시장 역학의 변화에도 AI와 자동화 기술에 대한 기업들의 투자 \\n의지는 확고하며 , 기업들은 AI 도입이 사업 성공과 경쟁우위에 필수적이라고 인식\\n∙IDC 설문조사에 따르면 향후 12개월 동안 응답자의 3분의 1은 기업이 특정 사용 사례나 응용 영역에서 \\n외부 AI 소프트웨어의 구매를 고려하거나 외부 AI 소프트웨어와 내부 자원의 결합을 고려\\nn한편, AI 소프트웨어 시장에 포함되지 않는 생성 AI 플랫폼과 애플리케이션은 2027 년까지 283억 \\n달러의 매출을 창출할 전망 \\n☞ 출처 : IDC, IDC Forecasts Revenue for Artificial Intelligence Software Will Reach $279 Billion Worldwide in \\n2027, 2023.10.31.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n빌 게이츠 , AI 에이전트로 인한 컴퓨터 사용의 패러다임 변화 전망\\nn빌 게이츠가 5년 내 일상언어로 모든 작업을 처리할 수 있는 AI 에이전트가 보급되며 컴퓨터를 \\n사용하는 방식이 완전히 바뀔 것으로 예상\\nn에이전트의 보급은 컴퓨터 분야를 넘어 산업 전 영역에 영향을 미칠 전망으로 특히 의료와 \\n교육, 생산성 , 엔터테인먼트 ·쇼핑 영역에서 고가로 제공되던 서비스가 대중화될 전망KEY Contents\\n£5년 내 기기에 일상언어로 말하기만 하면 되는 AI 에이전트의 보급 예상\\nn빌 게이츠 마이크로소프트 창업자가 2023 년 11월 9일 공식 블로그를 통해 AI 에이전트가 컴퓨터  \\n사용방식과 소프트웨어 산업을 완전히 변화시킬 것이라는 전망을 제시\\n∙자연어에 반응하고 사용자에 대한 지식을 바탕으로 다양한 작업을 수행하는 소프트웨어를 의미하는 \\n에이전트는 컴퓨터 사용방식이 키보드 입력에서 아이콘 클릭으로 바뀐 이후 최대의 컴퓨팅 혁명을 \\n가져올 전망\\n∙현재는 컴퓨터 작업 시 작업 내용에 따라 각각 다른 앱을 사용해야 하지만 5년 내 에이전트의 발전으로 \\n기기에 일상언어로 말하기만 하면 되는 미래가 도래할 것\\n∙온라인에 접속하는 모든 사람이 AI 기반의 개인 비서를 사용할 수 있게 되며, 에이전트는 사용자에 대한 \\n풍부한 지식을 바탕으로 맞춤화된 대응이 가능하며 시간이 지날수록 개선됨\\n∙일례로 여행 계획 수립 시 AI 챗봇이 예산에 맞는 호텔을 제안하는데 머문다면 , 에이전트는 사용자의 여행 \\n패턴을 분석해 여행지를 제안하고 관심사에 따른 활동을 추천하며 선호하는 스타일의 레스토랑 예약도 가능  \\n£AI 에이전트가 의료와 교육, 생산성 , 엔터테인먼트 ·쇼핑 영역의 서비스 대중화를 주도할 것\\nn에이전트로 인해 주목할 만한 변화는 고비용 서비스의 대중화로 특히 △의료 △교육 △생산성 △\\n엔터테인먼트 ·쇼핑의 4개 영역에서 대규모 변화 예상\\n∙(의료) 에이전트가 환자 분류를 지원하고 건강 문제에 대한 조언을 제공하며 치료의 필요 여부를 결정하면서 \\n의료진의 의사결정과 생산성 향상에 기여\\n∙(교육) 에이전트가 1대 1 가정교사의 역할을 맡아 모든 학생에게 평등한 교육 기회를 제공할 수 있으며 , \\n아이가 좋아하는 게임이나 노래 등을 활용해 시청각 기반의 풍부한 맞춤형 교육 경험을 제공\\n∙(생산성 ) 사용자의 아이디어를 기반으로 에이전트가 사업계획과 발표 자료 작성, 제품 이미지 생성을 \\n지원하며 , 임원의 개인 비서와 같은 역할도 수행 \\n∙(엔터테인먼트 ·쇼핑) 쇼핑 시 에이전트가 모든 리뷰를 읽고 요약해 최적의 제품을 추천하고 사용자 대신 \\n주문할 수 있으며 사용자의 관심사에 맞춤화된 뉴스와 엔터테인먼트를 구독 가능\\n☞ 출처 : GatesNotes, AI is about to completely change how you use computers, 2023.11.09.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17'}, page_content='SPRi AI Brief |  \\n2023-12 월호\\n14유튜브 , 2024년부터 AI 생성 콘텐츠 표시 의무화 \\nn유튜브가 몇 달 안에 생성 AI를 사용한 콘텐츠에 AI 라벨 표시를 의무화하기로 했으며 , 이를 \\n준수하지 않는 콘텐츠는 삭제하고 크리에이터에 대한 수익 배분도 중단할 수 있다고 설명\\nn유튜브는 AI 생성 콘텐츠가 신원 파악이 가능한 개인을 모방한 경우 개인정보 침해 신고 \\n절차에 따라 콘텐츠 삭제 요청도 받을 계획KEY Contents\\n£유튜브 , 생성 AI 콘텐츠에 AI 라벨 표시 안 하면 콘텐츠 삭제\\nn유튜브가 2023 년 11월 14일 공식 블로그를 통해 몇 달 안에 생성 AI를 사용한 콘텐츠에 AI \\n라벨을 표시하는 새로운 규칙을 시행한다고 발표 \\n∙실제로 일어나지 않은 사건을 사실적으로 묘사하거나 실제로 하지 않은 말이나 행동을 보여주는 콘텐츠와 \\n같이 AI 도구를 사용해 사실적으로 변경되거나 합성된 콘텐츠에는 AI 라벨을 표시 필요\\n∙유튜브는 이러한 규칙이 선거나 분쟁 상황, 공중 보건, 공직자 관련 문제와 같이 민감한 주제를 다루는 \\n콘텐츠에서 특히 중요하다고 강조했으며 , 크리에이터가 AI로 제작한 콘텐츠에 AI 라벨을 표시하지 않으면 \\n해당 콘텐츠는 삭제되고 광고 수익을 배분하는 유튜브 파트너 프로그램도 정지될 수 있음\\n∙유튜브는 두 가지 방식으로 AI를 이용한 콘텐츠의 변경이나 합성 여부를 시청자에게 전달할 계획으로 \\n동영상 설명 패널에 라벨을 표시하는 방식이 기본이며 , 민감한 주제를 다루는 특정 유형의 콘텐츠는 동영상 \\n플레이어에 더욱 눈에 띄는 라벨을 적용 \\n∙유튜브는 커뮤니티 정책에 위반되는 일부 합성 콘텐츠에 대해서는 라벨 지정 여부와 관계없이 삭제할 \\n방침으로 , 가령 사실적인 폭력을 보여주는 합성 동영상이 시청자에게 충격이나 혐오감을 줄 수 있다면 \\n삭제될 수 있음\\n£유튜브 , 특정인을 모방한 AI 생성 콘텐츠에 대한 삭제 요청에도 대응 계획\\nn유튜브는 몇 달 내에 신원 파악이 가능한 개인의 얼굴이나 음성을 모방한 AI 생성 콘텐츠에 대하\\n여 개인정보 침해 신고 절차를 마련해 삭제 요청을 받을 계획  \\n∙단, 모든 콘텐츠가 삭제 대상은 아니며 유튜브는 콘텐츠가 패러디나 풍자인지 , 해당 영상에서 삭제 요청을 \\n한 특정인을 식별할 수 있는지 , 공직자나 유명인이 등장하는지 등 다양한 요소를 고려할 예정\\n∙유튜브는 음반사가 아티스트의 고유한 노래나 목소리를 모방한 AI 생성 음악에 대하여 삭제를 요청할 수 \\n있는 기능도 도입할 방침\\n☞ 출처 : Youtube, Our approach to responsible AI innovation, 2023.11.14.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n영국 과학혁신기술부 , AI 안전 연구소 설립 발표\\nn영국 과학혁신기술부가 첨단 AI 시스템에 대한 평가를 통해 안전성을 보장하기 위한 AI \\n안전 연구소를 설립한다고 발표\\nnAI 안전 연구소는 핵심 기능으로 첨단 AI 시스템 평가 개발과 시행, AI 안전 연구 촉진, \\n정보교류 활성화를 추진할 계획KEY Contents\\n£영국 AI 안전 연구소 , 첨단 AI 시스템 평가와 AI 안전 연구, 정보 교류 추진\\nn영국 과학혁신기술부가 2023 년 11월 2일 첨단 AI 안전에 중점을 둔 국가 연구기관으로 AI \\n안전 연구소 (AI Safety Institute) 를 설립한다고 발표\\n∙AI 안전 연구소는 첨단 AI의 위험을 이해하고 거버넌스 마련에 필요한 사회·기술적 인프라 개발을 통해 \\n영국을 AI 안전 연구의 글로벌 허브로 확립하는 것을 목표로 함\\n∙영국 정부는 향후 10년간 연구소에 공공자금을 투자해 연구를 지원할 계획으로 , 연구소는 △첨단 AI 시스템 \\n평가 개발과 시행 △AI 안전 연구 촉진 △정보 교류 활성화를 핵심 기능으로 함\\nn(첨단 AI 시스템 평가 개발과 시행) 시스템의 안전 관련 속성을 중심으로 안전과 보안 기능을 이해\\n하고 사회적 영향을 평가\\n∙평가 우선순위는 △사이버범죄 조장, 허위 정보 유포 등 악의적으로 활용될 수 있는 기능 △사회에 미치는 \\n영향 △시스템 안전과 보안 △인간의 통제력 상실 가능성 순\\n∙연구소는 외부 기관과 협력해 자체 시스템 평가를 개발 및 수행하고 , 평가와 관련된 의견 공유 및 지침 \\n마련을 위해 전문가 커뮤니티를 소집할 계획\\nn(AI 안전 연구 촉진) 외부 연구자를 소집하고 다양한 예비 연구 프로젝트를 통해 AI 안전 기초연구를 수행\\n∙AI 시스템의 효과적 거버넌스를 위한 도구 개발* 및 안전한 AI 시스템 개발을 위한 새로운 접근 방식 연구를 수행\\n* 편향된 훈련 데이터에 대한 분석기술 , 민감한 정보를 포함하는 AI 시스템에 대한 미세 조정 방법\\nn(정보 교류 활성화 ) 현행 개인정보보호와 데이터 규제 하에서 연구소와 정책입안자 , 국제 파트너 , \\n학계, 시민사회 및 일반 대중과 정보 공유 채널을 구축\\n∙AI 안전성 정상회의 (AI Safety Summit) 에서 합의된 대로 첨단 AI 모델의 평가 후 해당 모델이 배포된 \\n타국의 정부 및 연구소와 평가 결과를 공유하고 , 학계와 대중이 AI 시스템의 피해와 취약점을 보고할 수 \\n있는 명확한 절차를 수립\\n☞ 출처 : Gov.uk, Introducing the AI Safety Institute, 2023.11.02.\\n             Venturebeat, Researchers turn to Harry Potter to make AI forget about copyrighted material, 2023.10.06.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19'}, page_content='16구글 딥마인드 , 범용 AI 모델의 기능과 동작에 대한 분류 체계 발표\\nn구글 딥마인드 연구진이 성능과 범용성 , 자율성을 기준으로 범용 AI(AGI) 의 수준을 \\n0~5단계까지 총 6단계로 구분한 프레임워크를 공개\\nn현재 AGI는 단백질 구조를 예측하는 알파폴드와 같은 특정 용도에서는 5단계 수준을 달성했지만 \\n광범위하게 활용될 수 있는 범용에서는 1단계 수준에 머물러 있음KEY Contents\\n£챗GPT와 구글 바드와 같은 AI 챗봇은 범용 AI 1단계 수준\\nn구글 딥마인드 연구진은 2023 년 11월 4일 범용 AI(Artificial General Intelligence, AGI) 모델을 용도와 \\n성능에 따라 분류하는 프레임워크를 제시한 논문을 발표\\n∙프레임워크의 목적은 AGI의 성능, 범용성 , 자율성 수준을 정의하여 모델 간 비교와 위험 평가, AGI \\n달성까지의 진행 상황을 측정할 수 있는 공통 기준을 제공하기 위함\\nn연구진은 AGI 개념 정의에 필요한 기준을 수립하기 위한 6가지 원칙을 아래와 같이 도출\\n∙(프로세스가 아닌 기능에 중점) AI가 어떻게 작동하는지보다 무엇을 할 수 있는지가 더 중요\\n∙(범용성과 성능을 모두 평가) 진정한 AGI는 인간을 능가하는 폭넓은 범용성과 기술의 깊이를 모두 요구\\n∙(인지와 메타인지 작업에 중점) 물리적 작업의 수행 능력은 AGI의 필수 전제조건이 아니며 , 인지 작업과 \\n메타인지 작업(예; 새로운 작업의 학습 능력, 인간에게 도움을 요청할 시점을 아는 능력)이 핵심\\n∙(실제 구현보다 잠재력에 집중) 통제된 상황에서 발휘되는 성능에 따라 AGI를 규정하고 테스트를 진행 \\n∙(생태학적 타당도를 갖춘 벤치마크 사용) AGI에 대한 벤치마크는 사람들이 경제적 · 사회적 또는 예술적으로 \\n가치 있게 여기는 실질적인 작업을 대상으로 성능 평가 필요\\n∙(종점이 아닌 AGI를 향한 경로에 중점) 단계별 접근방식을 통해 AGI의 발전 상태를 점진적으로 측정\\nn연구진은 상기 원칙에 따라 AI를 성능에 따라 0~5단계와 광범위한 목적에 활용될 수 있는 범용 AI 및 특정 \\n과업에 활용되는 특수 AI로 분류했으며 , 특수 AI에서는 5단계까지 달성되었으나 , 범용 AI는 현재 1단계 수준\\n성능 특수 AI 예시 범용 AI 예시\\n0단계: AI 아님 계산기 소프트웨어 , 컴파일러 아마존 메커니컬 터크\\n1단계: 신진(숙련되지 않은 인간) GOFAI(Good Old Fashioned Artificial Intelligence) 챗GPT, 바드, 라마2\\n2단계: 유능(숙련된 인간의 50% 이상)스마트 스피커 (애플 시리, 아마존 알렉사 , 구글 \\n어시스턴트 ), IBM 왓슨 미달성\\n3단계: 전문가 (숙련된 인간의 90% 이상)문법 교정기 (그래머리 ), 생성 이미지 모델(달리2) 미달성\\n4단계: 거장(숙련된 인간의 99% 이상) 딥블루 , 알파고 미달성\\n5단계: 초인간 (인간을 100% 능가) 알파폴드 , 알파제로 , 스톡피시 미달성<구글 딥마인드의 범용 AI 분류 프레임워크 > \\n☞ 출처 : Arxiv.org, Levels of AGI: Operationalizing Progress on the Path to AGI, 2023.11.04.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20'}, page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n갈릴레오의 LLM 환각 지수 평가에서 GPT-4 가 가장 우수\\nn주요 LLM의 환각 현상을 평가한 ‘LLM 환각 지수’에 따르면 GPT-4 는 작업 유형과 관계없이 \\n가장 우수한 성능을 보였으며 GPT-3.5 도 거의 동등한 성능을 발휘\\nn오픈소스 모델 중에서는 메타의 라마2가 RAG 없는 질문과 답변 및 긴 형식의 텍스트 \\n생성에서 가장 우수한 성능을 발휘KEY Contents\\n£주요 LLM 중 GPT-4 가 가장 환각 현상 적고 GPT-3.5 터보도 비슷한 성능 기록\\nn머신러닝 데이터 관리 기업 갈릴레오 (Galileo) 가 2023년 11월 15일 주요 LLM의 환각 현상을 평가한 \\n‘LLM 환각 지수(LLM Hallucination Index)’ 를 발표\\n∙생성 AI의 환각 현상은 AI 시스템이 잘못된 정보를 생성하거나 , 현실과 다른 부정확한 결과를 내놓는 \\n현상으로 , 기업의 AI 도입을 가로막는 주요 장애물이며 , 환각 지수는 신뢰할 수 있는 생성 AI 구축을 위해 \\n환각을 평가하고 측정하는 구조화된 접근방식을 제공\\n∙환각 지수는 △검색 증강 생성(Retrieval-Augmented Generation, RAG)* 을 포함한 질문과 답변 △RAG \\n없는 질문과 답변 △긴 형식의 텍스트 (보고서나 기사, 에세이 ) 생성의 3개 작업 유형에 대하여 환각을 \\n기준으로 LLM의 순위를 평가\\n* 기존에 학습된 데이터가 아닌 외부 소스(데이터셋 , 데이터베이스 , 문서 등)에서 가져온 정보를 검색해 활용하는 기술\\nn3개의 작업 유형 평가 전체에서 오픈AI의 GPT-4 가 최고의 성능을 기록했으며 , GPT-3.5 터보도 \\nGPT-4 와 거의 동등한 성능을 발휘\\n∙메타의 라마2(Llama-2-70b) 는 RAG 없는 질문과 답변 유형에서 오픈소스 모델 가운데 가장 우수했고 긴 \\n형식의 텍스트 생성에서도 GPT-4 에 준하는 성능을 기록했으나 , RAG 포함 질문과 답변에서는 허깅 \\n페이스의  제퍼(Zephyr-7b) 가 라마2를 능가\\n<갈릴레오의 LLM 환각 지수(RAG 포함 질문과 답변 기준)>\\n☞ 출처: Galileo, LLM Hallucination Index, 2023.11.15.'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21'}, page_content='18영국 옥스퍼드 인터넷 연구소 , AI 기술자의 임금이 평균 21% 높아\\nn옥스퍼드 인터넷 연구소의 연구에 따르면 특정 기술의 경제적 가치는 다른 기술과 결합 \\n가능성이 높을수록 높게 평가됨 \\nnAI의 확산은 기술의 경제적 가치에 크게 영향을 미치며 , AI 기술을 가진 근로자는 평균 21%, \\n최대 40% 높은 임금을 받을 수 있음  KEY Contents\\n£AI 기술 중 머신러닝 , 텐서플로우 , 딥러닝의 임금 프리미엄이 높게 평가\\nn옥스퍼드 인터넷 연구소 (Oxford Internet Institute) 가 2023년 10월 24일 962개 기술과 2만 5천 \\n명을 대상으로 한 연구에서 AI를 포함한 주요 기술의 경제적 가치를 분석한 결과를 발표 \\n∙연구에 따르면 한 기술의 경제적 가치는 근로자의 여타 역량과 얼마나 잘 결합하는지를 보여주는 \\n‘상보성 (complementarity)’ 에 따라 결정됨\\n∙특정 기술은 다른 기술과 결합 가능성이 높을수록 경제적 가치가 높아지며 , 일례로 데이터 분석과 같은 \\n기술은 여타 고부가가치 기술과 결합할 수 있어 가치가 높지만 , 사진 리터칭 같은 기술은 특정 기술과만 \\n결합할 수 있어 가치가 낮게 평가됨 \\n∙대부분 직업은 여러 기술의 조합이 필요하며 , 근로자의 재교육에서 경제적 효율성을 높이려면 기존 기술과 \\n신기술 간 상보성을 극대화할 필요\\nnAI의 확산은 기술의 경제적 가치에 크게 영향을 미치는 요소로 , AI 기술을 가진 근로자는 평균적으로  \\n21% 높은 임금을 획득 가능\\n∙AI 기술 중 근로자에 대한 경제적 가치(시간당 임금 증가율 기준) 측면에서 상위 5개 기술은 \\n머신러닝 (+40%), 텐서플로우 (+38%), 딥러닝 (+27%), 자연어처리 (+19%), 데이터 과학(+17%) 순\\n☞  출 처  : Oxford Internet Institute, AI com es out on top: Oxford Study identifies the economic value of specific skills, 2023.10.24.\\n<AI 기술 유형 평균 기술 대비 갖는 임금 프리미엄 >'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22'}, page_content='행사명 행사 주요 개요\\nCES 2024\\n-미국 소비자기술 협회(CTA) 가 주관하는 세계 최대 가전·IT·소\\n비재 전시회로 5G, AR&VR, 디지털헬스 , 교통·모빌리티 등 \\n주요 카테고리 중심으로 기업들이 최신의 기술 제품군을 전시\\n-CTA 사피로 회장은 가장 주목받는 섹터로 AI를 조명하였으며 , \\n모든 산업을 포괄한다는 의미에서 ‘올 온(All on)’을 주제로 한 \\n이번 전시에는 500곳 이상의 한국기업 참가 예정\\n기간 장소 홈페이지\\n2024.1.9~12 미국, 라스베가스 https://www.ces.tech/\\nAIMLA 2024\\n-머신러닝 및 응용에 관한 국제 컨퍼런스 (AIMLA 2024) 는 \\n인공지능 및 머신러닝의 이론, 방법론 및 실용적 접근에 관한 \\n지식과  최신 연구 결과 공유\\n-이론 및 실무 측면에서 인공지능 , 기계학습의 주요 분야를 \\n논의하고 , 학계, 산업계의 연구자와 실무자들에게 해당 분\\n야의 최첨단 개발 소식 공유\\n기간 장소 홈페이지\\n2024.1.27~28 덴마크 , 코펜하겐https://ccnet2024.org/aimla\\n/index\\nAAAI Conference \\non Artificial \\nIntelligence\\n-AI 발전 협회 컨퍼런스 (AAAI) 는 AI 연구를 촉진하고 , AI 분야 \\n연구원 , 실무자 , 과학자 , 학생 및 공학자 간 교류의 기회 제공\\n-컨퍼런스에서 AI 관련 기술 발표, 특별 트랙, 초청 연사, \\n워크숍 , 튜토리얼 , 포스터 세션, 주제 발표, 대회, 전시 프\\n로그램 등 진행   \\n기간 장소 홈페이지\\n2024.2.20~27 캐나다 , 밴쿠버https://aaai.org/aaai-confere\\nnce/\\nⅡ. 주요 행사 일정'),\n",
       " Document(metadata={'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': './week5Data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23'}, page_content='홈페이지 : https://spri.kr/\\n보고서와 관련된 문의는 AI정책연구실 (jayoo@spri.kr, 031-739-7352) 으로 연락주시기 바랍니다 .')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef2de729851004bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 02. PDF",
   "id": "888a2e91f968b3d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`PDF`\n",
    "\n",
    "Portable Document Format (PDF), ISO 32000으로 표준화된 파일 형식은 Adobe가 1992년에 문서를 제시하기 위해 개발했으며, 이는 응용 소프트웨어, 하드웨어 및 운영 시스템에 독립적인 방식으로 텍스트 서식 및 이미지를 포함합니다.\n",
    "\n",
    "이 가이드는 PDF 문서를 LangChain Document 형식으로 로드하는 방법을 다룹니다. 이 형식은 다운스트림에서 사용됩니다.\n",
    "\n",
    "LangChain은 다양한 PDF 파서와 통합됩니다. 일부는 간단하고 상대적으로 저수준이며, 다른 일부는 OCR 및 이미지 처리를 지원하거나 고급 문서 레이아웃 분석을 수행합니다.\n",
    "\n",
    "올바른 선택은 사용자의 애플리케이션에 따라 달라집니다."
   ],
   "id": "8ac871345d00faf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:06:06.776757Z",
     "start_time": "2025-12-13T10:06:06.685879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ],
   "id": "a5674507ebda5c3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "\n",
    "- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- 링크: https://spri.kr/posts/view/23669\n",
    "- 파일명: SPRI_AI_Brief_2023년12월호_F.pdf\n",
    "- 참고: 위의 파일은 data 폴더 내에 다운로드 받으세요"
   ],
   "id": "5dc446e50092e64a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:07:11.135777Z",
     "start_time": "2025-12-13T10:07:11.131160Z"
    }
   },
   "cell_type": "code",
   "source": "FILE_PATH = \"week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\"",
   "id": "71944e432fcbcfcd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:07:12.242128Z",
     "start_time": "2025-12-13T10:07:12.237088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_metadata(docs):\n",
    "    if docs:\n",
    "        print(\"[metadata]\")\n",
    "        print(list(docs[0].metadata.keys()))\n",
    "        print(\"\\n[examples]\")\n",
    "        max_key_length = max(len(k) for k in docs[0].metadata.keys())\n",
    "        for k, v in docs[0].metadata.items():\n",
    "            print(f\"{k:<{max_key_length}} : {v}\")\n"
   ],
   "id": "be408fb61c2021fd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyPDF",
   "id": "8fbe1c7695758859"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "여기에서는 pypdf를 사용하여 PDF를 문서 배열로 로드하며, 각 문서는 page 번호와 함께 페이지 내용 및 메타데이터를 포함합니다.",
   "id": "968970056f48b1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 설치\n",
    "# !pip install -qU pypdf"
   ],
   "id": "54b4084a816b17ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:09:01.000028Z",
     "start_time": "2025-12-13T10:08:54.764693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 파일 경로 설정\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "\n",
    "# PDF 로더 초기화\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[10].page_content[:300])"
   ],
   "id": "5ed8743b2928f57a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPRi AI Brief |  \n",
      "2023-12 월호\n",
      "8코히어 , 데이터 투명성 확보를 위한 데이터 출처 탐색기 공개\n",
      "n코히어와 12개 기관이  광범위한 데이터셋에 대한 감사를 통해 원본 데이터 출처, 재라이선스 상태, \n",
      "작성자 등 다양한 정보를 제공하는 ‘데이터 출처 탐색기 ’ 플랫폼을 출시\n",
      "n대화형 플랫폼을 통해 개발자는 데이터셋의 라이선스 상태를 쉽게 파악할 수 있으며 데이터셋의 \n",
      "구성과 계보도 추적 가능KEY Contents\n",
      "£데이터 출처 탐색기 , 광범위한 데이터셋 정보 제공을 통해 데이터 투명성 향상\n",
      "nAI 기업 코히어 (\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:09:09.683605Z",
     "start_time": "2025-12-13T10:09:09.679648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 메타데이터 출력\n",
    "show_metadata(docs)"
   ],
   "id": "a2f09aa9defc0e0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['producer', 'creator', 'creationdate', 'author', 'moddate', 'pdfversion', 'source', 'total_pages', 'page', 'page_label']\n",
      "\n",
      "[examples]\n",
      "producer     : Hancom PDF 1.3.0.542\n",
      "creator      : Hwp 2018 10.0.0.13462\n",
      "creationdate : 2023-12-08T13:28:38+09:00\n",
      "author       : dj\n",
      "moddate      : 2023-12-08T13:28:38+09:00\n",
      "pdfversion   : 1.4\n",
      "source       : week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "total_pages  : 23\n",
      "page         : 0\n",
      "page_label   : 1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PyPDF(OCR)\n",
    "일부 PDF에는 스캔된 문서나 그림 내에 텍스트 이미지가 포함되어 있습니다.\n",
    "\n",
    "`rapidocr-onnxruntime` 패키지를 사용하여 이미지에서 텍스트를 추출할 수도 있습니다."
   ],
   "id": "7fae9d07bd5c8ba7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 설치\n",
    "# !pip install -qU rapidocr-onnxruntime"
   ],
   "id": "f145f44c7d478bdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "이미지 추출 옵션이 있어서인지 이전의 `PyPDFLoader`보다 속도가 현저하게 낮다",
   "id": "1baaec16b3e98273"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:11:52.473461Z",
     "start_time": "2025-12-13T10:10:20.041492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PDF 로더 초기화, 이미지 추출 옵션 활성화\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\", extract_images=True)\n",
    "\n",
    "# PDF 페이지 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 페이지 내용 접근\n",
    "print(docs[4].page_content[:300])"
   ],
   "id": "ef34dc7c2a7734bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 5\n",
      "Table 1: Current layout detection models in the LayoutParser model zoo\n",
      "Dataset Base Model1Large Model Notes\n",
      "PubLayNet [38] F / M M Layouts of modern scientiﬁc documents\n",
      "PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\n",
      "Newspape\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:11:52.540031Z",
     "start_time": "2025-12-13T10:11:52.534036Z"
    }
   },
   "cell_type": "code",
   "source": "show_metadata(docs)",
   "id": "428e2bc0be2e0a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['producer', 'creator', 'creationdate', 'author', 'keywords', 'moddate', 'ptex.fullbanner', 'subject', 'title', 'trapped', 'source', 'total_pages', 'page', 'page_label']\n",
      "\n",
      "[examples]\n",
      "producer        : pdfTeX-1.40.21\n",
      "creator         : LaTeX with hyperref\n",
      "creationdate    : 2021-06-22T01:27:10+00:00\n",
      "author          : \n",
      "keywords        : \n",
      "moddate         : 2021-06-22T01:27:10+00:00\n",
      "ptex.fullbanner : This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2\n",
      "subject         : \n",
      "title           : \n",
      "trapped         : /False\n",
      "source          : https://arxiv.org/pdf/2103.15348.pdf\n",
      "total_pages     : 16\n",
      "page            : 0\n",
      "page_label      : 1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `PyMuPDF`\n",
    "\n",
    "PyMuPDF 는 속도 최적화가 되어 있으며, PDF 및 해당 페이지에 대한 자세한 메타데이터를 포함하고 있습니다.\n",
    "\n",
    "페이지 당 하나의 문서를 반환합니다:"
   ],
   "id": "b7320173fba55a39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:11:52.604934Z",
     "start_time": "2025-12-13T10:11:52.600111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 설치\n",
    "# !pip install -qU pymupdf"
   ],
   "id": "11b85abd1ca39cbd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# PyMuPDF 로더 인스턴스 생성\n",
    "loader = PyMuPDFLoader(FILE_PATH)\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[10].page_content[:300])"
   ],
   "id": "2f1ce17532c1930b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_metadata(docs)",
   "id": "e8f39ced629eec64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Unstructured\n",
    "`Unstructured`는 Markdown이나 PDF와 같은 비구조화된 또는 반구조화된 파일 형식을 다루기 위한 공통 인터페이스를 지원합니다.\n",
    "\n",
    "LangChain의 `UnstructuredPDFLoader`는 Unstructured와 통합되어 PDF 문서를 LangChain Document 객체로 파싱합니다."
   ],
   "id": "9bf1f22f4ccde67a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:12:39.293562Z",
     "start_time": "2025-12-13T10:12:39.290429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 설치\n",
    "# !pip install -qU unstructured"
   ],
   "id": "60a0b8e5f6fbc748",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:13:48.094213Z",
     "start_time": "2025-12-13T10:12:42.946830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# UnstructuredPDFLoader 인스턴스 생성\n",
    "loader = UnstructuredPDFLoader(FILE_PATH)\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[0].page_content[:300])"
   ],
   "id": "f7ad52ffb97b544b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023년 12월호\n",
      "\n",
      "2023년 12월호\n",
      "\n",
      "Ⅰ. 인공지능 산업 동향 브리프\n",
      "\n",
      "1. 정책/법제\n",
      "\n",
      "▹ 미국, 안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 발표 ·························1\n",
      "\n",
      "▹ G7, 히로시마 AI 프로세스를 통해 AI 기업 대상 국제 행동강령에 합의 ···························2\n",
      "\n",
      "▹ 영국 AI 안전성 정상회의에 참가한 28개국, AI 위험에 공동 대응 선언 ··························· 3\n",
      "\n",
      "▹ 미국 법원, 예술가들이 생성 AI 기업에 제기한\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:13:48.135005Z",
     "start_time": "2025-12-13T10:13:48.130491Z"
    }
   },
   "cell_type": "code",
   "source": "show_metadata(docs)",
   "id": "b9034df6f4f87ec3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source']\n",
      "\n",
      "[examples]\n",
      "source : week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "내부적으로 비정형에서는 텍스트 청크마다 서로 다른 \"요소\"를 만듭니다.\n",
    "\n",
    "기본적으로 이들은 결합되어 있지만 mode=\"elements\"를 지정하여 쉽게 분리할 수 있습니다."
   ],
   "id": "3751b7b2e5909fe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:13:49.962636Z",
     "start_time": "2025-12-13T10:13:48.162427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UnstructuredPDFLoader 인스턴스 생성(mode=\"elements\")\n",
    "loader = UnstructuredPDFLoader(FILE_PATH, mode=\"elements\")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[0].page_content)"
   ],
   "id": "9e113ea488e4655e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023년 12월호\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "처음에는 전부 출력되었는데, 이제는 인덱싱을 통해 부분 출력이 가능해진다",
   "id": "aee95bdc130138e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "이 특정 문서에 대한 전체 요소 유형 집합을 참조하세요",
   "id": "6bf6b05e8284abe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:13:49.995420Z",
     "start_time": "2025-12-13T10:13:49.989397Z"
    }
   },
   "cell_type": "code",
   "source": "set(doc.metadata[\"category\"] for doc in docs)  # 데이터 카테고리 추출",
   "id": "d5245e47a04c745",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Footer', 'Header', 'ListItem', 'NarrativeText', 'Title', 'UncategorizedText'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:13:50.013611Z",
     "start_time": "2025-12-13T10:13:50.008974Z"
    }
   },
   "cell_type": "code",
   "source": "show_metadata(docs)",
   "id": "89c757a0dac1df26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source', 'coordinates', 'file_directory', 'filename', 'languages', 'last_modified', 'page_number', 'filetype', 'category', 'element_id']\n",
      "\n",
      "[examples]\n",
      "source         : week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "coordinates    : {'points': ((256.579467, 282.444348), (256.579467, 303.42387300000007), (355.4236898438, 303.42387300000007), (355.4236898438, 282.444348)), 'system': 'PixelSpace', 'layout_width': 612, 'layout_height': 858}\n",
      "file_directory : week5Data\n",
      "filename       : SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "languages      : ['eng']\n",
      "last_modified  : 2025-12-13T14:08:21\n",
      "page_number    : 1\n",
      "filetype       : application/pdf\n",
      "category       : UncategorizedText\n",
      "element_id     : d6be7c84c90e17d08bcfd9f6daa807ca\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyPDFium2",
   "id": "1665912030e1a134"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "무슨 경고가 뜬다\n",
    "```\n",
    "UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
    "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n",
    "```\n",
    "\n",
    "구글링해봤지만 별소득이 없어 ChatGPT에게 물어봤다"
   ],
   "id": "8b58c552800750a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "에러라기보다 **경고(UserWarning)** 입니다. 의미는 다음과 같습니다.\n",
    "\n",
    "* 지금 코드에서 `get_text_range()`를 **기본 파라미터(default params)** 로 호출했는데,\n",
    "* 해당 라이브러리(또는 함수 구현)가 “앞으로는 그 호출을 그대로 처리하지 않고, 내부적으로 `get_text_bounded()` 쪽으로 **자동으로 바꿔서(redirect) 실행**하겠다”는 안내를 띄운 것입니다.\n",
    "* 즉, **현재는 동작하지만** 라이브러리 업데이트/동작 변경 때문에 **의도와 다른 결과가 나올 수 있으니 호출 방식을 명시적으로 바꾸라**는 신호에 가깝습니다.\n",
    "\n",
    "보통 대응은 3가지 중 하나입니다.\n",
    "\n",
    "1. **문서/릴리즈노트 확인 후 권장 API로 변경**\n",
    "\n",
    "* 실제로 `get_text_bounded()`로 바꾸거나,\n",
    "* `get_text_range()`에 필요한 파라미터를 **명시적으로** 넣어서 호출합니다.\n",
    "\n",
    "2. **경고를 없애고 싶을 때(원인 해결이 우선)**\n",
    "\n",
    "* Python 경고 필터로 숨길 수는 있지만, 근본 해결은 아닙니다.\n",
    "  (`warnings.filterwarnings(...)`)\n",
    "\n",
    "3. **왜 뜨는지 정확히 확인**\n",
    "\n",
    "* 어떤 라이브러리의 `get_text_range()`인지가 핵심입니다(예: PDF 처리, UI 텍스트 추출 등).\n",
    "\n",
    "원하시면, `get_text_range()`가 어느 패키지/모듈에 있는 함수인지(예: `import` 구문이나 전체 스택트레이스 일부) 5~10줄만 붙여주시면, **정확히 어떤 변경 때문에 뜨는지**와 **어떤 파라미터를 명시해야 하는지**까지 바로 정리해드릴 수 있습니다.\n"
   ],
   "id": "f7bb5fbbee2dbef3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:15:38.262697Z",
     "start_time": "2025-12-13T10:15:37.531879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "# PyPDFium2 로더 인스턴스 생성\n",
    "loader = PyPDFium2Loader(FILE_PATH)\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[10].page_content[:300])"
   ],
   "id": "ca9b4fb18548582e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPRi AI Brief | \n",
      "2023-12월호\n",
      "8\n",
      "코히어, 데이터 투명성 확보를 위한 데이터 출처 탐색기 공개\n",
      "n 코히어와 12개 기관이 광범위한 데이터셋에 대한 감사를 통해 원본 데이터 출처, 재라이선스 상태, 작성자 등 다양한 정보를 제공하는 ‘데이터 출처 탐색기’ 플랫폼을 출시\n",
      "n 대화형 플랫폼을 통해 개발자는 데이터셋의 라이선스 상태를 쉽게 파악할 수 있으며 데이터셋의 \n",
      "구성과 계보도 추적 가능\n",
      "KEY Contents\n",
      "£ 데이터 출처 탐색기, 광범위한 데이터셋 정보 제공을 통해 데이터 투명성 향상\n",
      "n AI 기업 코히어(Co\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\pypdfium2\\_helpers\\textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:21:20.855069Z",
     "start_time": "2025-12-13T10:21:20.849386Z"
    }
   },
   "cell_type": "code",
   "source": "show_metadata(docs)",
   "id": "be3ae3615a795ec1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['title', 'author', 'subject', 'keywords', 'creator', 'producer', 'creationdate', 'moddate', 'source', 'total_pages', 'page']\n",
      "\n",
      "[examples]\n",
      "title        : \n",
      "author       : dj\n",
      "subject      : \n",
      "keywords     : \n",
      "creator      : Hwp 2018 10.0.0.13462\n",
      "producer     : Hancom PDF 1.3.0.542\n",
      "creationdate : 2023-12-08T13:28:38+09:00\n",
      "moddate      : 2023-12-08T13:28:38+09:00\n",
      "source       : week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "total_pages  : 23\n",
      "page         : 0\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PDFMiner",
   "id": "9c178fc6e6b85449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:21:36.217007Z",
     "start_time": "2025-12-13T10:21:34.389435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "# PDFMiner 로더 인스턴스 생성\n",
    "loader = PDFMinerLoader(FILE_PATH)\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[0].page_content[:300])"
   ],
   "id": "320e122a4bdd9728",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023년  12월호\n",
      "\f2023년  12월호\n",
      "\n",
      "Ⅰ.  인공지능  산업  동향  브리프\n",
      "\n",
      "  1.  정책/법제 \n",
      "\n",
      "      ▹  미국,  안전하고  신뢰할  수  있는  AI  개발과  사용에  관한  행정명령  발표    ························· 1\n",
      "\n",
      "      ▹  G7,  히로시마  AI  프로세스를  통해  AI  기업  대상  국제  행동강령에  합의 ··························· 2\n",
      "\n",
      "      ▹  영국  AI  안전성  정상회의에  참가한  28개국,  AI  위험에  공동  \n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:21:42.240429Z",
     "start_time": "2025-12-13T10:21:42.235970Z"
    }
   },
   "cell_type": "code",
   "source": "show_metadata(docs)",
   "id": "77562d404abaea1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['author', 'creator', 'producer', 'creationdate', 'moddate', 'pdfversion', 'total_pages', 'source']\n",
      "\n",
      "[examples]\n",
      "author       : dj\n",
      "creator      : Hwp 2018 10.0.0.13462\n",
      "producer     : Hancom PDF 1.3.0.542\n",
      "creationdate : 2023-12-08T13:28:38+09:00\n",
      "moddate      : 2023-12-08T13:28:38+09:00\n",
      "pdfversion   : 1.4\n",
      "total_pages  : 23\n",
      "source       : week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PDFMiner를 사용하여 HTML 텍스트 생성\n",
    "\n",
    "이 방법은 출력된 HTML 콘텐츠를 `BeautifulSoup`을 통해 파싱함으로써 글꼴 크기, 페이지 번호, PDF 헤더/푸터 등에 대한 보다 구조화되고 풍부한 정보를 얻을 수 있게 하여 텍스트를 의미론적으로 섹션으로 분할하는 데 도움이 될 수 있습니다."
   ],
   "id": "16bdb35de32099b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:22:03.398200Z",
     "start_time": "2025-12-13T10:22:00.145199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "\n",
    "# PDFMinerPDFasHTMLLoader 인스턴스 생성\n",
    "loader = PDFMinerPDFasHTMLLoader(FILE_PATH)\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[0].page_content[:300])"
   ],
   "id": "45db2bc082b7061f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html\">\n",
      "</head><body>\n",
      "<span style=\"position:absolute; border: gray 1px solid; left:0px; top:50px; width:612px; height:858px;\"></span>\n",
      "<div style=\"position:absolute; top:50px;\"><a name=\"1\">Page 1</a></div>\n",
      "<div style=\"position:absolute; border\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:22:10.368265Z",
     "start_time": "2025-12-13T10:22:10.363264Z"
    }
   },
   "cell_type": "code",
   "source": "show_metadata(docs)",
   "id": "5428904974fba5bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source']\n",
      "\n",
      "[examples]\n",
      "source : week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:22:18.754286Z",
     "start_time": "2025-12-13T10:22:17.010491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(docs[0].page_content, \"html.parser\")  # HTML 파서 초기화\n",
    "content = soup.find_all(\"div\")  # 모든 div 태그 검색"
   ],
   "id": "c5ee64d82c6f87a8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:22:23.538666Z",
     "start_time": "2025-12-13T10:22:23.526754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "cur_fs = None\n",
    "cur_text = \"\"\n",
    "snippets = []  # 동일한 글꼴 크기의 모든 스니펫 수집\n",
    "for c in content:\n",
    "    sp = c.find(\"span\")\n",
    "    if not sp:\n",
    "        continue\n",
    "    st = sp.get(\"style\")\n",
    "    if not st:\n",
    "        continue\n",
    "    fs = re.findall(\"font-size:(\\d+)px\", st)\n",
    "    if not fs:\n",
    "        continue\n",
    "    fs = int(fs[0])\n",
    "    if not cur_fs:\n",
    "        cur_fs = fs\n",
    "    if fs == cur_fs:\n",
    "        cur_text += c.text\n",
    "    else:\n",
    "        snippets.append((cur_text, cur_fs))\n",
    "        cur_fs = fs\n",
    "        cur_text = c.text\n",
    "snippets.append((cur_text, cur_fs))\n",
    "# 중복 스니펫 제거 전략 추가 가능성 (PDF의 헤더/푸터가 여러 페이지에 걸쳐 나타나므로 중복 발견 시 중복 정보로 간주 가능)"
   ],
   "id": "cb41af695bb7cd42",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:22:27.952018Z",
     "start_time": "2025-12-13T10:22:27.944418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "cur_idx = -1\n",
    "semantic_snippets = []\n",
    "# 제목 가정: 높은 글꼴 크기\n",
    "for s in snippets:\n",
    "    # 새 제목 판별: 현재 스니펫 글꼴 > 이전 제목 글꼴\n",
    "    if (\n",
    "        not semantic_snippets\n",
    "        or s[1] > semantic_snippets[cur_idx].metadata[\"heading_font\"]\n",
    "    ):\n",
    "        metadata = {\"heading\": s[0], \"content_font\": 0, \"heading_font\": s[1]}\n",
    "        metadata.update(docs[0].metadata)\n",
    "        semantic_snippets.append(Document(page_content=\"\", metadata=metadata))\n",
    "        cur_idx += 1\n",
    "        continue\n",
    "\n",
    "    # 동일 섹션 내용 판별: 현재 스니펫 글꼴 <= 이전 내용 글꼴\n",
    "    if (\n",
    "        not semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "        or s[1] <= semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "    ):\n",
    "        semantic_snippets[cur_idx].page_content += s[0]\n",
    "        semantic_snippets[cur_idx].metadata[\"content_font\"] = max(\n",
    "            s[1], semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # 새 섹션 생성 조건: 현재 스니펫 글꼴 > 이전 내용 글꼴, 이전 제목 글꼴 미만\n",
    "    metadata = {\"heading\": s[0], \"content_font\": 0, \"heading_font\": s[1]}\n",
    "    metadata.update(docs[0].metadata)\n",
    "    semantic_snippets.append(Document(page_content=\"\", metadata=metadata))\n",
    "    cur_idx += 1\n",
    "\n",
    "print(semantic_snippets[4])"
   ],
   "id": "9b2e77d72ecade86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='KEY Contents\n",
      "n 미국 바이든 대통령이 ‘안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령’에 서명하고 \n",
      "광범위한  행정  조치를  명시\n",
      "n 행정명령은 △AI의 안전과 보안 기준 마련 △개인정보보호 △형평성과 시민권 향상 △소비자 \n",
      "보호  △노동자  지원  △혁신과  경쟁  촉진  △국제협력을  골자로  함\n",
      "' metadata={'heading': '미국,  안전하고  신뢰할  수  있는  AI  개발과  사용에  관한  행정명령  발표 \\n', 'content_font': 12, 'heading_font': 15, 'source': 'week5Data/SPRI_AI_Brief_2023년12월호_F.pdf'}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PyPDF 디렉토리\n",
    "\n",
    "디렉토리에서 PDF를 로드하세요"
   ],
   "id": "15607f6d1352d781"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:22:59.596996Z",
     "start_time": "2025-12-13T10:22:58.256948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# 디렉토리 경로\n",
    "loader = PyPDFDirectoryLoader(\"week5Data/\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 개수 출력\n",
    "print(len(docs))"
   ],
   "id": "b783b6488e55834c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:23:17.912211Z",
     "start_time": "2025-12-13T10:23:17.908348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 문서의 내용 출력\n",
    "print(docs[12].page_content[:300])"
   ],
   "id": "7720d8d85de4c10b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPRi AI Brief |  \n",
      "2023-12 월호\n",
      "10삼성전자 , 자체 개발 생성 AI ‘삼성 가우스 ’ 공개\n",
      "n삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성 \n",
      "AI 모델 ‘삼성 가우스 ’를 공개\n",
      "n삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로 , 온디바이스 작동이 가능한 \n",
      "삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유KEY Contents\n",
      "£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스 , 온디바이스 작동 지원\n",
      "n삼성전자가 2\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:23:24.524228Z",
     "start_time": "2025-12-13T10:23:24.519229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# metadata 출력\n",
    "print(docs[12].metadata)\n"
   ],
   "id": "cd25874493704ad5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': 'week5Data\\\\SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13'}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PDFPlumber\n",
    "\n",
    "PyMuPDF와 마찬가지로, 출력 문서는 PDF와 그 페이지에 대한 자세한 메타데이터를 포함하며, 페이지 당 하나의 문서를 반환합니다."
   ],
   "id": "458d1b32e4b3ea6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:23:49.934680Z",
     "start_time": "2025-12-13T10:23:45.028653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# PDF 문서 로더 인스턴스 생성\n",
    "loader = PDFPlumberLoader(FILE_PATH)\n",
    "\n",
    "# 문서 로딩\n",
    "docs = loader.load()\n",
    "\n",
    "# 첫 번째 문서 데이터 접근\n",
    "print(docs[10].page_content[:300])"
   ],
   "id": "c18682fe99f564d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPRi AI Brief |\n",
      "2023-12월호\n",
      "코히어, 데이터 투명성 확보를 위한 데이터 출처 탐색기 공개\n",
      "KEY Contents\n",
      "n 코히어와 12개 기관이 광범위한 데이터셋에 대한 감사를 통해 원본 데이터 출처, 재라이선스 상태,\n",
      "작성자 등 다양한 정보를 제공하는 ‘데이터 출처 탐색기’ 플랫폼을 출시\n",
      "n 대화형 플랫폼을 통해 개발자는 데이터셋의 라이선스 상태를 쉽게 파악할 수 있으며 데이터셋의\n",
      "구성과 계보도 추적 가능\n",
      "£데이터 출처 탐색기, 광범위한 데이터셋 정보 제공을 통해 데이터 투명성 향상\n",
      "n AI 기업 코히어(Cohere)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:23:50.051065Z",
     "start_time": "2025-12-13T10:23:50.039547Z"
    }
   },
   "cell_type": "code",
   "source": "show_metadata(docs)\n",
   "id": "c5029b043ac7805a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source', 'file_path', 'page', 'total_pages', 'Author', 'Creator', 'Producer', 'CreationDate', 'ModDate', 'PDFVersion']\n",
      "\n",
      "[examples]\n",
      "source       : week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "file_path    : week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "page         : 0\n",
      "total_pages  : 23\n",
      "Author       : dj\n",
      "Creator      : Hwp 2018 10.0.0.13462\n",
      "Producer     : Hancom PDF 1.3.0.542\n",
      "CreationDate : D:20231208132838+09'00'\n",
      "ModDate      : D:20231208132838+09'00'\n",
      "PDFVersion   : 1.4\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 03. 한글(HWP)",
   "id": "d104554120053ce2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:44:45.180821Z",
     "start_time": "2025-12-13T10:44:45.176618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 설치\n",
    "# !pip install -qU langchain-teddynote"
   ],
   "id": "417dfa5c67aa2916",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:47:40.548771Z",
     "start_time": "2025-12-13T10:47:40.536440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_teddynote.document_loaders import HWPLoader\n",
    "\n",
    "# HWP Loader 객체 생성\n",
    "loader = HWPLoader(\"week5Data/디지털 정부혁신 추진계획.hwp\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()"
   ],
   "id": "5f5ae6171cf19883",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:47:53.971411Z",
     "start_time": "2025-12-13T10:47:53.965740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 결과 출력\n",
    "print(docs[0].page_content[:1000])"
   ],
   "id": "286ceae3c01d8adc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디지털 정부혁신 추진계획2019. 10. 29.      관계부처 합동순    서Ⅰ. 개요ȃ 1Ⅱ. 디지털 정부혁신 추진계획ㆬȃ 2  1. 우선 추진과제ȃ 2     ① 선제적·통합적 대국민 서비스 혁신     ② 공공부문 마이데이터 활성화     ③ 시민참여를 위한 플랫폼 고도화     ④ 현장중심 협업을 지원하는 스마트 업무환경 구현     ⑤ 클라우드와 디지털서비스 이용 활성화     ⑥ 개방형 데이터·서비스 생태계 구축  2. 중장기 범정부 디지털 전환 로드맵 수립ᲈȃ 4Ⅲ. 추진체계 및 일정ȃ 4 [붙임] 디지털 정부혁신 우선 추진과제(상세)ᬜȃ 8Ⅰ. 개 요□ 추진 배경 ○ 우리나라는 국가적 초고속 정보통신망 투자와 적극적인 공공정보화 사업 추진에 힘입어 세계 최고수준의 전자정부를 구축‧운영     * UN전자정부평가에서 2010‧12‧14년 1위, 16‧18년 3위, UN공공행정상 13회 수상 ○ 그러나, 인공지능‧클라우드 중심의 디지털 전환(Digital Transformation) 시대가 도래함에 따라 기존 전자정부의 한계 표출   - 축적된 행정데이터에도 불구하고 기관간 연계‧활용 미흡, 부처 단위로 단절된 서비스, 신기술 활용을 위한 제도‧기반 부족   - 디지털 전환을 위한 컨트롤타워가 없고, 구체적 전략도 부재 ○ 이에, ‘19.3월부터 공공부문 ICT 활용현황 및 문제점 검토에 착수하여 공공분야 디지털 전환을 위한 추진계획 마련     * 관계부처 협의 21회(행안,과기정통,기재,복지,권익위,국정원 등), 민간전문가 의견청취 10회□ 문제점 진단 및 평가 ○ (서비스) 국민과 최종 이용자 관점에서 서비스 혁신 미흡   - 자격이 있어도 자신이 받을 수 있는 공공서비스를 파악하기 어려워 사각지대가 발생하고, 온라인 신청 가능한 서비스도 제한적 ○ (데이터) 기관별로 축적·보유한 데이터의 연계와 활용 부족   - A기관에서 서류를 발급받아 B기관에 제출하는 관행(연간 증명서 9.5억건‘18년 발급) 등 데이터가 국민편익 향상에 제대로 활용\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "metadata 에는 파일명 정보가 담겨 있습니다.",
   "id": "aff7a518f7190482"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:48:41.213283Z",
     "start_time": "2025-12-13T10:48:41.206416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 결과 출력\n",
    "print(docs[0].metadata)"
   ],
   "id": "f6616f13b9939214",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'week5Data/디지털 정부혁신 추진계획.hwp'}\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 04. CSV",
   "id": "75882e4ff2291995"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`CSV`\n",
    "\n",
    "`Comma-Separated Values` (CSV) 파일은 쉼표로 값을 구분하는 구분된 텍스트 파일입니다. 파일의 각 줄은 데이터 레코드입니다.\n",
    "\n",
    "각 레코드는 쉼표로 구분된 하나 이상의 필드로 구성됩니다."
   ],
   "id": "b4c0f6fa8a60ee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## CSVLoader\n",
    "\n",
    "CSV 데이터를 문서당 한 행씩 로드합니다."
   ],
   "id": "c86c5bc7b151b7b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:51:34.909219Z",
     "start_time": "2025-12-13T10:51:34.876036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# CSV 로더 생성\n",
    "loader = CSVLoader(file_path=\"./week5Data/titanic.csv\")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].metadata)\n"
   ],
   "id": "f75931894217fcf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "{'source': './week5Data/titanic.csv', 'row': 0}\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## CSV 파싱 및 로딩 커스터마이징\n",
    "\n",
    "csv module 문서를 참조하여 지원되는 csv args에 대한 자세한 정보를 확인하세요."
   ],
   "id": "d49de8e6fe59c0ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:53:13.572793Z",
     "start_time": "2025-12-13T10:53:13.556411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 컬럼정보:\n",
    "# PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
    "\n",
    "# CSV 파일 경로\n",
    "loader = CSVLoader(\n",
    "    file_path=\"./data/titanic.csv\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",  # 구분자\n",
    "        \"quotechar\": '\"',  # 인용 부호 문자\n",
    "        \"fieldnames\": [\n",
    "            \"Passenger ID\",\n",
    "            \"Survival (1: Survived, 0: Died)\",\n",
    "            \"Passenger Class\",\n",
    "            \"Name\",\n",
    "            \"Sex\",\n",
    "            \"Age\",\n",
    "            \"Number of Siblings/Spouses Aboard\",\n",
    "            \"Number of Parents/Children Aboard\",\n",
    "            \"Ticket Number\",\n",
    "            \"Fare\",\n",
    "            \"Cabin\",\n",
    "            \"Port of Embarkation\",\n",
    "        ],  # 필드 이름\n",
    "    },\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 데이터 출력\n",
    "print(docs[1].page_content)"
   ],
   "id": "a2af3d5a85b4c155",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger ID: 1\n",
      "Survival (1: Survived, 0: Died): 0\n",
      "Passenger Class: 3\n",
      "Name: Braund, Mr. Owen Harris\n",
      "Sex: male\n",
      "Age: 22\n",
      "Number of Siblings/Spouses Aboard: 1\n",
      "Number of Parents/Children Aboard: 0\n",
      "Ticket Number: A/5 21171\n",
      "Fare: 7.25\n",
      "Cabin: \n",
      "Port of Embarkation: S\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`source_column` 인자를 사용하여 각 행에서 생성된 문서의 출처를 지정하세요. 그렇지 않으면 모든 문서의 출처로 file_path가 사용됩니다.\n",
    "\n",
    "이는 CSV 파일에서 로드된 문서를 출처를 사용하여 질문에 답하는 체인에 사용할 때 유용합니다."
   ],
   "id": "57811da6ab9714b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:53:40.832296Z",
     "start_time": "2025-12-13T10:53:40.827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"./data/titanic.csv\", source_column=\"PassengerId\"\n",
    ")  # CSV 로더 설정, 파일 경로 및 소스 컬럼 지정\n",
    "\n",
    "docs = loader.load()  # 데이터 로드\n",
    "\n",
    "print(docs[1])  # 데이터 출력"
   ],
   "id": "63fe984583f207fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='PassengerId: 2\n",
      "Survived: 1\n",
      "Pclass: 1\n",
      "Name: Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n",
      "Sex: female\n",
      "Age: 38\n",
      "SibSp: 1\n",
      "Parch: 0\n",
      "Ticket: PC 17599\n",
      "Fare: 71.2833\n",
      "Cabin: C85\n",
      "Embarked: C' metadata={'source': '2', 'row': 1}\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## UnstructuredCSVLoader\n",
    "\n",
    "`UnstructuredCSVLoader`를 사용하여 테이블을 로드할 수도 있습니다. `UnstructuredCSVLoader`를 사용하는 한 가지 장점은 \"elements\" 모드에서 사용할 경우, 메타데이터에서 테이블의 HTML 표현이 제공된다는 것입니다."
   ],
   "id": "ee4af31f8e478e69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:54:11.833063Z",
     "start_time": "2025-12-13T10:54:10.357328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader\n",
    "\n",
    "# 비구조화 CSV 로더 인스턴스 생성\n",
    "loader = UnstructuredCSVLoader(file_path=\"./data/titanic.csv\", mode=\"elements\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 첫 번째 문서의 HTML 텍스트 메타데이터 출력\n",
    "print(docs[0].metadata[\"text_as_html\"][:1000])"
   ],
   "id": "41d1c0249fa99cea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table><tr><td>PassengerId</td><td>Survived</td><td>Pclass</td><td>Name</td><td>Sex</td><td>Age</td><td>SibSp</td><td>Parch</td><td>Ticket</td><td>Fare</td><td>Cabin</td><td>Embarked</td></tr><tr><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td/><td>S</td></tr><tr><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr><tr><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td/><td>S</td></tr><tr><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr><tr><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35</\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataFrameLoader\n",
    "\n",
    "Pandas는 Python 프로그래밍 언어를 위한 오픈 소스 데이터 분석 및 조작 도구입니다. 이 라이브러리는 데이터 과학, 머신러닝, 그리고 다양한 분야의 데이터 작업에 널리 사용되고 있습니다."
   ],
   "id": "aa1f1ed6cd3724cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:54:36.404428Z",
     "start_time": "2025-12-13T10:54:36.388263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(\"./data/titanic.csv\")"
   ],
   "id": "fbb9842b1cc44788",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:54:52.465091Z",
     "start_time": "2025-12-13T10:54:52.455091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터프레임의 처음 다섯 행 조회\n",
    "df.head()"
   ],
   "id": "67fa36448773eea7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:54:59.172011Z",
     "start_time": "2025-12-13T10:54:59.154308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# 데이터 프레임 로더 설정, 페이지 내용 컬럼 지정\n",
    "loader = DataFrameLoader(df, page_content_column=\"Name\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 데이터 출력\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# 메타데이터 출력\n",
    "print(docs[0].metadata)"
   ],
   "id": "825eca6d8b9bb9a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Braund, Mr. Owen Harris\n",
      "{'PassengerId': 1, 'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 22.0, 'SibSp': 1, 'Parch': 0, 'Ticket': 'A/5 21171', 'Fare': 7.25, 'Cabin': nan, 'Embarked': 'S'}\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:55:03.423951Z",
     "start_time": "2025-12-13T10:55:03.419147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 큰 테이블에 대한 지연 로딩, 전체 테이블을 메모리에 로드하지 않음\n",
    "for row in loader.lazy_load():\n",
    "    print(row)\n",
    "    break  # 첫 행만 출력"
   ],
   "id": "5d7c863632ad70e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Braund, Mr. Owen Harris' metadata={'PassengerId': 1, 'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 22.0, 'SibSp': 1, 'Parch': 0, 'Ticket': 'A/5 21171', 'Fare': 7.25, 'Cabin': nan, 'Embarked': 'S'}\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 05. Excel",
   "id": "b509cbfabe4d16d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`UnstructuredExcelLoader`는 Microsoft Excel 파일을 로드하는 데 사용됩니다.\n",
    "\n",
    "이 로더는 `.xlsx` 및 `.xls` 파일 모두에서 작동합니다. 페이지 내용은 Excel 파일의 원시 텍스트가 됩니다.\n",
    "\n",
    "\"elements\" 모드에서 로더를 사용하는 경우, 문서 메타데이터의 text_as_html 키 아래에서 Excel 파일의 HTML 표현이 제공됩니다"
   ],
   "id": "17f84bf8d26f66ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:56:53.631507Z",
     "start_time": "2025-12-13T10:56:53.628367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 설치\n",
    "# !pip install -qU langchain-community unstructured openpyxl"
   ],
   "id": "2d0642e808358f2a",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:57:19.074865Z",
     "start_time": "2025-12-13T10:57:08.001103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "# UnstructuredExcelLoader 생성\n",
    "loader = UnstructuredExcelLoader(\"./week5Data/titanic.xlsx\", mode=\"elements\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 길이 출력\n",
    "print(len(docs))\n"
   ],
   "id": "cae64011edad1fe0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1개의 문서로 로드되었음을 확인합니다.\n",
    "\n",
    "page_content 에는 각 행의 데이터가 저장되고, metadata 의 text_as_html 에는 각 행의 데이터를 HTML 형식으로 저장합니다."
   ],
   "id": "58f36d4d198c4926"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:57:20.070307Z",
     "start_time": "2025-12-13T10:57:20.065307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 문서 출력\n",
    "print(docs[0].page_content[:200])"
   ],
   "id": "78fb0ddd694507d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 1 0 3 Braund, Mr. Owen Harris male 22 1 0 A/5 21171 7.25 S 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) fem\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:57:26.122052Z",
     "start_time": "2025-12-13T10:57:26.117157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# metadata 의 text_as_html 출력\n",
    "print(docs[0].metadata[\"text_as_html\"][:1000])"
   ],
   "id": "811a0ba735a612b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table><tr><td>PassengerId</td><td>Survived</td><td>Pclass</td><td>Name</td><td>Sex</td><td>Age</td><td>SibSp</td><td>Parch</td><td>Ticket</td><td>Fare</td><td>Cabin</td><td>Embarked</td></tr><tr><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td/><td>S</td></tr><tr><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr><tr><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td/><td>S</td></tr><tr><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr><tr><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35</\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataFrameLoader\n",
    "\n",
    "CSV 파일과 마찬가지로 Excel 파일을 로드하는 read_excel() 기능을 사용하여 DataFrame 으로 만든 뒤, 로드합니다."
   ],
   "id": "b0ae8e7d5a708484"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:57:45.312878Z",
     "start_time": "2025-12-13T10:57:45.221794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel 파일 읽기\n",
    "df = pd.read_excel(\"./week5Data/titanic.xlsx\")\n"
   ],
   "id": "d2aec904d902db57",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T10:57:49.635855Z",
     "start_time": "2025-12-13T10:57:49.601911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# 데이터 프레임 로더 설정, 페이지 내용 컬럼 지정\n",
    "loader = DataFrameLoader(df, page_content_column=\"Name\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 데이터 출력\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# 메타데이터 출력\n",
    "print(docs[0].metadata)"
   ],
   "id": "467797f3da485427",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Braund, Mr. Owen Harris\n",
      "{'PassengerId': 1, 'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 22.0, 'SibSp': 1, 'Parch': 0, 'Ticket': 'A/5 21171', 'Fare': 7.25, 'Cabin': nan, 'Embarked': 'S'}\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 06. Word",
   "id": "6b35a10639934adc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Microsoft Word\n",
    "\n",
    "Microsoft Word는 Microsoft에서 개발한 워드 프로세서입니다.\n",
    "\n",
    "이는 Word 문서를 하류에서 사용할 수 있는 문서 형식으로 로드하는 방법을 다룹니다."
   ],
   "id": "65d7b46ea06adeaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Docx2txtLoader\n",
    "\n",
    "Docx2txt를 사용하여 .docx 파일을 문서로 불러올 수 있습니다."
   ],
   "id": "83bc5028be51ca81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:03:40.980537Z",
     "start_time": "2025-12-13T11:03:40.976496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 설치\n",
    "# !pip install -qU docx2txt"
   ],
   "id": "beda25613060b19d",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:03:50.201606Z",
     "start_time": "2025-12-13T11:03:50.190260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"./week5Data/sample-word-document.docx\")  # 문서 로더 초기화\n",
    "\n",
    "docs = loader.load()  # 문서 로딩\n",
    "\n",
    "print(len(docs))"
   ],
   "id": "7592f603b03aa134",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## UnstructuredWordDocumentLoader",
   "id": "1c842d9b2607578b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:18.440648Z",
     "start_time": "2025-12-13T11:04:14.756602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "# 비구조화된 워드 문서 로더 인스턴스화\n",
    "loader = UnstructuredWordDocumentLoader(\"./week5Data/sample-word-document.docx\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ],
   "id": "3e52587a457b83a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "결과는 1개의 단일 Document 로 로드됩니다.",
   "id": "9366788d4a2018b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:25.848723Z",
     "start_time": "2025-12-13T11:04:25.842345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# metadata 출력\n",
    "print(docs[0].metadata)"
   ],
   "id": "89c1ff6a82f09c69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': './week5Data/sample-word-document.docx'}\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "내부적으로 비정형은 텍스트 덩어리마다 서로 다른 \"요소\"를 만듭니다.\n",
    "\n",
    "기본적으로 이들은 함께 결합되어 있지만 mode=\"elements\" 를 지정하여 쉽게 분리할 수 있습니다."
   ],
   "id": "6469e353476708fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:41.383405Z",
     "start_time": "2025-12-13T11:04:41.187139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UnstructuredWordDocumentLoader\n",
    "loader = UnstructuredWordDocumentLoader(\n",
    "    \"./week5Data/sample-word-document.docx\", mode=\"elements\"\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 로드한 문서의 개수 출력\n",
    "print(len(docs))\n"
   ],
   "id": "bf36936e7cebd7c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:46.334077Z",
     "start_time": "2025-12-13T11:04:46.330266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 첫번째 문서의 내용 출력\n",
    "print(docs[0].page_content)"
   ],
   "id": "6d40fdf46ef82c8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:54.149159Z",
     "start_time": "2025-12-13T11:04:54.142805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 첫번째 문서의 내용 출력\n",
    "docs[0].metadata"
   ],
   "id": "b248ca638331e497",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './week5Data/sample-word-document.docx',\n",
       " 'category_depth': 0,\n",
       " 'file_directory': './week5Data',\n",
       " 'filename': 'sample-word-document.docx',\n",
       " 'last_modified': '2025-10-03T21:08:16',\n",
       " 'page_number': 1,\n",
       " 'languages': ['kor'],\n",
       " 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
       " 'category': 'UncategorizedText',\n",
       " 'element_id': 'a7703edf875ec776dc2bb839ca335b45'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 07. PowerPoint",
   "id": "d0a35aa6c971affa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`Microsoft PowerPoint`\n",
    "> Microsoft PowerPoint는 Microsoft에서 개발한 프레젠테이션 프로그램입니다.\n",
    "\n",
    "이는 Microsoft PowerPoint 문서를 다운스트림에서 사용할 수 있는 문서 형식으로 로드하는 방법을 다룹니다.\n",
    "\n",
    "자세한 Unstructured 설정 방법은 공식 도큐먼트를 참조하십시오."
   ],
   "id": "cb5e147bfc6a1554"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:09:22.093010Z",
     "start_time": "2025-12-13T11:09:22.087788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 패키지 설치\n",
    "# !pip install -qU unstructured python-pptx"
   ],
   "id": "5797632873fd7493",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:10:34.006657Z",
     "start_time": "2025-12-13T11:10:31.114997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "\n",
    "# UnstructuredPowerPointLoader 생성\n",
    "loader = UnstructuredPowerPointLoader(\"./week5Data/sample-ppt.pptx\")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 로드한 문서의 개수 출력\n",
    "print(len(docs))"
   ],
   "id": "ded936486639b955",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`Unstructured`는 텍스트의 다양한 chunks에 대해 다양한 \"elements\"를 생성합니다.\n",
    "\n",
    "기본적으로 결합되어 하나의 document 로 반환하지만, mode=\"elements\"를 지정함으로써 쉽게 요소들을 분리할 수 있습니다."
   ],
   "id": "31ca7eae56112728"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:10:55.546918Z",
     "start_time": "2025-12-13T11:10:55.490491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UnstructuredPowerPointLoader 생성\n",
    "loader = UnstructuredPowerPointLoader(\"./week5Data/sample-ppt.pptx\", mode=\"elements\")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ],
   "id": "1240e06d3b1cf4d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Unstructured는 텍스트의 다양한 chunks에 대해 다양한 \"elements\"를 생성합니다.\n",
    "\n",
    "기본적으로 결합되어 하나의 document 로 반환하지만, mode=\"elements\"를 지정함으로써 쉽게 요소들을 분리할 수 있습니다."
   ],
   "id": "86982416241e2e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:12:10.615397Z",
     "start_time": "2025-12-13T11:12:10.562012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UnstructuredPowerPointLoader 생성\n",
    "loader = UnstructuredPowerPointLoader(\"./week5Data/sample-ppt.pptx\", mode=\"elements\")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ],
   "id": "9caa8c1bcd635ceb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:12:15.258761Z",
     "start_time": "2025-12-13T11:12:15.254630Z"
    }
   },
   "cell_type": "code",
   "source": "print(docs[0].page_content)",
   "id": "44ca734dcf6ffa7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랭체인 한국어 튜토리얼\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:12:19.033073Z",
     "start_time": "2025-12-13T11:12:19.027513Z"
    }
   },
   "cell_type": "code",
   "source": "docs[0].metadata",
   "id": "c95c75c959567fe4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './week5Data/sample-ppt.pptx',\n",
       " 'category_depth': 0,\n",
       " 'file_directory': './week5Data',\n",
       " 'filename': 'sample-ppt.pptx',\n",
       " 'last_modified': '2025-10-03T21:08:16',\n",
       " 'page_number': 1,\n",
       " 'languages': ['kor'],\n",
       " 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
       " 'category': 'Title',\n",
       " 'element_id': '784918cd25f6cbe0084401b85fe2f3d8'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 08. 웹문서(WebBaseLoader)\n",
    "WebBaseLoader 는 웹 기반 문서를 로드하는 로더입니다.\n",
    "\n",
    "bs4 라이브러리를 사용하여 웹 페이지를 파싱합니다.\n",
    "\n",
    "bs4.SoupStrainer 를 사용하여 파싱할 요소를 지정합니다.\n",
    "bs_kwargs 매개변수를 사용하여 bs4.SoupStrainer 의 추가적인 인수를 지정합니다."
   ],
   "id": "4a069bc1c61da244"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:16:24.082593Z",
     "start_time": "2025-12-13T11:16:23.520320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 뉴스기사 내용을 로드합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://n.news.naver.com/article/437/0000378416\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    "        )\n",
    "    ),\n",
    "    header_template={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\",\n",
    "    },\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs"
   ],
   "id": "7cb7292a3c9123d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content=\"\\n출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\\n\\n\\n[앵커]올해 아이 낳을 계획이 있는 가족이라면 솔깃할 소식입니다. 정부가 저출생 대책으로 매달 주는 부모 급여, 0세 아이는 100만원으로 올렸습니다. 여기에 첫만남이용권, 아동수당까지 더하면 아이 돌까지 1년 동안 1520만원을 받습니다. 지자체도 경쟁하듯 지원에 나섰습니다. 인천시는 새로 태어난 아기, 18살될 때까지 1억원을 주겠다. 광주시도 17살될 때까지 7400만원 주겠다고 했습니다. 선거 때면 나타나서 아이 낳으면 현금 주겠다고 밝힌 사람이 있었죠. 과거에는 표만 노린 '황당 공약'이라는 비판이 따라다녔습니다. 그런데 지금은 출산율이 이보다 더 나쁠 수 없다보니, 이런 현금성 지원을 진지하게 정책화 하는 상황까지 온 겁니다. 게다가 기업들도 뛰어들고 있습니다. 이번에는 출산한 직원에게 단번에 1억원을 주겠다는 회사까지 나타났습니다.이상화 기자가 취재했습니다.[기자]한 그룹사가 오늘 파격적인 저출생 정책을 내놨습니다.2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다.해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다.[오현석/부영그룹 직원 : 아이 키우는 데 금전적으로 많이 힘든 세상이잖아요. 교육이나 생활하는 데 큰 도움이 될 거라 생각합니다.]만약 셋째까지 낳는 경우엔 국민주택을 제공하겠다는 뜻도 밝혔습니다.[이중근/부영그룹 회장 : 3년 이내에 세 아이를 갖는 분이 나올 것이고 따라서 주택을 제공할 수 있는 계기가 될 것으로 생각하고.][조용현/부영그룹 직원 : 와이프가 셋째도 갖고 싶어 했는데 경제적 부담 때문에 부정적이었거든요. (이제) 긍정적으로 생각할 수 있을 것 같습니다.]오늘 행사에서는, 회사가 제공하는 출산장려금은 받는 직원들의 세금 부담을 고려해 정부가 면세해달라는 제안도 나왔습니다.이같은 출산장려책은 점점 확산하는 분위기입니다.법정기간보다 육아휴직을 길게 주거나, 남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]\\n\\t\\t\\n\")]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SSL 인증 오류를 우회하기 위해, \"verify\" 옵션을 설정할 수 있습니다.",
   "id": "ebfe61c5da8c3f23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:16:05.841791Z",
     "start_time": "2025-12-13T11:16:05.306234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ssl 인증 우회\n",
    "loader.requests_kwargs = {\"verify\": False}\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()"
   ],
   "id": "2c544c338abc1865",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'n.news.naver.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "여러 웹페이지를 한 번에 로드할 수도 있습니다. 이를 위해 urls의 리스트를 로더에 전달하면, 전달된 urls의 순서대로 문서 리스트를 반환합니다.",
   "id": "b8f0b85e9d075307"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:16:32.703962Z",
     "start_time": "2025-12-13T11:16:31.661335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=[\n",
    "        \"https://n.news.naver.com/article/437/0000378416\",\n",
    "        \"https://n.news.naver.com/mnews/hotissue/article/092/0002340014?type=series&cid=2000063\",\n",
    "    ],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    "        )\n",
    "    ),\n",
    "    header_template={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 수 확인\n",
    "print(len(docs))\n"
   ],
   "id": "32f4860fd1c48ec2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "웹에서 가져온 결과를 출력합니다.",
   "id": "f376f00140eced47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:16:43.816195Z",
     "start_time": "2025-12-13T11:16:43.810516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(docs[0].page_content[:500])\n",
    "print(\"===\" * 10)\n",
    "print(docs[1].page_content[:500])"
   ],
   "id": "b2bfdc92f59a4fbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\n",
      "\n",
      "\n",
      "[앵커]올해 아이 낳을 계획이 있는 가족이라면 솔깃할 소식입니다. 정부가 저출생 대책으로 매달 주는 부모 급여, 0세 아이는 100만원으로 올렸습니다. 여기에 첫만남이용권, 아동수당까지 더하면 아이 돌까지 1년 동안 1520만원을 받습니다. 지자체도 경쟁하듯 지원에 나섰습니다. 인천시는 새로 태어난 아기, 18살될 때까지 1억원을 주겠다. 광주시도 17살될 때까지 7400만원 주겠다고 했습니다. 선거 때면 나타나서 아이 낳으면 현금 주겠다고 밝힌 사람이 있었죠. 과거에는 표만 노린 '황당 공약'이라는 비판이 따라다녔습니다. 그런데 지금은 출산율이 이보다 더 나쁠 수 없다보니, 이런 현금성 지원을 진지하게 정책화 하는 상황까지 온 겁니다. 게다가 기업들도 뛰어들고 있습니다. 이번에는 출산한 직원에게 단번에 1억원을 주겠다는 회사까지 나타났습니다.이상화 기자가 취재했습니다.[기자]한 그룹사가 오늘 파격적인 저출생 정책을 내놨\n",
      "==============================\n",
      "\n",
      "고속 성장하는 스타트업엔 레드팀이 필요하다\n",
      "\n",
      "\n",
      "[이균성의 溫技] 초심, 본질을 잃을 때한 스타트업 창업자와 최근 점심을 같이 했다. 조언을 구할 게 있다고 했다. 당장 급한 현안이 있는 건 아니었다. 여러 번 창업한 경험이 있는데 지금 하고 있는 아이템은 대박 느낌이 든다고 헸다. 그런데 오히려 더 조심해야겠다는 생각이 들더란다. 조언을 구하고자 하는 바도 성장이 예상될 때 무엇을 경계해야 할지 알고 싶다는 거였다. 적잖은 스타트업 창업자를 만났지만 드문 사례였다.2년 가까이 스타트업 창업자를 릴레이 인터뷰 하면서 의미 있게 생각했던 것이 두 가지 있다. 첫째, 회사라는 단어보다 팀이라는 어휘를 주로 쓰고 있다는 점이었다. 그 표현의 유래나 의미 때문이라기보다는 팀이라는 말이 더 정겨워 뜻 깊게 생각된 듯하다. 이해관계보다 지향하는 뜻에 더 중점을 두고 하나의 마음으로 한 곳을 향해 달려가는 집단을 가리키는 표현이라는 생각에 더 정겨웠다.스타트업 대표들의 창업 동기는 대부분 ‘사회\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "여러 URL을 동시에 스크래핑하면 스크래핑 과정을 가속화할 수 있습니다.\n",
    "\n",
    "동시 요청에는 합리적인 제한이 있으며, 기본값은 초당 2회입니다. 서버 부하에 대해 걱정하지 않거나, 스크래핑하는 서버를 제어하는 경우라면 requests_per_second 매개변수를 변경하여 최대 동시 요청 수를 늘릴 수 있습니다. 이 방법은 스크래핑 속도를 높일 수 있지만, 서버로부터 차단될 수 있으므로 주의해야 합니다."
   ],
   "id": "29f8c6686e5092cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:18:43.031290Z",
     "start_time": "2025-12-13T11:18:43.024225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# jupyter notebook 에서만 실행(asyncio)\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "id": "97ef334e29cabb4a",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:18:47.118750Z",
     "start_time": "2025-12-13T11:18:46.730791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 초당 요청 수 설정\n",
    "loader.requests_per_second = 1\n",
    "\n",
    "# 비동기 로드\n",
    "docs = loader.aload()"
   ],
   "id": "70101867f524e25b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osca0\\AppData\\Local\\Temp\\ipykernel_7584\\132226298.py:5: LangChainDeprecationWarning: See API reference for updated usage: https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html\n",
      "  docs = loader.aload()\n",
      "Fetching pages: 100%|##########| 2/2 [00:00<00:00,  5.92it/s]\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:18:57.383219Z",
     "start_time": "2025-12-13T11:18:57.378096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 결과 출력\n",
    "docs"
   ],
   "id": "dad8b6222191d530",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content=\"\\n출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\\n\\n\\n[앵커]올해 아이 낳을 계획이 있는 가족이라면 솔깃할 소식입니다. 정부가 저출생 대책으로 매달 주는 부모 급여, 0세 아이는 100만원으로 올렸습니다. 여기에 첫만남이용권, 아동수당까지 더하면 아이 돌까지 1년 동안 1520만원을 받습니다. 지자체도 경쟁하듯 지원에 나섰습니다. 인천시는 새로 태어난 아기, 18살될 때까지 1억원을 주겠다. 광주시도 17살될 때까지 7400만원 주겠다고 했습니다. 선거 때면 나타나서 아이 낳으면 현금 주겠다고 밝힌 사람이 있었죠. 과거에는 표만 노린 '황당 공약'이라는 비판이 따라다녔습니다. 그런데 지금은 출산율이 이보다 더 나쁠 수 없다보니, 이런 현금성 지원을 진지하게 정책화 하는 상황까지 온 겁니다. 게다가 기업들도 뛰어들고 있습니다. 이번에는 출산한 직원에게 단번에 1억원을 주겠다는 회사까지 나타났습니다.이상화 기자가 취재했습니다.[기자]한 그룹사가 오늘 파격적인 저출생 정책을 내놨습니다.2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다.해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다.[오현석/부영그룹 직원 : 아이 키우는 데 금전적으로 많이 힘든 세상이잖아요. 교육이나 생활하는 데 큰 도움이 될 거라 생각합니다.]만약 셋째까지 낳는 경우엔 국민주택을 제공하겠다는 뜻도 밝혔습니다.[이중근/부영그룹 회장 : 3년 이내에 세 아이를 갖는 분이 나올 것이고 따라서 주택을 제공할 수 있는 계기가 될 것으로 생각하고.][조용현/부영그룹 직원 : 와이프가 셋째도 갖고 싶어 했는데 경제적 부담 때문에 부정적이었거든요. (이제) 긍정적으로 생각할 수 있을 것 같습니다.]오늘 행사에서는, 회사가 제공하는 출산장려금은 받는 직원들의 세금 부담을 고려해 정부가 면세해달라는 제안도 나왔습니다.이같은 출산장려책은 점점 확산하는 분위기입니다.법정기간보다 육아휴직을 길게 주거나, 남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]\\n\\t\\t\\n\"),\n",
       " Document(metadata={'source': 'https://n.news.naver.com/mnews/hotissue/article/092/0002340014?type=series&cid=2000063'}, page_content='\\n고속 성장하는 스타트업엔 레드팀이 필요하다\\n\\n\\n[이균성의 溫技] 초심, 본질을 잃을 때한 스타트업 창업자와 최근 점심을 같이 했다. 조언을 구할 게 있다고 했다. 당장 급한 현안이 있는 건 아니었다. 여러 번 창업한 경험이 있는데 지금 하고 있는 아이템은 대박 느낌이 든다고 헸다. 그런데 오히려 더 조심해야겠다는 생각이 들더란다. 조언을 구하고자 하는 바도 성장이 예상될 때 무엇을 경계해야 할지 알고 싶다는 거였다. 적잖은 스타트업 창업자를 만났지만 드문 사례였다.2년 가까이 스타트업 창업자를 릴레이 인터뷰 하면서 의미 있게 생각했던 것이 두 가지 있다. 첫째, 회사라는 단어보다 팀이라는 어휘를 주로 쓰고 있다는 점이었다. 그 표현의 유래나 의미 때문이라기보다는 팀이라는 말이 더 정겨워 뜻 깊게 생각된 듯하다. 이해관계보다 지향하는 뜻에 더 중점을 두고 하나의 마음으로 한 곳을 향해 달려가는 집단을 가리키는 표현이라는 생각에 더 정겨웠다.스타트업 대표들의 창업 동기는 대부분 ‘사회 문제를 기업 방식으로 푼다’는 거다. 인간 사회에는 풀어야 할 여러 문제가 있고 해결방법은 다양하다. 그런데 어떤 것은 기업의 방식으로 푸는 것이 가장 효율적일 수 있다. 이윤의 동기와 결합될 때 문제가 더 잘 풀릴 수 있는 사안도 많은 것이다. 그래서 ‘사회 문제를 기업 방식으로 푼다’는 창업 동기 또한 팀이라는 말 못잖게 마음에 들었다.\\n\\n\\n\\n스타트업 창업자 (출처=픽사베이)두 가지는 그러나 어떤 측면에서 보면 ‘정리된 지향’일 뿐 ‘내재적 실체’는 아닐 수도 있다. 지향은 관념과 구호만으로도 충분하지만, 실체는 쉬지 않고 끝없이 가꾸는 행동으로만 조금씩 구현되는 것일 텐데, 어떤 기업의 경우 전자만 요란하고 후자는 느슨해 보인다. 기업의 규모가 조금씩 커져갈수록 전자와 후자의 간극이 벌어지는 경향도 느껴진다. 이 경향이 기업 리스크의 본질로 생각된다.창업은 ‘사회 문제를 기업 방식으로 푼다’는 관념으로부터 출발한다. 어떤 문제의식을 느끼고 해결방안이 떠올랐을 때 팀을 꾸려 행동에 나서는 것이 창업이다. 그 행동이야 말로 ‘도전’이다. 그 행동을 통해 소비자가 느끼는 문제를 풀어낼 때 창업은 빛이 나고 기업의 존재 이유가 생기는 것이다. 안타까운 것은 첫 문제를 풀기도 어렵지만 문제를 풀고 나서는 더 큰 문제에 직면한다는 점이다.‘도전’은 그래서 대부분 ‘예고된 좌절’이기도 하다. 짧은 시간 안에 많은 창업가가 좌절하고 시간이 길어지면 좌절하지 않은 이가 드물다. 문제는 기하급수적으로 늘고 창업자 역량은 산술급수적으로 발전하기 때문이다. 문제는 달리는 호랑이와 같고 창업자는 그 위에 위태롭게 올라 탄 존재와 같다. 시간이 가면 갈수록 문제는 그의 통제 영역을 벗어난다. 그가 할 수 있는 일은 점점 줄어든다.스타트업으로 시작해 크게 성장한 여러 기업에서 최근 잡음이 끊이지 않는다. 창업자가 구속되거나, 파산 위기에 빠지거나, 협력사와 소비자의 거센 반발에 직면해 있다. 위기에 빠진 기업들에 대한 분석도 쏟아진다. 대부분 그 원인은 ‘무리(無理)’에서 찾아진다. 문어발식 확장, 인수합병을 위한 무리한 투자, 매출 부풀리기, 당장의 지표를 위해 미래의 이익을 당겨오는 방식 등이 그런 분석이다.무리는 사실 특별한 게 아니다. ‘정리된 지향’과 ‘내재적 실체’가 크게 벌어진 상태를 가리킨다. 지향과 실체가 벌어지면 팀이라는 초기 기업문화가 흔들리고, 소비자를 위한 사회문제 해결이라는 창업 동기도 사라져버린다. 오로지 앞뒤 가리지 않는 성장만이 유일한 가치가 된다. 맹목적 성장 추구야 말로 언젠가 반드시 떨어지고야 말 호랑이 등인데, 사달이 나기 전에는 그 사실을 알지 못 한다.기업은 달리는 자전거에 비유되곤 한다. 달리는 건 성장을 뜻하다. 성장하지 않으면 넘어진다는 의미다. 지속 성장은 그래서 꼭 필요하지만 그만큼 힘든 일이다. 하지만 성장 자체가 유일한 가치로 돌변하게 될 경우 기업은 사회 문제를 푸는 혁신 집단이 아니라 도리어 스스로 문제 덩어리로 전락해 사회에 부담이 될 수 있다. 찬사가 비난으로 바뀌는 건 순식간이다. 나락에 빠지는 것도 그렇다.문제를 풀기보다 키워 나락에 빠진 기업은 지향하는 모토와 실제 경영 사이의 간극이 되돌릴 수 없을 만큼 커져 있게 마련이다. 간극이 커가는 데도 승수 효과가 적용되는 것 같다. 처음엔 작았던 게 시간이 갈수록 감당할 수 없을 만큼 커진다. 그래서 고속 성장하는 스타트업에는 그 간극을 체크할 레드팀이 꼭 필요하다. 간극을 체크하지 않으면 본질을 잃게 되고 본질을 잃으면 지속성장도 없다.\\n\\n')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 프록시 사용\n",
    "IP 차단을 우회하기 위해 때때로 프록시를 사용할 필요가 있을 수 있습니다.\n",
    "\n",
    "프록시를 사용하려면 로더(및 그 아래의 requests)에 프록시 딕셔너리를 전달할 수 있습니다."
   ],
   "id": "d1529d6e9c0830ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:19:16.429973Z",
     "start_time": "2025-12-13T11:19:15.519558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = WebBaseLoader(\n",
    "    \"https://www.google.com/search?q=parrots\",\n",
    "    proxies={\n",
    "        \"http\": \"http://{username}:{password}:@proxy.service.com:6666/\",\n",
    "        \"https\": \"https://{username}:{password}:@proxy.service.com:6666/\",\n",
    "    },\n",
    "    # 웹 기반 로더 초기화\n",
    "    # 프록시 설정\n",
    ")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()"
   ],
   "id": "8152e13d0a8e7a8d",
   "outputs": [
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=parrots (Caused by ProxyError('Unable to connect to proxy', NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F1151C9E10>: Failed to resolve 'proxy.service.com' ([Errno 11001] getaddrinfo failed)\")))",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mgaierror\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\connection.py:198\u001B[39m, in \u001B[36mHTTPConnection._new_conn\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m198\u001B[39m     sock = \u001B[43mconnection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    199\u001B[39m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    200\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    201\u001B[39m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    202\u001B[39m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    203\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    204\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m socket.gaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001B[39m, in \u001B[36mcreate_connection\u001B[39m\u001B[34m(address, timeout, source_address, socket_options)\u001B[39m\n\u001B[32m     58\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m LocationParseError(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, label empty or too long\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43msocket\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msocket\u001B[49m\u001B[43m.\u001B[49m\u001B[43mSOCK_STREAM\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     61\u001B[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:962\u001B[39m, in \u001B[36mgetaddrinfo\u001B[39m\u001B[34m(host, port, family, type, proto, flags)\u001B[39m\n\u001B[32m    961\u001B[39m addrlist = []\n\u001B[32m--> \u001B[39m\u001B[32m962\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_socket\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    963\u001B[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001B[31mgaierror\u001B[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mNameResolutionError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:773\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    772\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m773\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_prepare_proxy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    774\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m, SocketTimeout) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001B[39m, in \u001B[36mHTTPSConnectionPool._prepare_proxy\u001B[39m\u001B[34m(self, conn)\u001B[39m\n\u001B[32m   1036\u001B[39m conn.set_tunnel(\n\u001B[32m   1037\u001B[39m     scheme=tunnel_scheme,\n\u001B[32m   1038\u001B[39m     host=\u001B[38;5;28mself\u001B[39m._tunnel_host,\n\u001B[32m   1039\u001B[39m     port=\u001B[38;5;28mself\u001B[39m.port,\n\u001B[32m   1040\u001B[39m     headers=\u001B[38;5;28mself\u001B[39m.proxy_headers,\n\u001B[32m   1041\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1042\u001B[39m \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\connection.py:704\u001B[39m, in \u001B[36mHTTPSConnection.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    703\u001B[39m sock: socket.socket | ssl.SSLSocket\n\u001B[32m--> \u001B[39m\u001B[32m704\u001B[39m \u001B[38;5;28mself\u001B[39m.sock = sock = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    705\u001B[39m server_hostname: \u001B[38;5;28mstr\u001B[39m = \u001B[38;5;28mself\u001B[39m.host\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\connection.py:205\u001B[39m, in \u001B[36mHTTPConnection._new_conn\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    204\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m socket.gaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m205\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m NameResolutionError(\u001B[38;5;28mself\u001B[39m.host, \u001B[38;5;28mself\u001B[39m, e) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    206\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m SocketTimeout \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[31mNameResolutionError\u001B[39m: <urllib3.connection.HTTPSConnection object at 0x000001F1151C9E10>: Failed to resolve 'proxy.service.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mProxyError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[31mProxyError\u001B[39m: ('Unable to connect to proxy', NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F1151C9E10>: Failed to resolve 'proxy.service.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mMaxRetryError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001B[39m, in \u001B[36mHTTPAdapter.send\u001B[39m\u001B[34m(self, request, stream, timeout, verify, cert, proxies)\u001B[39m\n\u001B[32m    643\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m644\u001B[39m     resp = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    645\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    646\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    647\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    648\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    651\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    652\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    653\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    654\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    655\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    656\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    658\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    839\u001B[39m     new_e = ProtocolError(\u001B[33m\"\u001B[39m\u001B[33mConnection aborted.\u001B[39m\u001B[33m\"\u001B[39m, new_e)\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m retries = \u001B[43mretries\u001B[49m\u001B[43m.\u001B[49m\u001B[43mincrement\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    842\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnew_e\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pool\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[43m=\u001B[49m\u001B[43msys\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    844\u001B[39m retries.sleep()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001B[39m, in \u001B[36mRetry.increment\u001B[39m\u001B[34m(self, method, url, response, error, _pool, _stacktrace)\u001B[39m\n\u001B[32m    518\u001B[39m     reason = error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause)\n\u001B[32m--> \u001B[39m\u001B[32m519\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, reason) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mreason\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    521\u001B[39m log.debug(\u001B[33m\"\u001B[39m\u001B[33mIncremented Retry for (url=\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m\"\u001B[39m, url, new_retry)\n",
      "\u001B[31mMaxRetryError\u001B[39m: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=parrots (Caused by ProxyError('Unable to connect to proxy', NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F1151C9E10>: Failed to resolve 'proxy.service.com' ([Errno 11001] getaddrinfo failed)\")))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mProxyError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      1\u001B[39m loader = WebBaseLoader(\n\u001B[32m      2\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mhttps://www.google.com/search?q=parrots\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      3\u001B[39m     proxies={\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m     \u001B[38;5;66;03m# 프록시 설정\u001B[39;00m\n\u001B[32m      9\u001B[39m )\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# 문서 로드\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m docs = \u001B[43mloader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001B[39m, in \u001B[36mBaseLoader.load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mlist\u001B[39m[Document]:\n\u001B[32m     31\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Load data into Document objects.\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.lazy_load())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:378\u001B[39m, in \u001B[36mWebBaseLoader.lazy_load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    376\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Lazy load text from the url(s) in web_path.\"\"\"\u001B[39;00m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.web_paths:\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m     soup = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_scrape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbs_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbs_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    379\u001B[39m     text = soup.get_text(**\u001B[38;5;28mself\u001B[39m.bs_get_text_kwargs)\n\u001B[32m    380\u001B[39m     metadata = _build_metadata(soup, path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:357\u001B[39m, in \u001B[36mWebBaseLoader._scrape\u001B[39m\u001B[34m(self, url, parser, bs_kwargs)\u001B[39m\n\u001B[32m    353\u001B[39m         parser = \u001B[38;5;28mself\u001B[39m.default_parser\n\u001B[32m    355\u001B[39m \u001B[38;5;28mself\u001B[39m._check_parser(parser)\n\u001B[32m--> \u001B[39m\u001B[32m357\u001B[39m html_doc = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequests_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raise_for_status:\n\u001B[32m    359\u001B[39m     html_doc.raise_for_status()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\requests\\sessions.py:602\u001B[39m, in \u001B[36mSession.get\u001B[39m\u001B[34m(self, url, **kwargs)\u001B[39m\n\u001B[32m    594\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[32m    595\u001B[39m \n\u001B[32m    596\u001B[39m \u001B[33;03m:param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[32m    597\u001B[39m \u001B[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001B[39;00m\n\u001B[32m    598\u001B[39m \u001B[33;03m:rtype: requests.Response\u001B[39;00m\n\u001B[32m    599\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    601\u001B[39m kwargs.setdefault(\u001B[33m\"\u001B[39m\u001B[33mallow_redirects\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m602\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mGET\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001B[39m, in \u001B[36mSession.request\u001B[39m\u001B[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[39m\n\u001B[32m    584\u001B[39m send_kwargs = {\n\u001B[32m    585\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m: timeout,\n\u001B[32m    586\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mallow_redirects\u001B[39m\u001B[33m\"\u001B[39m: allow_redirects,\n\u001B[32m    587\u001B[39m }\n\u001B[32m    588\u001B[39m send_kwargs.update(settings)\n\u001B[32m--> \u001B[39m\u001B[32m589\u001B[39m resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001B[39m, in \u001B[36mSession.send\u001B[39m\u001B[34m(self, request, **kwargs)\u001B[39m\n\u001B[32m    700\u001B[39m start = preferred_clock()\n\u001B[32m    702\u001B[39m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m703\u001B[39m r = \u001B[43madapter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[32m    706\u001B[39m elapsed = preferred_clock() - start\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\requests\\adapters.py:671\u001B[39m, in \u001B[36mHTTPAdapter.send\u001B[39m\u001B[34m(self, request, stream, timeout, verify, cert, proxies)\u001B[39m\n\u001B[32m    668\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m RetryError(e, request=request)\n\u001B[32m    670\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e.reason, _ProxyError):\n\u001B[32m--> \u001B[39m\u001B[32m671\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ProxyError(e, request=request)\n\u001B[32m    673\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e.reason, _SSLError):\n\u001B[32m    674\u001B[39m     \u001B[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001B[39;00m\n\u001B[32m    675\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m SSLError(e, request=request)\n",
      "\u001B[31mProxyError\u001B[39m: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=parrots (Caused by ProxyError('Unable to connect to proxy', NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F1151C9E10>: Failed to resolve 'proxy.service.com' ([Errno 11001] getaddrinfo failed)\")))"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 09. 텍스트(TextLoader)",
   "id": "6a44ac6958636885"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TXT Loader\n",
    "\n",
    ".txt 확장자를 가지는 파일을 로더로 로드하는 방법을 살펴보겠습니다."
   ],
   "id": "8e6dddd434b66664"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:21:38.251092Z",
     "start_time": "2025-12-13T11:21:38.245405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 텍스트 로더 생성\n",
    "loader = TextLoader(\"week5Data/appendix-keywords-CP949.txt\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\\n\")\n",
    "print(\"[메타데이터]\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n========= [앞부분] 미리보기 =========\\n\")\n",
    "print(docs[0].page_content[:500])"
   ],
   "id": "ca50ddbfcdc6c22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n",
      "\n",
      "[메타데이터]\n",
      "\n",
      "{'source': 'week5Data/appendix-keywords-CP949.txt'}\n",
      "\n",
      "========= [앞부분] 미리보기 =========\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TextLoader를 통한 파일 인코딩 자동 감지\n",
    "\n",
    "이 예제에서는 TextLoader 클래스를 사용하여 디렉토리에서 임의의 파일 목록을 대량으로 로드할 때 유용한 몇 가지 전략을 살펴보겠습니다.\n",
    "\n",
    "먼저 문제를 설명하기 위해 임의의 인코딩으로 여러 개의 텍스트를 로드해 보겠습니다.\n",
    "\n",
    "silent_errors: 디렉토리로더에 silent_errors 매개변수를 전달하여 로드할 수 없는 파일을 건너뛰고 로드 프로세스를 계속할 수 있습니다.\n",
    "\n",
    "autodetect_encoding: 또한 로더 클래스에 자동 감지_인코딩을 전달하여 실패하기 전에 파일 인코딩을 자동으로 감지하도록 요청할 수도 있습니다."
   ],
   "id": "c6e3fda219bd59dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:23:46.973799Z",
     "start_time": "2025-12-13T11:23:46.943821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "path = \"week5Data\"\n",
    "loader = DirectoryLoader(path)\n",
    "\n",
    "text_loader_kwargs = {\"autodetect_encoding\": True}\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path,\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    silent_errors=True,\n",
    "    loader_kwargs=text_loader_kwargs,\n",
    ")\n",
    "docs = loader.load()"
   ],
   "id": "946bd97194c5ed0b",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`week5Data/appendix-keywords.txt` 파일과 파일명이 유사한 파생 파일들은 모두 인코딩 방식이 다른 파일들입니다.",
   "id": "6305f62a365f838e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:23:49.641568Z",
     "start_time": "2025-12-13T11:23:49.635617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc_sources = [doc.metadata[\"source\"] for doc in docs]\n",
    "doc_sources"
   ],
   "id": "fa08e3d3d4dbe945",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week5Data\\\\appendix-keywords-CP949.txt',\n",
       " 'week5Data\\\\appendix-keywords-EUCKR.txt',\n",
       " 'week5Data\\\\appendix-keywords-utf8.txt',\n",
       " 'week5Data\\\\appendix-keywords.txt',\n",
       " 'week5Data\\\\chain-of-density.txt',\n",
       " 'week5Data\\\\reference.txt']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:24:07.394638Z",
     "start_time": "2025-12-13T11:24:07.388639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"[메타데이터]\\n\")\n",
    "print(docs[2].metadata)\n",
    "print(\"\\n========= [앞부분] 미리보기 =========\\n\")\n",
    "print(docs[2].page_content[:500])"
   ],
   "id": "c8cb3a86dbfc0485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[메타데이터]\n",
      "\n",
      "{'source': 'week5Data\\\\appendix-keywords-utf8.txt'}\n",
      "\n",
      "========= [앞부분] 미리보기 =========\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:24:21.837044Z",
     "start_time": "2025-12-13T11:24:21.833049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"[메타데이터]\\n\")\n",
    "print(docs[3].metadata)\n",
    "print(\"\\n========= [앞부분] 미리보기 =========\\n\")\n",
    "print(docs[3].page_content[:500])"
   ],
   "id": "cb7629849cc219db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[메타데이터]\n",
      "\n",
      "{'source': 'week5Data\\\\appendix-keywords.txt'}\n",
      "\n",
      "========= [앞부분] 미리보기 =========\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:24:29.866360Z",
     "start_time": "2025-12-13T11:24:29.861246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"[메타데이터]\\n\")\n",
    "print(docs[4].metadata)\n",
    "print(\"\\n========= [앞부분] 미리보기 =========\\n\")\n",
    "print(docs[4].page_content[:500])"
   ],
   "id": "5268f1ead62b7157",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[메타데이터]\n",
      "\n",
      "{'source': 'week5Data\\\\chain-of-density.txt'}\n",
      "\n",
      "========= [앞부분] 미리보기 =========\n",
      "\n",
      "Selecting the “right” amount of information to include in a summary is a difficult task. \n",
      "A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 10. JSON",
   "id": "f2f2c1bb400890ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "JSON\n",
    ".json 확장자를 가지는 파일을 로더로 로드하는 방법을 살펴보겠습니다.\n",
    "\n",
    "참고: https://python.langchain.com/docs/modules/data_connection/document_loaders/json\n",
    "사전 설치"
   ],
   "id": "634f5e18fd605ca0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:30:36.652270Z",
     "start_time": "2025-12-13T11:30:33.524860Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install jq",
   "id": "5ada2f62b3a1f45a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jq\n",
      "  Downloading jq-1.10.0-cp311-cp311-win_amd64.whl.metadata (7.3 kB)\n",
      "Downloading jq-1.10.0-cp311-cp311-win_amd64.whl (421 kB)\n",
      "Installing collected packages: jq\n",
      "Successfully installed jq-1.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:29:36.021150Z",
     "start_time": "2025-12-13T11:29:36.013367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "file_path = \"week5Data/people.json\"\n",
    "\n",
    "data = json.loads(Path(file_path).read_text(encoding=\"utf-8\"))\n",
    "pprint(data)"
   ],
   "id": "1a435b54091c149e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'address': {'city': '서울', 'street': '312번지', 'zipCode': '83795'},\n",
      "  'age': 31,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['요리', '음악 감상', '사진 촬영'],\n",
      "  'isMarried': True,\n",
      "  'name': '박시우',\n",
      "  'phoneNumbers': ['483-4639-1933', '947-4179-7976']},\n",
      " {'address': {'city': '서울', 'street': '877번지', 'zipCode': '36780'},\n",
      "  'age': 31,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['여행', '음악 감상', '등산'],\n",
      "  'isMarried': False,\n",
      "  'name': '정수아',\n",
      "  'phoneNumbers': ['337-5721-3227', '387-3768-9586']},\n",
      " {'address': {'city': '서울', 'street': '175번지', 'zipCode': '89067'},\n",
      "  'age': 43,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['등산', '독서', '게임'],\n",
      "  'isMarried': False,\n",
      "  'name': '최도윤',\n",
      "  'phoneNumbers': ['354-5563-4638', '471-9212-1826']},\n",
      " {'address': {'city': '서울', 'street': '690번지', 'zipCode': '70635'},\n",
      "  'age': 22,\n",
      "  'carOwnership': False,\n",
      "  'hobbies': ['여행', '등산', '게임'],\n",
      "  'isMarried': False,\n",
      "  'name': '정민준',\n",
      "  'phoneNumbers': ['468-2796-2152', '922-5760-7030']},\n",
      " {'address': {'city': '서울', 'street': '151번지', 'zipCode': '79118'},\n",
      "  'age': 79,\n",
      "  'carOwnership': False,\n",
      "  'hobbies': ['게임', '영화 감상', '음악 감상'],\n",
      "  'isMarried': True,\n",
      "  'name': '이민준',\n",
      "  'phoneNumbers': ['751-2823-8259', '722-7267-9516']},\n",
      " {'address': {'city': '서울', 'street': '855번지', 'zipCode': '21216'},\n",
      "  'age': 64,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['독서', '등산', '요리'],\n",
      "  'isMarried': False,\n",
      "  'name': '최도윤',\n",
      "  'phoneNumbers': ['462-4433-5968', '483-1709-4850']},\n",
      " {'address': {'city': '서울', 'street': '683번지', 'zipCode': '75013'},\n",
      "  'age': 72,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['여행', '사진 촬영', '독서'],\n",
      "  'isMarried': True,\n",
      "  'name': '최지훈',\n",
      "  'phoneNumbers': ['382-2779-3692', '835-4343-5346']},\n",
      " {'address': {'city': '서울', 'street': '547번지', 'zipCode': '33986'},\n",
      "  'age': 77,\n",
      "  'carOwnership': False,\n",
      "  'hobbies': ['독서', '등산', '음악 감상'],\n",
      "  'isMarried': True,\n",
      "  'name': '정도윤',\n",
      "  'phoneNumbers': ['136-2831-1021', '818-9721-7208']},\n",
      " {'address': {'city': '서울', 'street': '304번지', 'zipCode': '69380'},\n",
      "  'age': 44,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['독서', '게임', '여행'],\n",
      "  'isMarried': False,\n",
      "  'name': '정하린',\n",
      "  'phoneNumbers': ['423-5001-2734', '256-4271-3750']},\n",
      " {'address': {'city': '서울', 'street': '924번지', 'zipCode': '77191'},\n",
      "  'age': 26,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['등산', '독서', '여행'],\n",
      "  'isMarried': False,\n",
      "  'name': '김예준',\n",
      "  'phoneNumbers': ['668-1157-6180', '815-9997-6459']},\n",
      " {'address': {'city': '서울', 'street': '539번지', 'zipCode': '67491'},\n",
      "  'age': 48,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['요리', '게임', '등산'],\n",
      "  'isMarried': True,\n",
      "  'name': '최수아',\n",
      "  'phoneNumbers': ['745-5529-4411', '437-3892-3668']},\n",
      " {'address': {'city': '서울', 'street': '358번지', 'zipCode': '70195'},\n",
      "  'age': 41,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['여행', '영화 감상', '요리'],\n",
      "  'isMarried': False,\n",
      "  'name': '이서연',\n",
      "  'phoneNumbers': ['914-2071-3446', '539-6835-4629']},\n",
      " {'address': {'city': '서울', 'street': '741번지', 'zipCode': '82600'},\n",
      "  'age': 61,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['영화 감상', '독서', '여행'],\n",
      "  'isMarried': True,\n",
      "  'name': '이지훈',\n",
      "  'phoneNumbers': ['709-3578-3445', '907-3295-1822']},\n",
      " {'address': {'city': '서울', 'street': '932번지', 'zipCode': '57742'},\n",
      "  'age': 47,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['사진 촬영', '등산', '요리'],\n",
      "  'isMarried': False,\n",
      "  'name': '최서연',\n",
      "  'phoneNumbers': ['508-9125-7029', '939-1920-5084']},\n",
      " {'address': {'city': '서울', 'street': '603번지', 'zipCode': '30841'},\n",
      "  'age': 23,\n",
      "  'carOwnership': False,\n",
      "  'hobbies': ['등산', '독서', '여행'],\n",
      "  'isMarried': True,\n",
      "  'name': '이민준',\n",
      "  'phoneNumbers': ['891-2980-9497', '811-3249-9899']},\n",
      " {'address': {'city': '서울', 'street': '464번지', 'zipCode': '91295'},\n",
      "  'age': 52,\n",
      "  'carOwnership': False,\n",
      "  'hobbies': ['게임', '음악 감상', '요리'],\n",
      "  'isMarried': False,\n",
      "  'name': '박하은',\n",
      "  'phoneNumbers': ['499-4872-5904', '140-3733-7715']},\n",
      " {'address': {'city': '서울', 'street': '401번지', 'zipCode': '71129'},\n",
      "  'age': 61,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['음악 감상', '요리', '여행'],\n",
      "  'isMarried': False,\n",
      "  'name': '정수아',\n",
      "  'phoneNumbers': ['672-6315-8675', '975-1259-1656']},\n",
      " {'address': {'city': '서울', 'street': '356번지', 'zipCode': '40080'},\n",
      "  'age': 19,\n",
      "  'carOwnership': False,\n",
      "  'hobbies': ['등산', '영화 감상', '요리'],\n",
      "  'isMarried': False,\n",
      "  'name': '이지훈',\n",
      "  'phoneNumbers': ['853-1953-3723', '408-3476-1336']},\n",
      " {'address': {'city': '서울', 'street': '940번지', 'zipCode': '60335'},\n",
      "  'age': 52,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['요리', '사진 촬영', '등산'],\n",
      "  'isMarried': True,\n",
      "  'name': '최도윤',\n",
      "  'phoneNumbers': ['290-8270-9786', '483-1765-4028']},\n",
      " {'address': {'city': '서울', 'street': '289번지', 'zipCode': '59793'},\n",
      "  'age': 18,\n",
      "  'carOwnership': True,\n",
      "  'hobbies': ['여행', '요리', '영화 감상'],\n",
      "  'isMarried': True,\n",
      "  'name': '최주원',\n",
      "  'phoneNumbers': ['460-4533-7245', '344-2344-7362']}]\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:30:01.612548Z",
     "start_time": "2025-12-13T11:30:01.606434Z"
    }
   },
   "cell_type": "code",
   "source": "type(data[0])",
   "id": "1741ccf170943f25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## JSONLoader\n",
    "JSON 데이터의 메시지 키 내 content 필드 아래의 값을 추출하고 싶다고 가정하였을 때, 아래와 같이 JSONLoader를 통해 쉽게 수행할 수 있습니다."
   ],
   "id": "ba3948d00d25af20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:31:07.696152Z",
     "start_time": "2025-12-13T11:31:07.634597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# JSONLoader 생성\n",
    "loader = JSONLoader(\n",
    "    file_path=\"week5Data/people.json\",\n",
    "    jq_schema=\".[].phoneNumbers\",\n",
    "    text_content=False,\n",
    ")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 결과 출력\n",
    "pprint(docs)"
   ],
   "id": "7799cbb354a4b2e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 1}, page_content='[\"483-4639-1933\", \"947-4179-7976\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 2}, page_content='[\"337-5721-3227\", \"387-3768-9586\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 3}, page_content='[\"354-5563-4638\", \"471-9212-1826\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 4}, page_content='[\"468-2796-2152\", \"922-5760-7030\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 5}, page_content='[\"751-2823-8259\", \"722-7267-9516\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 6}, page_content='[\"462-4433-5968\", \"483-1709-4850\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 7}, page_content='[\"382-2779-3692\", \"835-4343-5346\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 8}, page_content='[\"136-2831-1021\", \"818-9721-7208\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 9}, page_content='[\"423-5001-2734\", \"256-4271-3750\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 10}, page_content='[\"668-1157-6180\", \"815-9997-6459\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 11}, page_content='[\"745-5529-4411\", \"437-3892-3668\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 12}, page_content='[\"914-2071-3446\", \"539-6835-4629\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 13}, page_content='[\"709-3578-3445\", \"907-3295-1822\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 14}, page_content='[\"508-9125-7029\", \"939-1920-5084\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 15}, page_content='[\"891-2980-9497\", \"811-3249-9899\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 16}, page_content='[\"499-4872-5904\", \"140-3733-7715\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 17}, page_content='[\"672-6315-8675\", \"975-1259-1656\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 18}, page_content='[\"853-1953-3723\", \"408-3476-1336\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 19}, page_content='[\"290-8270-9786\", \"483-1765-4028\"]'),\n",
      " Document(metadata={'source': 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\week5Data\\\\people.json', 'seq_num': 20}, page_content='[\"460-4533-7245\", \"344-2344-7362\"]')]\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 11. Arxiv",
   "id": "2d7dec21e6ceff87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Arxiv\n",
    "\n",
    "arXiv은 물리학, 수학, 컴퓨터 과학, 정량 생물학, 정량 금융, 통계, 전기공학 및 시스템 과학, 경제학 분야의 200만 편의 학술 논문을 위한 오픈 액세스 아카이브입니다. API 도큐먼트\n",
    "\n",
    "Arxiv 문서 로더에 접근하려면 arxiv, PyMuPDF 및 langchain-community 통합 패키지를 설치해야 합니다.\n",
    "\n",
    "PyMuPDF 는 arxiv.org 사이트에서 다운로드한 PDF 파일을 텍스트 형식으로 변환합니다."
   ],
   "id": "2ddfb9422fa84e8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:32:30.624860Z",
     "start_time": "2025-12-13T11:32:30.619381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 설치\n",
    "# !pip install -qU langchain-community arxiv pymupdf"
   ],
   "id": "fa268e0e5b1195b1",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 객체 생성\n",
    "\n",
    "이제 model 객체를 인스턴스화하고 문서를 로드할 수 있다:"
   ],
   "id": "96ce7cb4a24f9fb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:33:48.193341Z",
     "start_time": "2025-12-13T11:33:48.188858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Query 에 검색하고자 하는 논문의 주제를 입력합니다.\n",
    "loader = ArxivLoader(\n",
    "    query=\"Animal Behaviour\",\n",
    "    load_max_docs=2,  # 최대 문서 수\n",
    "    load_all_available_meta=True,  # 메타데이터 전체 로드 여부\n",
    ")"
   ],
   "id": "f6c0e36969272ea0",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:33:53.619053Z",
     "start_time": "2025-12-13T11:33:49.631564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 문서 로드 결과출력\n",
    "docs = loader.load()\n",
    "docs\n"
   ],
   "id": "63020874c6595610",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2008-02-12', 'Title': 'The STARFLAG handbook on collective animal behaviour: Part I, empirical methods', 'Authors': 'Andrea Cavagna, Irene Giardina, Alberto Orlandi, Giorgio Parisi, Andrea Procaccini, Massimiliano Viale, Vladimir Zdravkovic', 'Summary': 'The most startling examples of collective animal behaviour are provided by very large and cohesive groups moving in three dimensions. Paradigmatic examples are bird flocks, fish schools and insect swarms. However, because of the sheer technical difficulty of obtaining 3D data, empirical studies conducted to date have only considered loose groups of a few tens of animals. Moreover, these studies were very seldom conducted in the field. Recently the STARFLAG project achieved the 3D reconstruction of thousands of birds under field conditions, thus opening the way to a new generation of quantitative studies of collective animal behaviour. Here, we review the main technical problems in 3D data collection of large animal groups and we outline some of the methodological solutions adopted by the STARFLAG project. In particular, we explain how to solve the stereoscopic correspondence - or matching - problem, which was the major bottleneck of all 3D studies in the past.', 'entry_id': 'http://arxiv.org/abs/0802.1668v1', 'published_first_time': '2008-02-12', 'comment': 'To be published in Animal Behaviour', 'journal_ref': 'Animal Behaviour 76 (1), 217-236 (2008)', 'doi': None, 'primary_category': 'q-bio.QM', 'categories': ['q-bio.QM', 'cond-mat.stat-mech', 'q-bio.PE'], 'links': ['https://arxiv.org/abs/0802.1668v1', 'https://arxiv.org/pdf/0802.1668v1']}, page_content=\"The STARFLAG handbook on collective animal behaviour:\\nPart I, empirical methods\\nANDREA CAVAGNA1,2, IRENE GIARDINA1,2, ALBERTO ORLANDI1, GIORGIO\\nPARISI1,3, ANDREA PROCACCINI1,3, MASSIMILIANO VIALE3, VLADIMIR\\nZDRAVKOVIC1\\n1 Centre for Statistical Mechanics and Complexity (SMC), CNR-INFM\\n2 Istituto dei Sistemi Complessi (ISC), CNR\\n 3 Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’\\nCorrespondence to Andrea Cavagna:\\nSMC, CNR-INFM, Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’, Piazzale\\nAldo Moro 2, 00185 Roma, Italy\\nandrea.cavagna@roma1.infn.it\\nIrene Giardina, Alberto Orlandi, and Vladimir Zdravkovic:\\nSMC, CNR-INFM, Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’, Piazzale\\nAldo Moro 2, 00185 Roma, Italy\\nGiorgio Parisi, Andrea Procaccini and Massimiliano Viale:\\nDipartimento di Fisica, Universita’ di Roma ‘La Sapienza’, Piazzale Aldo Moro 2,\\n00185 Roma, Italy\\nMassimiliano Viale (current address):\\nDipartimento di Fisica, Universita’ di Roma 3, via della Vasca Navale 84, 00146 Roma,\\nItaly\\n2\\nAbstract\\nThe most startling examples of collective animal behaviour are provided by very large\\nand cohesive groups moving in three dimensions. Paradigmatic examples are bird flocks,\\nfish schools and insect swarms. However, because of the sheer technical difficulty of\\nobtaining 3D data, empirical studies conducted to date have only considered loose\\ngroups of a few tens of animals. Moreover, these studies were very seldom conducted in\\nthe field. Recently the STARFLAG project achieved the 3D reconstruction of individual\\nstarlings’ positions in cohesive flocks consisting of thousands of birds under field\\nconditions, thus opening the way to a new generation of quantitative studies of collective\\nanimal behaviour. Here, we review the main technical problems in 3D data collection of\\nlarge animal groups and we outline some of the methodological solutions adopted by the\\nSTARFLAG project. In particular, we explain how to solve the stereoscopic\\ncorrespondence - or matching - problem, which was the major bottleneck of all 3D\\nstudies in the past.\\nKeywords: collective animal behaviour; self-organization; bird flocking; fish schooling;\\nswarms; stereo photography.\\n3\\nThe first speculations about collective animal behaviour date back to the\\nobservations of Pliny about flocks of starlings (translated by Rackham 1933). His\\nremarks were necessarily very qualitative, although very reasonable too. Most of the\\nhypotheses formulated almost two thousands years later were based on equally\\nqualitative observations (Selous 1931; Emlen 1952). At times, the study of collective\\nanimal behaviour has been complemented by analogies with instances of collective\\nbehaviour in other fields of science, prominently physics (Radokov 1973). However, in\\nthe absence of any quantitative empirical insight, attempts to address the fundamental\\nissues of collective behaviour rapidly became a matter of pure speculation.\\nIt was only in the mid 1960s that the first empirical studies of collective animal\\nbehaviour in three dimensions led to some quantitative results. Cullen and co-workers\\nstudied small groups of up to 10 fish in a tank using the shadow method for three-\\ndimensional (3D) reconstruction (Cullen et al. 1965). The same technique was later\\nemployed to study groups of up to 30 fish (Pitcher & Partridge 1979; Partridge et al.\\n1980; Partridge 1980). For the first time it was possible to measure interesting quantities,\\nlike the average nearest neighbour distance among individuals, and their angular\\ndistribution. Several other studies of fish groups have been performed since (see, for\\nexample, Van Long et al. 1985), although always with very small groups.\\nEmpirical 3D data for birds have been harder to come by, because working in the\\nfield imposes serious constraints on accuracy. The first studies (Sugg 1965; van Tets\\n1966) focused on two-dimensional estimates of flocks’ densities using a single\\nphotograph technique. It was only in 1978 that Major and Dill, using stereometry, could\\nreconstruct the 3D positions of individual birds in small flocks of dunlins and starlings\\n(on average 55 individuals per flock) (Major & Dill 1978). Subsequently, Pomeroy and\\nHeppner (1992) managed to reconstruct individual birds’ trajectories in flocks of up to\\n11 pigeons, using the orthogonal method, and the dynamics of turns was unveiled for the\\nfirst time. Ikawa et al. (1994) studied swarms of up to 20 mosquitoes in the field, and\\nrecorded the nearest neighbour distance.\\nThe time span over which these empirical investigations occurred, and the small\\nnumbers of animals considered, illustrate the difficulty of obtaining high-quality 3D\\ndata. Even the most recent studies, such as that by Budgey (1998) could only reconstruct\\nthe positions of a maximum of 61 lapwings, basically using the same technology\\nemployed by Major and Dill (1978) twenty years before. Compared to the advances in\\nmost experimental fields of science, the situation in 3D studies of collective animal\\nbehaviour is somewhat disappointing. The number of animals in all these studies is very\\nlow compared to natural conditions, where groups can range up to thousands, or even\\nhundreds of thousands of individuals. As we note in our companion paper (Cavagna et\\nal. 2008), analysing small groups has some serious drawbacks, related to the\\npreponderance of border effects. Moreover, compactness of the groups in all these\\nanalysed cases has been quite poor, due to the technical difficulties of dealing with\\npacked groups of animals.\\nNotwithstanding these criticisms, it must be noted that the first generation of\\nempirical studies (1970-2000) was essential to establish the field of collective animal\\nbehaviour on a firm basis and their value should not be underestimated. They gave a first\\nrealistic perception of the structure of animal groups and their dynamic properties.\\nMoreover, the experimental techniques were sound. Although there was a need for them\\n4\\nbe refined and developed, in order to produce more substantial results, the evidence\\nsuggests that the direction taken by these studies was the right one.\\nHowever, in the last twenty years the main focus of the community has shifted to\\na different target, leading to a drastic reduction in the number of empirical investigations\\nand of 3D field studies in particular. With the advent of large computers, the field of\\ncollective behaviour has been dominated by numerical models (Sakai 1973; Aoki 1982;\\nReynolds 1987; Heppner & Grenander 1990; Huth & Wissel 1992; Vicsek et al. 1995;\\nGregoire & Chate 2004). Of course, models are extremely useful. The fundamental\\ningredients of a model can be kept under complete control, so that it is possible to\\nunderstand clearly what is the connection between a certain phenomenon (the model’s\\noutput) and the biological ingredients causing it (the model’s input). It is mainly thanks\\nto models that we now understand how collective behaviour may emerge as the\\nconsequence of self-organization, without the need for centralized co-ordination of\\nindividuals.\\nWhile models can complement empirical investigation, they cannot replace it.\\nApart from some very general issues that can be assessed on the basis of purely\\nqualitative observations (as, for example, the emergence or not of collective behaviour),\\na model’s outcome should always be compared to empirical data. Without this essential\\n‘ground-truthing’, models can proliferate in an uncontrolled fashion and, as a by-\\nproduct, so does the number of different theoretical frameworks. Selecting the right\\nmodel, and deciding the correct hypothesis, becomes merely a matter of taste, or\\nguesswork, in absence of real data.\\nAs feared by Davis (1980), we believe that, within the field of collective animal\\nbehaviour, speculation has outgrown empirical groundwork. As we show in this paper,\\nnew tools from statistical physics and computer vision can now help solving the major\\noutstanding problems of 3D reconstruction, and so allow a much needed return to\\nempirical data collection. Using such tools, we managed to reconstruct, for the very first\\ntime, the 3D positions of several thousand individual birds in cohesive flocks under\\nnatural conditions (Ballerini et al. 2008a, b). Compared to previous empirical studies,\\nwhich considered few tens of loosely organized animals, this is an advance of two orders\\nof magnitude in the empirical study of collective animal behaviour. This result has been\\nachieved in the context of STARFLAG (Starlings in flight: understanding the patterns\\nof animal group movement), a project financed by the European Commission in the\\ncontext of the 6th framework programme. Within STARFLAG, biologists, physicists, and\\ncomputer scientists joined forces to study collective animal behaviour, both with an\\nempirical and with a theoretical approach. We report here the main methods developed\\nby STARFLAG to obtain empirical 3D data of animal groups.\\nThe key to achieving the STARFLAG results was the solution of the\\ncorrespondence, or matching, problem. This has been the most serious bottleneck of all\\nformer empirical studies. When using any 3D technique, one must place into\\ncorrespondence different images of the same animal. For example, in stereometry, one\\nhas two images of the group taken from two different points of view. In order to perform\\na 3D reconstruction, one must take a given animal’s image on one photograph, and\\nidentify the corresponding image on the other photograph (Osborn 1997). The matching\\nproblem is particularly severe when there are many similar animals positioned very close\\nto each other, which, unfortunately, is typical in natural flocks. Until now, no computer\\n5\\nalgorithm has been able to do this automatically, and thus the matching was performed\\nby hand. Clearly, this severely limited the number of animals and the density of the\\ngroups that could be studied. STARFLAG solved this problem by using a blend of\\nstatistical physics, computer vision and mathematics.\\n Some of the techniques used by STARFLAG may be unfamiliar to scientists\\nworking in the field of animal behaviour. In this paper, we explain all these techniques\\nand provide the necessary tools to reproduce our results, illustrating our points using the\\n3D empirical study of STARFLAG. The paper is divided into two main parts. First, we\\nwill explain how to set up an apparatus for the 3D reconstruction of large, cohesive and\\npotentially distant animal groups. Our method is stereometry, so we will also give a very\\nbrief reminder of this technique. The second part of the paper explains how to transform\\na set of stereoscopic images into a list of individual 3D coordinates of the animals within\\nthe group. Here, our solution to the correspondence problem plays a major role. This part\\nof the paper is, therefore, the most technical, as it was heavily inspired by statistical\\nphysics, but it is also the most innovative part of the entire work. Details of how to\\nanalyse 3D data sets are given in our companion paper (Cavagna et al. 2008) where the\\ntechniques described can be applied to the results of numerical models, in addition to\\nempirical data.\\nSETTING UP THE APPARATUS\\nWe begin this section with a discussion of digital imaging; in particular, the problems\\nthat arise when working with commercial, non-metric cameras. We then introduce some\\nbasic notions of stereometry, and show how a careful analysis of the required\\nexperimental accuracy fixes most of the technical parameters of the apparatus, such as\\nthe distance between the cameras and the necessary alignment. Finally, we deal with the\\nsynchronization problems of standard commercial cameras, and how these can affect\\nmeasurement accuracy.\\nThe 3D reconstruction of animal groups can be performed at two levels the\\nglobal and the individual. At the global level, one describes the group as a whole,\\ndisregarding the 3D position of the individuals. For example, with sonar techniques, it is\\npossible to study a school of sardines giving its overall position, shape, size, and even\\nthe 3D map of its inner density, without knowing the position of individual fish. At the\\nindividual level, in contrast, the 3D positions of all (or as many as possible) animals in\\nthe group are calculated. Given a set of individual 3D positions, it is possible to\\nreconstruct all global quantities, whereas global descriptions obviously miss the\\ninformation related to individual behaviour. A global description is thus unable to reveal\\nhow collective behaviour emerges from individual interactions, perhaps the most\\nfascinating question in the field. For this reason, 3D reconstruction at the individual level\\nis more valuable than a global level analysis, even though it is technically more difficult.\\nIn the present work, we deal only with individual 3D reconstruction.\\n6\\nCameras\\nIn order to perform 3D reconstruction at the individual level, optical techniques are by\\nfar the most precise, compared to radar or sonar. Unless one has the resources to build\\none’s own optical equipment, tailored to the specific needs of the study, most researchers\\nwill have to rely on commercial digital still or video cameras. Cameras are normally the\\nmost expensive piece of equipment in the apparatus, so that the entire budget for a study\\ncan be gauged on their cost. It is therefore seems wise to start this handbook with a brief\\ndiscussion of camera-related problem (Adams 1983).\\nStill Cameras vs. Video Cameras: a Matter of Resolution\\nIf one is only interested in studying the static structure of a group then, of course,\\nmultiple time images are not needed. On the other hand, if one wishes to  dynamically\\ntrack individual animals, the very first question to ask is: still cameras or video cameras?\\nTypically, still cameras have much higher digital (i.e. pixel) resolution, but rather low\\nrefresh rate, i.e. a small number of frames-per-second (fps), while video cameras have\\nhigh refresh rate, but lower digital resolution. It is therefore important to consider the\\ntrade off between these two factors. We start by discussing digital resolution.\\nThere are two very important requirements to meet in this respect: first, we need\\nto resolve different nearby animals on the image; second, the whole group must be\\nentirely contained within the field of view of the camera, i.e. the image must be entirely\\ncontained in the camera’s charge-coupled device (CCD). This second requirement\\nfollows from the fact that we operate with immobile cameras, which do not track the\\ngroup (alignment issues become severe with moving cameras). We call E the width in\\nmeters of the group, e its width in pixels on the CCD, R0 the minimal inter-individual\\ndistance in meters, and r0 the same distance in pixels on the CCD. In order to satisfy the\\nfirst condition, it is reasonable to require that different animals are separated, on average,\\nby at least 4 pixels, and therefore to set r0 > 4. For the second condition, we require that\\nthe optical dimension of the group must not exceed, on average, 1/3 of the length of the\\nCCD, otherwise the group is not going to remain in the field of view for very long. We\\ntherefore require that e < n/3, where n is the total number of pixels of the CCD along its\\nshortest direction. These two basic requirements therefore imply:\\nr0\\ne\\n> 12\\nn      .    (1)\\nSimple proportions fix the ratio between a length L in real space and its projection l onto\\nthe CCD (Fig.1). We have:\\nl\\nL =\\nf\\nz + f       ,   (2)\\nwhere f is the focal length and z is the distance of the target, both measured in meters.\\nWe note that typically f is much smaller than z (for example, 35mm vs.100m), so that\\nwithout loss of generality we can write:\\nl\\nL = f\\nz       .      (3)\\n7\\nThis equation implies that the ratio between projected length and real space length is the\\nsame for all targets at the same distance z, and therefore:\\ne\\nE = r0\\nR0\\n    ,     (4)\\nPutting together relations (1) and (4), we obtain the minimum amount of linear pixels n\\nneeded to meet both our requirements:\\nn > 12 E\\nR0\\n       .   (5)\\nA typical example from the STARFLAG project is a flock of starlings with a width E =\\n40m and an inter-individual distance R0 = 0.2m. In this case we need about\\n12x40/0.2=2400 pixels in the shortest frame direction, which means a total of\\n3600x2400=8.6 Mega pixels (Mp) in a standard 3:2 image ratio. Note that this example is\\nquite typical of many kinds of large and cohesive animal groups: the ratio E/R0 is an\\nindication of the linear number of animals in the group, so it cannot be very small for\\ngroups with a large number of animals.\\nAt the time of writing, standard commercial video cameras are far from this level\\nof digital resolution: even high definition (HD) video cameras only have around 1.0Mp.\\nIndeed, very few products at the moment have significantly higher resolution, and they\\nare very expensive. Still digital cameras, on the other hand, have excellent pixel\\nresolution, at the price of slower frame rate, typically around 5fps (higher frame rates\\ncause desynch problems –see below). This is too small for tracking most kind of animals\\nin 3D. However, we will describe later how the refresh rate of the apparatus can be\\nimproved by doubling (or more) the number of cameras. The final message is that, in\\norder to study cohesive and large groups, high-resolution digital cameras are probably\\nthe best choice at the moment.\\nGroup Distance and Focal Length\\nRelation (5) between number of pixels n, width E and inter-individual distance R0\\ndoes not depend on the distance z of the group. When this distance increases, the\\nmagnification decreases, but in principle one can compensate for this by choosing a lens\\nwith a longer focal length, thus keeping magnification constant. In fact, the relation\\nbetween focal length f and minimal inter-individual distance R0 can be derived from\\nequation (3),\\nr0\\nR0\\n= f\\nz      .       (6)\\nWe have to be careful here, however: if we measure the inter-individual distance r0 on\\nthe CCD in pixels, we must do the same for the focal length. We call W the focal length\\nin pixels, such that f = W p, where p is the linear length of a pixel. We can now write:\\nr0\\nR0\\n= W\\nz      ,     (7)\\nwhere now both r0 and W are measured in units of pixels. If we require, as we did above,\\nthat the inter-individual distance on the CCD must be at least 4 pixels, r0 > 4, we get:\\nW > 4z\\nR0\\n     .   (8)\\n8\\nFor example, for a group at z=150m, with an inter-individual distance R0=0.2m, equation\\n(8) says that we need a lens with focal length W > 3000 pixels. Considering that the\\nlinear size of a pixel in standard digital cameras is p~8.2x10-3mm, we get a minimal\\nfocal length f =24mm.  If the distance z is now doubled to 300m, we have to double the\\nfocal length f in order to keep the image’s magnification the same. If we do not do this,\\nthe image becomes too small, and the inter-individual distance r0 on the CCD falls below\\nthe desired number of pixels (~4). In a typical STARFLAG photograph, the linear size of\\na bird at about 100m is between 5 and 15 pixels.\\nTherefore, from equation (8), one can conclude that, in order to consider groups\\nat longer distances, one can, in principle, simply use lenses with longer focal lengths. In\\npractice, this is not the case. The maximum distance, z, is sharply limited by the fact that\\nthe optical resolution of a system decreases very steeply with distance from the target.\\nThis is mainly due to the diffraction from atmospheric mist, especially in urban areas,\\nwhich increases drastically with distance from the target. On the other hand, using very\\nshort focal lengths also has its shortcomings, because these lenses normally have\\nsubstantial optical distortion and a larger number of internal elements. It is advisable,\\ntherefore, to use ‘normal’ lenses (i.e. lenses whose focal length is comparable to the\\nCCD size), since they offer a good balance in terms of resolution, distance and\\ndimension of the groups.\\nWithin STARFLAG, we used Canon EOS 1D Mark II cameras, whose CCD size\\nis 28x19mm, corresponding to 3504x2336=8.2Mp. We mounted Canon lenses with focal\\nlength f=35mm lenses (maximum aperture f2.0), corresponding to W =4273. This\\nallowed us to have to have a very good magnification, at the cost of a slightly smaller\\nfield of view.\\nCamera Calibration\\nThe most serious drawback in working with commercial (albeit high end) cameras is that\\nthey are not precision instruments. This means that they are not conceived to produce a\\nquantitative measure, but rather a qualitative representation of reality. As a result, the\\nnominal specifications of each camera may differ substantially from the real\\nspecifications. If not properly accounted for, this discrepancy can have an enormous\\nimpact on accuracy of measurements.\\nThe most basic model of a camera is the pinhole model (Hartley & Zisserman\\n2003). This model is specified by the focal length W and by the x-y position u and v of\\nthe optical axis with respect to the CCD centre (all these quantities are expressed in units\\nof pixels). Knowing these values is crucial, since they are an input of the 3D\\nreconstruction formulas. Therefore, they have to be very precisely determined.\\nMoreover, all lenses are affected by radial distortion (Forsyth & Ponce 2002), which is\\nparticularly large close to the borders of the field of view. Such distortion may cause a\\nshift of an image point of up 20-30 pixels (see Figure 2), with catastrophic effects on the\\nfinal accuracy of the 3D apparatus.\\nThe only way to transform a commercial camera into a precision instrument is to\\nhave each camera professionally calibrated. In this way, the focal length W, and the\\ncoordinates u and v of the optical axis are carefully measured (this should be done at a\\ntemperature close to the final working temperature). For example, the nominal value of\\n9\\nthe focal length of the lenses we used within STARFLAG was 35mm, equivalent to\\nW=4273 pixels. The average calibrated value of the focal lenght, on the other hand, was\\nW=4335 pixels (different STARFLAG cameras had a calibrated focal length that differed\\none from another of up to 50 pixels).\\nProfessional calibration also includes the measurement of the four parameters of\\nradial and tangential distortion within a quadratic expansion (Forsyth & Ponce 2002;\\nHartley & Zisserman 2003), such that each image can be at least partially corrected for\\ndistortion. Note that it is quite difficult to perform camera calibration in an unspecialized\\nlaboratory. In particular, we had unsatisfying results with ready-to-use calibration\\nroutines that rely on 2D targets grids.\\nCamera calibration has enormous importance. At the same time, professional\\ncamera calibration is not a panacea: some radial distortion may persist even after this\\nprocedure. Tests in reconstructing the known 3D positions of targets, like those we\\ndescribe later, must always be performed to check the accuracy of the apparatus. Finally,\\nwe note that camera calibration must be performed at least annually.\\nFocus, Contrast and Exposure\\nSharpness is a key issue, especially in dense and cohesive groups. For this reason, focus\\nmust be carefully controlled. Cameras must be switched to manual focus, since auto\\nfocus is slow and inaccurate. Given that, typically, animals are quite distant from the\\ncameras, the most common error is to set the camera’s focusing ring to infinity. In\\nprinciple, this is the correct choice: for example, with a focal length of 35mm, targets\\nbeyond 50m are, for all practical purposes, at infinity. However, when the focusing ring\\nis set to the default infinity mark, a standard camera is in fact focusing beyond infinity,\\nand photos will be out of focus significantly (see Fig.3). The reason why camera\\nmanufacturers do this is to compensate for the fact that, at different temperatures, the\\nmaterials in the lens expand and contract, which changes the infinity focus-point.\\nAllowing the lens to focus to what, in average situations, is beyond infinity will, under\\nextreme temperatures, just allows the lens to focus at infinity. Therefore, the focus of\\neach camera used must be calibrated manually before field observations begin. This is\\nachieved in the following way: by using a standard laser meter, one can locate targets at\\nabout the same distance as the groups of interest, and take several test photos at various\\npositions of the focusing ring. One can then find the optimal focus position, and mark on\\nthis on the focusing ring such position. An important remark is in order: when the\\ncamera gets calibrated it is important to specify what is the distance at which it will be\\nfocused, since the focal length is the distance between focal plane and rear nodal plane.\\nSharpness is also significantly affected by aperture, shutter speed and sensitivity\\n(ISO). First of all, aperture and shutter speed must be set manually. This is essential for\\nthe camera to save time, and thus have a faster shooting rate and a more accurate\\nsynchronization with other cameras (see later). Ideally, in good light conditions,\\nmaximum sharpness is obtained with low ISO (which implies low noise), slow shutter\\nspeed, and the aperture set in the mid-range (to increase depth of field). There are,\\nhowever, several constraints, mainly set by the need of a decent exposure and, in\\nparticular, by the need to obtain a reasonable contrast between animals and the\\nbackground. As we shall see later, in order to automatically recognize animals, one uses\\n10\\na luminosity threshold, in order to separate them from the background. For this to be\\neffective, it is important to maximize the difference in luminosity between animals and\\nbackground. The best way to achieve this is to have a well-balanced exposure, meaning\\nthat the average level of the image’s exposure should be in the mid-range. In strict\\nphotographic terms, this is equivalent to saying that, when using a spot light meter, the\\nbackground (for example, the sky) should be placed as close as possible to zone V, i.e.,\\nto 18% grey (Adams 1983). This prevents the contrast-flattening effect of underexposed\\nand overexposed images. Achieving perfect exposure can be tricky, especially if photos\\nhave to be taken at dusk (as with starling flocks). In this case, one must find a reasonable\\ntrade off between ISO and shutter speed, to optimize sharpness and contrast.\\nDetermination of optimum aperture is tricky: a small aperture increases depth of\\nfield, and thus enhance sharpness, but even just a few stops below full aperture can\\ndecrease drastically the camera’s refresh rate, and thus compromise camera\\nsynchronization. This is due to an associated increase in release time lag (see below). In\\nthe STARFLAG project, we found that a maximum of two stops below full aperture\\ngranted a reasonable depth of field, together with accurate synchronization (full aperture\\nof our lens was f2.0, so we used down to f4.0). In some cases (i.e., dusk or dawn), light\\nconditions change rapidly, so that exposure settings must be changed during the course\\nof observations.\\nElements of Stereometry\\nThe best technique to reconstruct the 3D position of distant animals in the field is\\nprobably stereometry (Longuet-Higgins 1981). Compared to other techniques, most\\nprominently the orthogonal method, stereometry is more flexible, in that the group’s\\ndistance is limited only by optical resolution, and not by being enclosed within the\\napparatus. Stereometry was the technique used by the STARFLAG project, and we\\nprovide a brief review of its most basic elements.\\nThe fundamental idea of stereoscopy is that, by taking images of the same object\\nfrom two different positions in space, we have enough information to reconstruct its 3D\\ncoordinates, provided that we know the mutual position of the two cameras. The\\nmathematics behind this is simple: the two images consist of a series of two pairs of 2D\\ncoordinates, and these four values are sufficient to work out the three independent 3D\\ncoordinates. Even though the principle is simple, the actual equations are quite\\ncomplicated, and it is essential to refer to the most recent technical literature (Hartley &\\nZisserman 2003) in order not to make gross mistakes. Here, we sketch the main\\nequations in their simplest form. This is sufficient to conduct an error analysis, and thus\\ndetermine the key parameters of the apparatus.\\nThe Minimal Baseline\\nIn Fig.4 we depict the simplest stereoscopic setup: two parallel cameras are\\nseparated by a distance d in the x direction. This separation is the baseline, and the first\\nexperimental question we have to answer is: how large must d be in order to achieve\\nsufficient sensitivity in the apparatus? Clearly, the larger the baseline, the higher the\\n11\\nsensitivity (indeed, with d=0 there would be no sensitivity at all!). To quantify this, we\\nwrite the fundamental equation of stereoscopy:\\ns = uL - uR = Wd\\nz      .      (9)\\nIn this expression s is the stereoscopic shift, or disparity, namely the separation in pixels\\nbetween the two images. This is the difference between the abscissa of the image in the\\nleft photo, uL, and that in the right photo, uR. W is the focal length expressed in units of\\npixels, d is the baseline in meters, and z is the distance of the object in meters. We stress\\nthat this equation only holds when the optic axes of the two cameras are perfectly\\nparallel, when the separation d is purely along the x direction, when the calibration\\nparameters are exactly the same for the two cameras, and when there is no radial\\ndistortion. In this case, the two images are shifted purely in the x direction, while they\\nhave the same photographic ordinate y, which goes through the plane in Fig.4. \\nImagine that an animal initially at distance z1 moves to a distance z2= z1  + dz. This\\nwill cause a change in disparity, ds. The relation between ds and dz can be computed by\\ndifferentiating equation (9):\\ndz = z2\\nWd ds        .      (10)\\nIn order for our apparatus to be sensitive to this shift, it is essential that the variation, ds,\\nof the disparity is larger than one pixel; otherwise the space separation, dz, will be\\nundetectable. We can also invert the argument: it is not possible to locate the position of\\nan animal on the image with a resolution better than one pixel, so that there is an\\nunavoidable error, dz. Therefore, the minimal baseline necessary for resolving the shift\\ndz is obtained by setting ds >1:\\nd > z2\\nWdz      .     (11)\\nThis is a very instructive equation, and one of the most important in stereo analysis: the\\nminimal baseline grows very fast with the distance of the target, and, as expected, it is\\nlarger the lower dz, i.e. the higher the required spatial sensitivity. For example, for a\\nstandard 35mm lens, we have W = 4273. If we need to resolve birds at a distance\\nz=150m, with a sensitivity dz=0.5m, we get d >10m. Note that 0.5m is by no means an\\nexaggerated sensitivity, considering that birds may get as close to each other as 0.3-\\n0.2m. If we want to reconstruct a group 200m away with the same accuracy, we would\\nthen need d > 20m. Most previous stereo studies have tried to reconstruct animals\\n(typically birds) at similar distances from the apparatus, but with baselines in the 2m-5m\\nrange: inverting equation (11) shows that this amounts to a resolution dz between 2.3m\\nand 4.6m; hardly satisfying if we want to be able to say something accurate about the\\nrelative distances among individuals. Within the STARFLAG project, we used a\\nbaseline of d=25m, sufficient to have a decent accuracy for starling flocks at a distance\\nof up to 200m.\\nThe Need for Alignment\\nAs we have seen, the error in the relative distance between animals is dominated\\nby the pixel resolution, ds, which cannot be smaller than one pixel. There is, however, a\\n12\\nsecond important source of experimental error, which comes from the misalignment of\\nthe apparatus. If the two cameras are non-parallel, and have a convergence (divergence)\\nangle a between them, equation (9) is modified:\\ns = Wd\\nz - Wa      ,    (12)\\n(this equation is only valid for small values of a). An error, da, in the determination of\\nthe angle, produces an error, Dz,  in the absolute determination of the distance, z:\\nDz = z2\\nd da            .    (13)\\nThis equation tells us that the smaller the baseline, d, the smaller the alignment error,\\nda, must be in order to keep the error, Dz, as small as possible. The situation looks quite\\ngrim: even with a 20m baseline, in order to make an error, Dz, smaller than 1m in the\\ndetermination of targets at z=100m (an accuracy of 1%), one would need to align the\\ncameras with an accuracy better than 0.002rad (or 0.11°). The same 1% accuracy on\\ntargets 300m distant would require an alignment better than 0.0006rad!\\nAny misalignment in the cameras affects absolute distances more than relative\\nones, meaning that the positions of two animals close to each other in space will be\\nshifted by a similar amount, with little effect on their inter-individual distance. However,\\nthis is small consolation: inter-individual distances may not change substantially, but the\\nshape of the group as whole will be altered considerably, with serious consequences for\\nthe calculation of all group morphological properties.\\nOf course, the situation improves dramatically if the average distance of the\\ngroup from the apparatus is significantly lower than those considered so far in our\\ndiscussion. This is typically the case in a laboratory. For example, in order to study a 2m\\nlong school of fish in a tank at a 5m distance from the apparatus, with the same 35mm\\nfocal length lens, and a relative resolution of 0.05m, a baseline of 1m is more than\\nadequate. Even here, however, an alignment better than 0.002rad is needed, although this\\nis much easier to achieve with two cameras mounted on the same support just 1m apart.\\nWhen recording measurements in the field, however, it is often very difficult to get close\\nto animal groups without perturbing them, and it is practically impossible with birds.\\nA long baseline (>20m) is necessary, therefore, for the sake of accuracy, because\\nboth the error on relative distances and the error on absolute distances are inversely\\nproportional to the baseline. A long baseline is also a big inconvenience, however, for\\ntwo good reasons: first, it is impossible to mount the two stereo cameras on the same\\nsupport, which makes it very difficult to align them to the desired accuracy (<0.002rad),\\nespecially in the field, where many lab facilities are absent. Second, a long baseline\\nimplies a large stereometric disparity, making it very difficult to match different animals\\nin the two photos. The latter problem is the hardest one, and we will deal with it in the\\nsecond part of the paper. For now, we concentrate on camera alignment.\\nAccurate Alignment in the Field\\nIn the STARFLAG project, we set the baseline to 25m, and we operated on the roof of a\\npublic building, where we were not allowed to leave any piece of equipment. This meant\\nthat we had to mount and dismount the whole apparatus on every observation session.\\nThese constraints forced us to discard any form of optical alignment, such as lasers.\\n13\\nLasers are very helpful in the lab, where they can be carefully calibrated on a single\\noccasion along with the rest of the apparatus, but much less useful under the\\nuncontrolled conditions typical of animal observations in the field.\\nWhen trying to develop an effective alignment method, we also had to consider\\nthe following fact: working with parallel cameras is very inconvenient, because the\\nmaximum overlap of the two fields of view corresponds to an infinite distance of the\\ntargets. If the average distance of the group is z, it is optimal to have the two cameras\\nslightly convergent at an angle a=d/z. In the STARFLAG setup, we used a convergence\\nangle of a=0.22rad (12.6 degrees), giving optimal overlap of the fields of views at about\\n100m, the average distance of the flocks from the apparatus. Precisely fixing a non-zero\\nangle between two distant cameras is, however, a non-trivial task, and is much more\\ndifficult than aligning cameras in a parallel fashion.\\nWithin STARFLAG, we developed a basic, but effective method (Fig.5).  Each\\ncamera was mounted on a rigid bar, made of an aluminium alloy that deformed as little\\nas possible under temperature changes. The bar was of length L and the camera was\\naligned to the back of the bar. The bar was then mounted on a sturdy professional tripod,\\nwith a micrometric head (we used Manfrotto 475 tripods and Manfrotto 400 heads). A\\nthin line, as inextensible as possible, connected the external sides of the two bars to each\\nother. The line was under strong tension, so that it was very rectilinear (this can be tested\\nwith a laser). We used a fishing line with a diameter a=0.25mm. On the internal sides of\\nthe two bars there were two fine gauges, over which the line passed. Once the line was\\ntense and rectilinear, it was possible to adjust the micrometric head until the line passed\\nexactly over the desired reading, H, on the two gauges (Fig.5). This fixed the mutual\\nconvergence angle between the two cameras to the value:\\na = 2H\\nL     .    (14)\\nThe alignment precision is given by the ratio between the bars’ length and the reading\\nerror of the position of the line over the gauge, which is comparable to the line’s\\ndiameter. This gives:\\nda = a\\nL          ,        (15)\\nSTARFLAG used bars 680mm long, giving an alignment accuracy of 0.25/680 =\\n0.0004rad, well within the required accuracy of most stereoscopic observations. The\\nonly problem with this alignment method is wind, since it moves the line, making it hard\\nto align the line to the gauges. However, as long as the wind is below 12m/s, the method\\nworks well, with a baseline of up to 25m.\\nThe convergence angle a is only one of the three Euler angle that specifies the\\nmutual orientation of the two cameras. Using flight terminology, a is the yaw angle,\\ncorresponding to a rotation around the y-axis. The roll angle g corresponds to a rotation\\naround the z-axis, and finally the pitch b corresponds to a rotation around the x-axis\\n(Fig.6). In the STARFLAG setup, the mutual roll angle g between the cameras was set to\\nzero. This can be done very accurately using the line method above: on each bar, we\\nmounted a second gauge parallel to the y axis and rotated the micrometric heads until the\\nline passed over the desired marks on both bars (Fig.5).\\nIn contrast, the mutual pitch b cannot be fixed using the line. In STARFLAG, we\\nused a Suunto clinometer to measure the tilt-up of the two bars (typically around 30-\\n14\\n40%) and thus to set to zero the mutual pitch angle. The maximal error of the clinometer\\nwas 0.02rad, which, by default, became the error on the relative pitch. This  is much\\nworse than the error on the yaw and roll angles (~0.0004rad). Fortunately, however,, the\\nerror on the pitch angle b has very small influence on the accuracy of the 3D\\nreconstruction formulas (Hartley & Zisserman 2003). Various tests performed within\\nSTARFLAG (see later) showed that an error on the relative pitch of 0.02rad is more than\\nadequate to achieve very accurate results.\\nThe last thing that must be measured is the baseline, i.e. the distance d between\\nthe two cameras. This can be done accurately and easily using a commercial laser meter.\\nOne point that deserves attention, however, is that, strictly speaking, the two cameras are\\nseparated by a 3D vector, and not simply by a scalar quantity (Hartley & Zisserman\\n2003). Let us take as a reference frame the one of the right camera, with the x and z-axes\\nas in Fig.4. If the cameras are parallel (and so are their focal planes), then the x-axis runs\\nparallel to both their focal planes, so that all the displacement, d, takes place along the x-\\naxis. Under these conditions, the displacement vector of the left camera has components\\n(-d, 0, 0). If, on the other hand, the cameras are convergent by an angle a (yaw), then the\\ndisplacement vector of the left camera has a small z component, and it reads (-d, 0,\\nda/2), where we have neglected second order terms in a. We repeat once again that an\\naccurate determination of the alignment angles, and of the baseline length, is of\\nparamount importance to generating reliable results, because all these parameters enter\\ncritically in the reconstruction formulas.\\nStereometric Tests and STARFLAG accuracy\\nThe 3D apparatus must be carefully and regularly tested. This can be achieved by\\ntaking a set of stereo photos of some artificial targets located at a distance comparable to\\nthe animal groups under study (the optimal stage is the façade of a building; targets can\\nbe located at different windows). The mutual distances among the targets and their\\nabsolute distance from the origin of the reference frame (normally the focal point of the\\nright camera) are then measured by means of a Hilti laser meter. The laser estimate is\\nthen compared to the 3D streometric reconstruction, so that an assessment of error can\\nbe made. It is important to have several targets, covering as many different parts of the\\nfield of view as possible, since radial distortion strongly depends on the position on the\\nfield of view. Tests must be performed regularly, in order to be sure that the calibration\\nparameters are not drifting away from their original values. Note that, in order to work\\nout the 3D coordinates of the targets, one needs to have the reconstruction algorithms\\nready. The full reconstruction formulas are more complicated than the simplified\\nformulas above, and they will be described at the end of the second part of this paper.\\nWithin the STARFLAG project, we performed several stereometric tests. The\\nbaseline was d=25m. The focal length in pixels of our lenses (Canon EOS 1D Mark II\\nwith 35mm lenses) was W=4335 (this is the average value, since the calibrated value\\nvaried from camera to camera of about 10 pixels). The error, dz, on the relative distance\\nof two nearby targets located at distance, z, from the cameras is dominated by the error.\\nds, in the determination of the image positions. For birds at a distance, z=100m, the\\nequations above give a nominal error on the relative distance equal to dz=0.09m, for two\\nobjects at 1m one from another. The error, Dz, on the absolute distance is dominated by\\n15\\nthe error da on the convergence. With bars 680mm-long and a line of diameter 0.25mm,\\nour method gave a maximum misalignment da=0.25/680=3.7¥10-4rad, thus giving a\\nnominal error Dz=0.14m (targets at 100m). Our tests gave dz<0.04m, corresponding to\\nds<0.4pixel, and Dz<0.92m, corresponding to da<2.3¥10-3rad. The maximal error on the\\nabsolute distance was thus small, but larger than the nominal value expected from our\\nalignment method. A careful investigation of this fact showed that residual radial\\ndistortion was responsible for this greater-than-nominal error.\\nSynchronization\\nWhenever the targets are on the move, which is the case for most animal groups, stereo\\nphotographs must be taken at the same instant. This is done by connecting both cameras\\nto an electronic device that shoots them synchronously. Of course, perfect\\nsynchronization is not possible. A certain amount of desynchronization is always present\\nand one must carefully check accuracy.\\nThe Release Time Lag\\nThe largest source of synch error is the cameras themselves:  in particular,\\nfluctuations in the release time lag (RTL). The RTL is the lapse of time between the\\nmoment the camera is fired and the moment the shutter curtain actually moves, allowing\\nlight to reach the CCD. Each camera model has a nominal RTL, but in practice this\\nfluctuates considerably. Clearly, a different RTL in the two stereo cameras can be a\\nmajor cause of desynchronization (desynch).  The best means of stabilizing the RTL is to\\nkeep the lens’ aperture as large as possible. For most cameras, the RTL is stable if the\\naperture is kept within two stops from the largest value allowed by the lens. The reason\\nfor this is that, of the many things contributing to the RTL, one of the most prominent is\\nthe time taken by the camera to close down the aperture. Consequently, the smaller the\\naperture, the longer and more erratic the RTL becomes. Note that this remains true even\\nwhen the exposure is preset manually.\\nThe effects of desynch are easy to calculate if the animals’ typical velocity is\\nknown. The distance travelled by animals within a desynch time, dt, is simply v x dt,\\nwhere v is the velocity. The effect of desynch is therefore to create a false extra\\nstereoscopic shift, which is solely due to animal movement, rather than to bona fide\\ndistance. This false shift will therefore alter the reconstructed 3D position of the target.\\nA reasonable rule of thumb is that this desynch distance should be smaller that the\\nphysical size of the animal. In the STARFLAG setup, we measured the desynch to be\\nsmaller than 10ms, which for birds travelling at about10m/s gives a desynch distance of\\n0.10m, acceptable when compared to starlings’ wingspan of 0.40m. The RTL of a\\ncommercial camera should never be trusted, and must always be measured carefully.\\n16\\nShooting Multiple Images\\nSo far we have explained how to synchronize two cameras on a single photo. This\\nis fine if one is interested only in the static structure of groups. However, much of the\\ninterest in the 3D reconstruction of animal groups lies in determining their dynamical\\nproperties, and to achieve this aim, one needs to take several successive images, at the\\nfastest possible rate.  Excluding video cameras, whose shortcomings we have already\\ndiscussed, this means firing two cameras at a fast rate, while keeping them closely\\nsynchronized. As with synchronization for single photos, this is a non-trivial task.\\nHigh-end cameras have a continuous mode: by keeping the shutter release button\\npressed down, the camera shoots continuously at a certain rate, measured in frames-per-\\nseconds (fps). Although intuitively attractive, a camera’s continuous mode must not be\\nused for the purposes of dynamic 3D reconstruction. This is because a camera whose\\nnominal refresh rate is, for example, 8fps, does indeed shoot eight photos in a second,\\nbut it does not shoot one photo every 1/8 of a second. Shooting during continuous mode\\nis typically quite irregular, and fluctuations are large, normally exceeding the minimal\\nthreshold for a decent synchronization.\\nThe only solution to this problem is to fire each individual camera independently,\\nusing a purpose-built electronic device. Even so, one needs to be very careful: the\\nmaximum shooting rate must be kept well below the nominal continuous rate, otherwise\\nthe RTL will fluctuate too much, and synchronization will be lost. For example, in\\nSTARFLAG, our cameras had a nominal continuous rate of 8.5fps. After accurate tests,\\nhowever, we discovered that, if the camera was fired faster than 5fps, the RTL became\\nerratic, with fluctuations that were too large to guarantee reasonable synchronization.\\nThus, we had to fire the cameras at 5fps, rather than at 8.5fps. The only way to\\ndetermine the maximum shooting rate that a camera can sustain without becoming\\nerratic is to conduct appropriate tests, such as taking photos of a high precision\\nchronometer. In the STARFLAG project, we built a simple LED-based device that\\ndisplayed the time in milliseconds and then we photographed it to check the cameras’\\nsynchronization.\\nFor birds that fly fast as starlings, a rate of 5fps is not high enough to produce\\nsatisfactory dynamic tracking of the animals’ trajectories. Even though this problem is\\nlikely to be solved in the near future by improving cameras technology, it is worth\\nbriefly discussing this issue. Our solution to this problem was to use twin pairs of\\ncameras shooting in an interlaced fashion (Fig.7). That is, two cameras (1 and 2) were\\nmounted as close as possible to each other on the bar. The electronic device fires both\\ncameras at 5fps, but with opposite phases, so that the net effect is a refresh rate of 10fps\\nof the twin camera system. The two cameras must be carefully aligned, but this is\\nrelatively easy, since they are mounted rigidly on the same bar.\\nThis solution comes at a cost, however. First, doubling the number of cameras\\nmeans doubling the budget of a study. As we discuss below, our matching procedure\\nrequires a “trifocal” method, with three points of view this means a total of six cameras\\nis needed. Second, interlacing has a small but disturbing effect on 3D reconstruction: the\\ntwo series of photos are taken from two coordinate frames (1 and 2) that are slightly\\nshifted (0.2m or so) with respect to each other. In addition, the two stereo couples\\n(1Left-1Right vs. 2Left-2Right) will always be slightly different from each other,\\n17\\nbecause it is impossible to achieve identical calibration and alignment for the two stereo\\ncouples. The shift of the reference frame is easy to correct. However, the difference in\\ncalibration and alignment is more annoying: the dynamic trajectory of a target is an\\ninterlaced series of reconstructions, 1-2-1-2-1-etc. Given that systems 1 and 2\\nreconstruct in a slightly different way, the resulting 3D trajectory therefore resembles a\\nzigzag path. The magnitude of this effect depends on the specific apparatus used. Within\\nSTARFLAG, the effect was small, in the order of a centimetre. At one level, this low\\nvalue represents a good consistency check for the whole 3D reconstruction procedure\\nand the reliability of the error analysis. However, even a small value like this quickly\\nbecomes very annoying as soon as one begins to look at dynamic trajectories. In this\\ncase, even small variations of the heading of a bird can make big differences when\\ncomputing individual velocity. Compensating for this zigzag effect is possible, although\\nnot completely straightforward: once a certain number of trajectories has been\\ndetermined, one can then calculate the optimal transformation (rotation plus translation)\\nof coordinate frame 2 that minimizes the overall roughness of the trajectory. We will\\ndiscuss this point further in a separate publication on individual dynamic trajectories.\\nFROM 2D IMAGES TO 3D POSITIONS\\nOnce digital stereoscopic images of an animal group have been taken, there is still some\\nwork needed to generate a 3D reconstruction. First of all, individual animals on the\\nphotos must be recognized and their 2D, photographic positions measured carefully\\n(Fig.8). This process is technically known as segmentation. This is a relatively\\nstraightforward process if animals’ images are large and well separated from each other\\n(unlikely case), or relatively hard when animals’ images are small, packed and partly\\noverlapping (likely case). After the segmentation, one deals solely with the sets of 2D\\nphotographic positions. This brings us to the hardest part of the task, namely\\nstereoscopic matching, also known as the correspondence problem. Basically, we have\\nto match each animal’s image in the right photo to its corresponding image in the left\\nphoto. This is done for thousands of very tightly packed identical points, because\\nanimals’ features are of no use, mainly due to distance, poor light conditions, and stereo\\ndisparity (Fig.9). Solving the matching problem represents the major original advance of\\nthe STARFLAG project. Finally, once the matching is done, the 3D position of each\\nanimal can be worked out by using standard algorithms. The key references for this\\nsection are Forsyth & Ponce (2002) and Hartley & Zisserman (2003).\\nSegmentation\\nThe basic principle behind segmentation is quite simple: animals are dark objects against\\na lighter background (or, more rarely, vice-versa), so that a threshold on the intensity of\\nthe luminosity signal can distinguish between animal and background in the photograph.\\nA simple code can then be run to identify all connected clusters of pixels with intensity\\nlower than the threshold. Each cluster corresponds, in the best-case scenario, to a single\\nanimal. In this case, the centre of mass (i.e. the average position of all pixels in the\\n18\\ncluster) is calculated, and this gives the best estimate of the 2D position of that animal.\\nIn practice, the problem is more complicated than this.\\nBackground Subtraction\\nThe first, and most prominent, problem is that there are many inanimate objects in the\\nphoto (trees, buildings, and in particular clouds) that are similar to animals with respect\\nto contrast with the background. Therefore, a naïve, threshold-based method will fail.\\nFortunately, unlike most unwanted objects, animals move, and this simple fact helps\\nsolve the problem. For our purposes here, we assume that animals are darker against a\\nlight background.\\nWe take several repeated images, at a given refresh rate (for example 5fps).\\nGiven a certain photo, K, we want to find a method to subtract its background, so that\\nthe only signal remaining is the animals’. To this end, we can consider a short series of\\nm successive photos centred on K. For example, for m=3 we consider photos K-1, K,\\nK+1. Let us focus on a certain pixel with coordinates (i,j): either this pixel is occupied by\\nthe immobile background in all m photos, or it is occupied by a moving animal in at least\\none of the m photos. If m is large enough, it is very unlikely that pixel (i,j) will be\\noccupied by moving animals in all m photos of the series, because animals are moving.\\nThis means that pixel (i,j) has ‘seen’ the background in at least one photo. We then\\nchoose, for each pixel, its highest intensity value within the m photos. Since animals are\\ndarker than the background, by doing this we are selecting an instant of time in which\\nthat pixel is occupied by the background. In this way, we can reconstruct an image of all\\nimmobile objects in the scene (the background), while cutting out all moving animals.\\n(Note that this procedure generalizes the simpler method of subtracting two successive\\nimages, and is more reliable)\\nOnce the background is known, we can subtract it from the central photo K,\\nand take the absolute value of the intensity difference in each pixel. We thus obtain a\\nnew image where only mobile targets, the animals, stand out against a dark background\\nwith zero intensity. At this point the threshold algorithm described above can be more\\nsafely applied. We finally repeat the same procedure for all photos.\\nThere are, as usual, some important caveats. The first is noise. We normally\\nwork with JPG images, at various degrees of compression (in order to minimize the size\\nof the photos and thus optimize the camera buffer), but even in the absence of JPG\\ncompression (raw format), some noise is unavoidable. This implies that even after\\nsubtraction, the background’s intensity will be non-zero. If an animal is very distant or\\nvery small, its image will be faint, and its intensity may be barely larger than the\\nsubtracted background. For this reason the intensity threshold must be selected very\\ncarefully. In order to improve the animal/background contrast it is important to know the\\ncolour channel in which the method works best. This strongly depends on light\\nconditions, so that only empirical tests can select the right colour channel. Moreover,\\nafter subtraction, it is also useful to apply a noise reduction filter, as median or mean;\\none must pay careful attention, however, in order to avoid too drastic a reduction in the\\nimage’s sharpness.\\nAn obvious question raised by this method is determining the ideal number m\\nof images used for background subtraction. The probability that a given pixel is not\\n19\\noccupied by an animal’s image in at least one of the m photographs, grows with m. So,\\ntaking large values of m is useful. There is a side effect of increasing m, however,\\nbecause the background may contain some inanimate but slowly moving objects, such as\\nclouds (unfortunately, shooting only on clear or uniformly overcast days would reduce\\ntoo much the amount of collected data). If the number of photos, m, is so large that it\\ncovers a substantial lapse of time, clouds may have moved more than one pixel. As a\\nconsequence, the subtraction algorithm will identify clouds as moving ‘animals’, and so\\nintroduce them in the analysis. For this reason the number of photos for background\\nsubtraction must be carefully tuned to the animals’ velocity and size. In the STARFLAG\\nproject, we employed a value of m between 3 and 5.\\nBlob-Splitter\\nUnless the group is exceedingly sparse, which almost never happens in the most\\ninteresting cases, there will always be some animals so close to each other on the photo\\nthat they will be recognized as a single objects by the threshold algorithm. We call these\\nobjects ‘blobs’ (see Fig.8). A blob-splitting method is therefore needed.\\nFirst, one needs to identify which objects are likely to be blobs. This can be\\nachieved by identifying those objects whose size (area) is larger than expected. Fixing\\nsuch a threshold in the size may not be easy, however, especially if there is high\\nvariability in animals’ sizes and shapes.  Second, a blob-splitting algorithm must be run\\non each blob, in order to identify individual animals belonging to the blob. The\\nSTARFLAG project used a variant of the watershed method (Forsyth & Ponce 2002):\\nthe intensity threshold is raised gradually, until the object splits in two (or more) parts,\\ncorresponding to individual animals (Fig.8). The centres of mass of these parts are\\ncomputed to give their 2D positions. In order to be sure that these parts correspond to\\nindividual animals, they are then followed separately by further increasing the threshold,\\nuntil they finally disappear (i.e. the object’s luminosity goes fully below the threshold).\\nIf, on the other hand, the objects split again, then the new centres of mass are calculated,\\nuntil one is sure that only individual animals are left.\\nThe effectiveness of the whole segmentation process, and in particular of the\\nblob-splitting algorithm, can effectively tell us whether or not the group under study is\\ntoo dense to be reconstructed. If, after careful optimization of the parameters, the\\nsegmentation produces huge super-blobs of hundreds of animals, then it is very likely\\nthat, even after applying the blob-splitter, the animals’ positions will be so noisy that it\\nwill be very hard to continue with the analysis. In these cases the only thing one can do\\nis try to improve  resolution, both digital and optical (more pixels and better lenses). On\\nthe other hand, if blobs contain few animals (up to ten, or a few more), then the blob-\\nsplitter can produce excellent results.\\nMatching\\nAny method for the 3D reconstruction of a scene (stereoscopy, orthogonal method,\\nshadow method) requires placing different images of the same object into\\ncorrespondence. This is the correspondence, or matching, problem. For small and/or\\n20\\nsparse groups this task is not problematic, and it can be done manually for each animal.\\nHowever, as soon as the number of animals is larger than a few tens, and the group is\\nmore cohesive, the time and effort costs of manual matching become prohibitive (Fig.9).\\nThere are standard methods in computer vision to deal with the matching\\nproblem, but they mostly rely on objects’ features to do the correspondence. The features\\nused may be colour, shape, size, and orientation, or any other attribute useful for\\nrecognizing the same object in two images. Unfortunately, these methods are unsuitable\\nfor most animal groups. In order to have the whole group within the field of view, the\\ndistance at which photos are taken are often so great that the images of individual\\nanimals cannot be identified by their features, and those features that are present are\\ntypically the same for all animals. In the case of starlings, for example, a flock looks like\\na set of dark, featureless points (Fig.9), and matching birds by their features is out of\\nquestion. Moreover, even in those cases where animals’ images are larger and more\\ndetailed, it is possible that an animal’s feature in one photograph may look significantly\\ndifferent from that in the other one, due to the necessarily large stereo disparity. We\\nconclude that features-based matching cannot be used in animal groups reconstruction.\\nElements of Epipolar Geometry\\nLet us name a certain animal Frank, and let us specify A as the right camera/photo and C\\nas the left camera/photo of the stereo pair. The first practical aid in trying to develop a\\nmatching algorithm comes from the following principle of projective geometry: given\\nthe position of Frank’s image on photo A, we can calculate the straight line in photo C\\nalong which Frank’s image lies (Hartley & Zisserman 2003) (Fig.10). This is called the\\nepipolar line.  The epipolar line in photo C is the intersection between the CCD plane of\\ncamera C, and a second plane defined by the following three points: Frank; Frank’s\\nimage on C; Frank’s image on A (this second plane is called the epipolar plane). As a\\nsimple illustration of this principle, consider two identical cameras, perfectly parallel,\\nand shifted only in the x direction.  In this case, the two images of Frank have identical y\\ncoordinates, whereas the x coordinate is shifted by an amount equal to the stereo\\ndisparity. So, given the image on the right photo, with coordinates (xR, yR), the\\ncorresponding image on the left photo is found along the line specified by the equation\\nyL= yR. In more general cases, with non-parallel and non-identical cameras, the epipolar\\nline is no longer parallel to the x-axis. The principle, however, is still valid (Armangue &\\nSalvi 2003).\\nThe advantage of knowing the epipolar line associated with an animal is\\nobvious. Given Frank’s image on A, we want to identify Frank’s image on C. By using\\nFrank’s image on A we can compute its epipolar line on C, and instead of searching for\\nFrank’s image anywhere in C, we can restrict the algorithm to do so only along the\\nepipolar line. This decreases significantly the complexity of the problem. However, due\\nto noise, the matched image will never lie exactly on the epipolar line, but slightly off it,\\nby an amount that depends on the amount of noise present (Fig.10). As a result, in a\\ndense group, there will be many animals in C sufficiently close to the epipolar line to be\\nconfused with Frank. For example, in a starling flock of 2000 birds, knowing the\\nepipolar line may reduce the number of possible candidates (depending on the amount of\\nnoise) to 100-200 birds, but this is still a lot of birds. How do we find the right match?\\n21\\nTrifocal Method in a Nutshell\\nTo solve this problem we must bring in a third camera, let us call it B. This camera must\\nbe located quite close to the right camera A (say 2.5m), and it must be pointing roughly\\nin the same direction as A (Fig.11).  It is important to note that the system A-B will not\\nbe used as a stereometric pair (i.e., it will not be used for 3D reconstruction). The only\\npurpose of camera B is to help solving the matching problem. For this reason, cameras A\\nand B need not to be aligned. They still need to have similar fields of views, however, so\\nthey should roughly be aligned by eye.\\nLet us assume that we know Frank’s location on both A and on B. In other words, we\\nare assuming that we have solved the matching problem for the nearby pair A-B. This is\\na reasonable assumption, since the pair A-B has a very small baseline; this fact implies\\nthat the stereo disparity is small, making the matching quite easy (we explain later how\\nthe A-B matching is performed). There are now two epipolar lines running through\\nphoto C, namely the line generated by Frank’s image A and the line generated by\\nFrank’s image B. The crucial point is that, by construction, Frank’s image on C (the\\nmatch we are after) must lie on both epipolar lines, so that the two lines must cross at\\nFrank’s position. This means that we no longer have to look for Frank’s image along one\\nline, but at the intersection of two lines, i.e. around a point (Fig.12). This is clearly a\\ngreat advantage because it dramatically decreases the size of the search area. Due to\\nsome inevitable noise, there may be a couple of candidates (including Frank) close to the\\npoint of intersection. However, the number of candidates is now reduced enormously,\\neven in very dense groups. In fact, it goes down to less than 5 in a standard STARFLAG\\ncase. Moreover, it often turns out that Frank is simply the closest animal to the\\nintersection point, so that he is very easily identified. In this way, the matching problem\\nis (in principle) solved.\\nEven though this description captures the is the spirit of it, the actual trifocal method\\nis somewhat more complex. For mathematical reasons, rather than using the two\\ncrossing epipolar lines, A-C and B-C, as in earlier methods (Ito & Ishii 1986; Ikawa et\\nal. 1994), it is much more convenient to use a mathematical object called a trifocal\\ntensor (Hartley 1997; Hartley & Zisserman 2003). The fundamental idea, though, is the\\nsame as described above: if Frank’s A-B match is known, it is possible to estimate\\nFrank’s position in C. An alternative explanation of the trifocal principle is that by using\\nthe A-B match, one can generate a very rough estimate of the 3D position of Frank, and\\nthus project it back onto C. The technical details on how to compute and use the trifocal\\ntensor can be found in the literature (Hartley & Zisserman 2003).\\nUnavoidable complications\\nThere are two key problems to be solved before the trifocal technique can be used.\\nFirst, the trifocal technique only works if all A-B matches are known, since the method\\njust transfers every A-B match to C. Even though we expect the A-B matching to be\\nrelatively easy, due to the small A-B baseline, it is still unclear how this matching is\\nachieved in practical terms. Second, epipolar lines and the trifocal tensor do not come\\nfree. In fact, in order to compute these mathematical objects, a sufficiently high number\\n22\\nof matches of the A-B-C triangle must already be known. More precisely, not only do\\nwe need all the easy A-B matches, but we also need some of the hard A-C matches to\\ncompute the trifocal tensor. This presents something of a paradox: we have a very\\npowerful method to match animals in the essential A-C pair, but we need to know at\\nleast some of these matches in order to use this method. Why is this?\\nA simple example may clarify this point. To know the position x, at time t, of a car\\nmoving at constant speed we can use the equation x = A + B t, provided that we know the\\nvalues of the parameters A (the initial position) and B (the velocity). Yet, in order to\\ndetermine A and B we must already know some points of the trajectory, for example,\\nwhere the car was after 10sec, and after 90sec. If there is no noise, i.e. no error in the\\npositions, two points are enough, since we must determine two parameters. However, in\\nthe more general noisy case, the larger the number of points, the more accurate the\\ndetermination (via a fit) of the parameters A and B, and the better our estimate of the\\nunknown position, x, at time t.\\nThe epipolar and trifocal relations among the objects in A-B-C are equivalent to the\\nrelation between x and t in this example. Similarly, even though the trifocal tensor is in\\nprinciple determined by 27 parameters (that describe the actual geometric relations\\namong the three cameras), because of noise one needs as many A-B-C matches as\\npossible; in practice, this means at least 50. Once these 50 matches are known, one can\\ncompute, via a non-linear fit, the trifocal tensor, which is conceptually equivalent to the\\ntwo epipolar lines (Hartley & Zisserman 2003).\\nIn order to solve both problems (the A-B complete matching and the A-C partial\\nmatching), therefore, we need a matching method that works without any a priori\\nknowledge (or a very poor one) of the geometry of the pair of cameras under\\nconsideration. With such a method, we can determine all matches of the easy A-B\\ncamera pair. Moreover, the same method can be used to determine the required number\\nof difficult matches (>50) involving the A-C camera pair, and thus to initiate the\\npowerful trifocal process described above. Such a method was first developed by\\nSTARFLAG, and forms the core of the whole project. The method is called matching\\nzero (M-Zero), because it is the matching algorithm everything else in the matching\\nprocedure is based upon.\\nMatching Zero\\nOur brain is quite good at matching. When we look at a pair of stereo images, we\\nclearly see that the global arrangement of points is very different in the two photos\\nbecause of the stereoscopic disparity (Fig.9). However, what we also recognize\\nimmediately is that the mutual geometric relationships among points that are close to\\neach other are only weakly deformed. In other words, our brain is geared to detecting\\nsimilar patterns in the two photos. Once detected, we determine the matches simply by\\nidentifying the vertexes of these two patterns (Fig.13). M-Zero was born with the aim of\\nmimicking this procedure, and it is, very broadly speaking, a pattern recognition\\nalgorithm, although one tailored specifically to epipolar geometry.\\nLet us consider the M nearest neighbours of Frank in real 3D space, N1, N2, … , NM.\\nWe call NR1, NR2, …, NRM the images of Frank’s neighbours on the right (R) photo, and\\nNL1, NL2, …, NLM the images of Frank’s neighbours on the left (L) photo. Finally, we\\n23\\ncall FR and FL the images of Frank on the two stereo photos (Fig.14). The basic idea is\\nas follows: if the neighbours are close to Frank in real space, then their stereoscopic shift\\nis similar to Frank’s. As a consequence, the 2D pattern formed by Frank and his M\\nneighbours must be similar on the two photos. The M-Zero algorithm looks for these\\nsimilar patterns.\\nIn order to assess the similarity of two patterns, the first thing to do is to superimpose\\ntheir centres.  To fix ideas, let us take the right (R) pattern as a reference frame. We then\\ntranslate all points belonging to the left (L) pattern in the same direction and by the same\\namount. We choose this translation such that the centre of the left pattern, FL, ends up\\non top of the centre of the right pattern, FR (Fig.14). If the stereoscopic disparity\\nbetween left and right images were a constant shift, equal for all birds, then this\\ntranslation would bring on top of each other not only FL and FR, but also all their\\nneighbours, and the two patterns would be identical. However, due to their different\\ndepths, all birds have different stereo shifts. Therefore, once the centres of the two\\npatterns are superimposed, there is a residual stereo shift of each pair of neighbours. For\\nexample, the residual distance between NR1 and NL1 is proportional to the depth\\ndifference, Dz, between neighbour N1 and Frank. If this difference is small, i.e. if N1 is\\nindeed close to Frank in real 3D space, then the residual stereo shift is also small, so that\\nNR1 and NL1 will be close to each other in the common (translated) reference frame.\\nWe can say more. If the cameras are nearly parallel (or have a small convergence\\nangle a), then the stereo shift will be mostly in the x direction, whereas the y coordinates\\nof NR1 and NL1 will not differ very much (Fig.14). Thus, in order to measure the extent\\nto which the two patterns formed by the neighbours overlap, we can count how many\\npairs of R-L neighbours fall within the same small elongated box-shaped area, with sizes\\nbx and by, where bx >> by (Fig.14). If, for example, NR1 and NL1 are within such a box,\\nand no other points fall inside it, then we can say that this pair of neighbours overlaps.\\nThe total number of overlapping pairs of neighbours then gives the global overlap of the\\npattern. In this way, one can assign a large overlap to those patterns (and thus to their\\ncentres) that correctly match with each other.  On the other hand, if we run the same\\nprocedure using two non-matching animals as centres, there is no reason, apart from\\nstatistical fluctuations, to expect the patterns of their neighbours to show any significant\\noverlap.\\nThe M-Zero procedure then consists in running this test for all possible candidate\\npairs on the two photos, assigning to each candidate pair a probability proportional to the\\noverlap of their patterns: the larger the overlap, the larger the probability that this is a\\ncorrect match. Of course, we do not know the 3D neighbours of an animal a priori, so\\nthe algorithm must be run using the photographic 2D, neighbours on each photo, under\\nthe reasonable assumption that these will include many 3D neighbours. The number M\\nof these neighbours, and the sizes bx and by of the overlap box are crucial parameters of\\nthe algorithm, and they must be optimized for specific cases. Typical STARFLAG\\nvalues are M=50 neighbours, bx = 30 pixels and by = 1.5 pixels.\\nEven though the fundamental ingredients are those described above, there are\\nalso some highly technical details of the actual M-Zero algorithm developed by\\nSTARFLAG that we will present elsewhere.  Here, we will just mention briefly the most\\nrelevant ones. First, as the cameras have a small convergence angle, the residual stereo\\nshift is not exactly in the x direction; for this reason, the rectangular boxes (Fig.14) used\\n24\\nto define the overlap must be generalized in order to follow the epipolar structure of the\\nphotos. Second, by using overlap measurements, we can assign a matching probability to\\neach pair of animals, i.e. to all potential matches. Once this is done, an assignment\\nalgorithm can then be used to maximize to total probability, and thus find the actual\\nmatches (Gabow 1976).\\nIn the case of the A-B pair (baseline 2.5m), the stereo disparity is very small and\\nthe M-Zero method is thus extremely effective, because only correctly matched pairs\\nobtain a significant probability from the pattern test. Therefore, the M-Zero alone can\\nmatch virtually all the objects in the A-B pair. Within STARFLAG, we were able to\\nmatch, on average, 95% of the birds in the A-B pair. In contrast, the A-C stereo pair\\n(baseline 25m) is trickier: due to the large baseline, stereo distortion is substantial, so\\nthat the overlap between the two patterns of a correct match is not always very large, and\\nit can be of the same order as the overlap between two non-matching, random patterns.\\nThis makes it impossible to use M-Zero to work out all matches in the A-C pair.\\nHowever, recall that we need only a small number of such matches to compute the\\ntrifocal tensor. This is much more tractable because there are always some matches\\nwhose patterns overlap very neatly. Such matches have an outstanding probability\\ncompared to random cases, so that it is quite easy to identify them. Typically. the\\nnumber of these safe matches is sufficiently large (>50) that they can be fed to the\\ntrifocal method. The matching problem is thus solved.\\nEfficiency of the Matching Algorithms\\nEven though the matching procedure just described is quite effective, there will\\nalways be cases where it fails. Typically, if group density is too high, the very process of\\nsegmentation (animal recognition) becomes very imprecise, the animals’ positions are\\npoorly determined, and the blob-splitter is unable to separate different animals. Under\\nthese conditions, the input of the matching procedure is very unreliable, and the\\nmatching may fall apart. When this happens the only option is to discard the sample.\\nBetter optical and pixel resolution may improve the situation.\\nAs a benchmark, we can say that our algorithms work well when the average\\nnearest neighbour distance of the group is larger or equal than 0.6m, at an average\\ndistance from the apparatus of 100m, for animals of size 0.4m (starlings). Sparser groups\\nat much larger distances or, conversely, denser groups at smaller distances, can also be\\nstudied successfully. However, if we increase the density (decrease the nearest\\nneighbour distances) and at the same time we increase the distance, the whole process\\nbecomes progressively harder.\\nIn any case, matching 100% of the animals in large groups (>1000 members) is,\\ngenerally speaking, out of question. Within STARFLAG, we were able to match and\\nreconstruct on average 88% of the birds in a given flock, and never less than 80%. Tests\\nwith synthetic data (with  similar distance and density as real flocks) gave less than 5%\\nof mismatches (outliers). Moreover, by means of synthetic data, we were also able to\\ncheck that our matching procedure did not introduce any bias into the analysis.\\n25\\nReconstruction\\nOnce animals on the stereo pair A-C are matched, their 3D position can be\\nreconstructed. This, however, cannot be done using equation (9): as mentioned above,\\nthis is a highly simplified equation that, while useful for error analysis, is unsuitable for\\nreal reconstruction. Fortunately, reconstruction formulas and algorithms are extensively\\nand clearly described in the literature (Hartley & Zisserman 2003), so here we give only\\na brief sketch of two major issues.\\n3D Reconstruction Algorithms\\nThe two stereo photos consist of two pairs of 2D coordinates, (xL,yL) and (xR,yR).\\nEpipolar geometry, i.e. the set of geometric relations between the cameras, provides four\\nequations connecting these four coordinates to the three real space coordinates (X, Y, Z)\\nof the target. There is therefore a redundancy in the number of equations (four) vs. the\\nnumber of unknowns (three). In principle, in a system unaffected by noise, these four\\nequations are not independent, such that one could discard any one of them, and solve\\nfor (X, Y, Z) using the three remaining equations. In practice, there is always a certain\\namount of noise, and this means that the four equations are, in fact, independent of each\\nother. Under these conditions, we cannot simply discard an equation because the\\nresulting 3D reconstruction would depend on exactly which equation we decide to\\ndiscard, and this would be unacceptably arbitrary. Thus, all four equations must be kept.\\nThere are two possibilities to obtain an accurate 3D reconstruction (Hartley &\\nZisseramn 2003). First, one can search for the value of (X, Y, Z) that minimizes the\\nresidual of the set of four equations (due to noise, the residual cannot be zero). This is a\\nproblem that can be solved easily by linear algebra. Second, one can use the re-\\nprojection method. Given the 3D position (X, Y, Z), we can re-project this position onto\\nthe two photos, obtaining a new set of 2D positions, i.e. two new images. We can then\\ncalculate the square distance between the original 2D images and the new 2D\\nprojections, and change the (X, Y, Z) point until this square distance is at a minimum.\\nAlthough both methods have their virtues, within STARFLAG, we used the first\\ntechnique, which was easier to implement and gave very accurate results.\\nPost-Calibration does not Work\\nOne very important technical warning is essential at this point. Whichever\\nspecific reconstruction algorithm one decides to use, the orientation angles of the two\\ncameras (yaw, roll and pitch angle) and their calibration parameters (focal lengths,\\noptical axis positions, etc) must be entered into it. As we have seen, it is quite hard\\n(although by no means impossible) to determine these parameters accurately. In\\nparticular, field alignment is often seen as too tedious a task. For this reason, it is\\nsometimes suggested that some of these parameters, in particular the orientation angles,\\nmay be determined a posteriori, once a sufficiently large number of objects has been\\nmatched on the two photos (Osborn 1997). This procedure can be called post-calibration,\\nand, , in principle, it is very valuable, because it dispenses with the need for careful\\nalignment in the field.\\n26\\nOur experience with the STARFLAG setup, however, is that post-calibration\\ndoes not work. The reason for this is that, even after professional calibration, there is a\\nresidual radial distortion in commercial cameras that makes the determination of the\\nangles via post-calibration completely unreliable. In particular, the yaw angle, a, is\\nmuch less accurately determined by post-calibration. Unfortunately, a  is also the angle\\nto which 3D reconstruction is more sensitive. Whenever we tried to use a post-\\ncalibration value for a, we always obtained catastrophic results. The only safe way to\\nperform a reliable 3D reconstruction is to align the apparatus and to measure the angles\\nas accurately as possible in the field, and to use these values of the angles in the\\nreconstruction formulas. We cannot exclude the possibility, however, that a much more\\ncareful determination of the radial distortion parameters, or the use of non-commercial,\\nmetric cameras may change this state of affairs.\\nCONCLUSIONS\\nImproving the Apparatus\\nEven though they are powerful, the methods described in this paper inevitably have\\nsome limitations. Many of these limitations are likely to be resolved by the\\nimprovements in technology. Digital resolution is increasing very rapidly; indeed, so\\nrapidly as to suggest that optical resolution, which is improving more slowly, may soon\\nbecome the bottleneck in animal recognition. A drastic improvement in the resolution of\\nrecording apparatus may allow the study of very distant and/or large groups. However,\\nthere is often an unavoidable limit in the atmospheric diffraction, which severely cuts off\\nthe efficiency of the segmentation process.\\nConcerning lenses, there is also the problem of radial distortion and camera\\ncalibration. This is a key issue: our experience has been that, at present, radial distortion\\nis the main source of error in the 3D reconstruction. Novel and more accurate algorithms\\nin computer vision may be developed in the future to make it easier to calibrate cameras\\nwithout the need of professional assistance.\\nFinally, the refresh rate too is likely to increase steeply in the near future, so that\\none will be able to produce sufficient data without the need to double the number of\\ncameras. \\nIndividual Trajectories and Dynamical Matching\\nOur analysis of the 3D data has been limited to static observables. In other words, we\\nhave not discussed how to measure and analyse the individual trajectories of animals.\\nGiven that the experimental apparatus we described is capable of shooting sequences of\\nphotographs at a relatively high rate, one may wonder why this is the case. The main\\nreason is that reconstructing the 3D positions of all animals in a group in successive\\ninstants of time is not equivalent to constructing individual trajectories. In fact, there is\\none last step that must be performed: dynamic matching, or tracking, of each individual.\\nThe problem is somewhat similar to stereo matching. Given the 3D position of Frank at\\ninstant t, we need to identify Frank from among all the animals in the 3D reconstruction\\n27\\nat instant t+dt. Clearly, the shorter the separation dt between successive instants, the\\neasier will be to perform dynamical matching. In addition, as this takes place in three\\ndimensions, it is slightly easier than 2D matching, due to the lower density of animals.\\nHowever, being able to match most animals in two successive instants of times\\nis not sufficient. The problem is that, however large, the efficiency of this process will\\nnever reach 100%. The algorithm is bound to miss some animals, for a number of\\nreasons, either because they are completely occluded by other animals at that instant of\\ntime, or simply because the algorithm makes a mistake. This upshot is that there will\\nalways be gaps in the trajectories. Let us assume that the efficiency of the algorithm to\\nmatch animals in two successive time steps is 90%, which is very good. After three time\\nsteps, however, the fraction of uninterrupted matches goes down to 81%, while after four\\ntime steps, it falls below 73%. After 20 time steps, only about 12% of the trajectories are\\nuninterrupted. At 10fps, this means that we are able to track, at most, 12% of the animals\\nover a maximum of 2 seconds, which is quite disappointing. In order to solve this\\nproblem one must find a method to reliably connect the different branches of interrupted\\ntrajectories. This is not an easy task, but it can be tackled using methods of statistical\\nphysics. Work in this direction is already underway within STARFLAG.\\nOnce the individual trajectories become available, a number of very interesting\\nquestions can be tackled. In fact, most simulation models present a dynamical\\ninvestigation of their results, so that a full comparison between such models and\\nempirical data requires individual trajectories. The main questions of interest here\\nconcern the diffusion properties of animals within groups, the degree of polarization (i.e.\\nthe extent to which velocities are aligned), and the dependence in time of the local\\nstructure of neighbours of each animal. Moreover, statistical correlations in time of the\\nstatic observables will also be very useful to provide us with a clearer picture of the\\nmechanisms of collective co-ordination.\\nThe importance of a multidisciplinary approach\\nIn the last three decades, theoretical speculation and numerical modelling have largely\\ndominated the field of collective animal behaviour. A lack of empirical evidence,\\nhowever, made it rather difficult to assess the validity of different ideas. It has been thus\\nimpossible to select the “best” model, or the “best” theoretical framework. In this paper\\nwe have showed that, by bringing together biology, statistical physics, computer vision\\nand mathematics, it is possible to obtain empirical data on large and cohesive groups of\\nmoving animals in three dimensions. The STARFLAG project focused on starlings.\\nHowever, the same techniques can easily be exported to other cases, most notably to fish\\nschools, insect swarms, and even to flying mammals, as bats. We hope that our methods\\nmay give rise to a new generation of empirical data.\\nQualitatively new data sets need new tools of analysis. The same mixture of\\ndifferent disciplines that solved the empirical problems can be used to introduce a new\\nclass of observables able to characterize in quantitative way the properties of the group.\\nWe describe these new tools of analysis in a companion paper (Cavagna et al. 2008). As\\nusual when new data become available, some of the results coming from the new\\nmeasurements are bound to clash with some of the “well-established” theoretical ideas in\\n28\\ncollective animal behaviour. It is necessary to keep an open mind, and not forget that it\\nis theory that must adhere to experiments, and not the other way around.\\nThe amount of physics, mathematics and computer vision we had to use in\\norder to obtain our empirical data is perhaps unusual when compared to the standard\\ntools of field-biology and ethology. It may thus be tempting to use the methods\\nexplained here as a “black-box”, without any real grasp on the underlying technical\\ndetails. Not only this would be impossible (we did not produce any user-friendly\\nmatching package, just to make an example), but it would also be a mistake. Collective\\nanimal behaviour is a truly interdisciplinary field: the same general questions asked by\\nan ethologist about starlings, are of interest also for a mechanical engineer working in\\nmobile robotics, or a condensed matter physicist. It is therefore necessary for research\\ngroups in collective behaviour to be equally interdisciplinary, as it happens, for example,\\nin molecular biology. We thus believe that the cross-fertilization of biology, physics,\\nmathematics and computer vision is an indispensable ingredient for any future\\ndevelopment in the field of collective animal behaviour.\\nAcknowledgements\\nWe warmly thank Frank Heppner for many useful suggestions and for carefully reading the manuscript.\\nA.C. is particularly indebted to him for several illuminating discussions, both on technical issues and on\\nbird flocking. We are also grateful to Michele Ballerini, Simone Cabasino, Nicola Cabibbo, Raphael\\nCandelier, Alessio Cimarelli, Evaristo Cisbani, Margherita Fiani, Luca Menci, Carmelo Piscitelli, Raffaele\\nSantagati, and Fabio Stefanini for their help. This work was financed by a grant from the European\\nCommission under the FP6-STARFLAG project.\\nBIBLIOGRAPHY\\nAdams, A. 1983. The Ansel Adams Photography Series. (Books 1, 2 and 3). Boston:\\nLittle, Brown and Company.\\nArmangue', X. & Salvi, J.  2003. Overall view regarding fundamental matrix\\nestimation. Image and Vision Computing  21, 205-220.\\nAoki, I. 1982. A simulation study on the schooling mechanism in fish. Bullettin of the\\nJapanese Society of Scientific Fisheries 48, 1081-1088.\\nBallerini, M., Cabibbo, N., Candelier, R., Cavagna, A., Cisbani, E., Giardina, I.,\\nLecomte, V., Orlandi, A.,  Parisi, G., Procaccini, A., Viale, M., & Zdravkovic, V.\\n2008. Interaction Ruling Animal Collective Behaviour Depends on Topological rather\\nthan Metric Distance: Evidence from a Field Study. Proceedings of the National\\nAcademy of Sciences USA 105, 1232-1237.\\nBallerini, M., Cabibbo, N., Candelier, R., Cavagna, A., Cisbani, E., Giardina, I.,\\nOrlandi, A.,  Parisi, G., Procaccini, A., Viale, M., & Zdravkovic, V. 2008. Empirical\\ninvestigation of starling flocks: a benchmark study in collective animal behaviour.\\nAnimal Behaviour, in press.\\n29\\nBudgey, R. 1998. Three dimensional bird flock structure and its implications for\\nbirdstrike tolerance in aircraft. International Bird Strike Committee Proceedings.\\nIBSC24/WP 12.\\nCavagna, A., Giardina, I., Orlandi, A.,  Parisi, G., & Procaccini, A. 2008. The\\nSTARFLAG handbook on collective animal behaviour: Part II, three-dimensional\\nanalysis. Animal Behaviour, in press.\\nCullen, J. M., Shaw, E. & Baldwin, H.B. 1965. Methods for measuring the three-\\ndimensional structure of fish schools. Animal Behaviour 13, 534-543.\\nDavis, J. M. 1980. The coordinated aerobatics of dunlin flocks. Animal Behaviour 28,\\n668-673.\\nEmlen, J. T. 1952. Flocking behaviour in birds. The Auk 69, 160-170.\\nForsyth, D. & Ponce, J.  2002. Computer Vision: A modern approach, Upper Saddle\\nRiver NJ: Prentice Hall Professional Technical Reference.\\nGabow, H. N.  1976. An Efficient Implementation of Edmonds' Algorithm for\\nMaximum Matching on Graphs. Journal of the ACM 23, 221-234.\\nGregoire, G. & Chate, H. 2004. Onset of collective and cohesive motion. Physical\\nReview Letters 92, 025702.\\nHartley, R. 1997. Lines and points in three views and the trifocal tensor. International\\nJournal of computer vision 22, 125-140.\\nHartley, R. & Zisserman, A. 2003. Multiple View geometry in computer vision,\\nCambridge: Cambridge Universiy Press.\\nHeppner, F. H. & Grenander, U. 1990. A stochastic nonlinear model for coordinated\\nbird flocks, in The ubiquity of chaos (Ed. by Krasner, S), pp. 233-238. Whashington DC:\\nAmerican Association for the Advancement of Science.\\nHuth, A. & Wissel, C. 1992. The simulation of the movement of fish schools. Journal\\nof Theoretical Biology 156, 365-385.\\nIkawa, T., Okabe H., Mori, T., Urabe, K. & Ikeshoji T. 1994. A method for\\nreconstructing the three-dimensional positions of swarming mosquitoes. Journal of\\nInsect Behaviour 7, 237-248.\\nIto, M. & Ishii, A. 1986. Three-view stereo-analysis. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence. 8, 524-532.\\n30\\nLonguet-Higgins, H.C. 1981. A computer algorithm for reconstructing  a scene from\\ntwo projections. Nature 293, 133-135.\\nMajor, P. F. & Dill, L. M. 1978. The three-dimensional structure of airborne bird\\nflocks. Behavioral Ecology and Sociobiology 4, 111-122.\\nOsborn, J. 1997. Analytical and digital photogrammetry, in Animal groups in three\\ndimensions (Ed. By Parrish, J. K. & Hammer, W. M.), pp. 36-60. Cambridge:\\nCambridge University Press.\\nParrish, J.K. & Edelstein-Keshet, L. 1999. Complexity, pattern, and evolutionary\\ntrade-offs in animal group. Science 284, 99-101.\\nPartridge, B. L., Pitcher, T., Cullen, J. M. & Wilson, J. 1980. The three-dimensional\\nstructure of fish schools. Behavioral Ecology and Sociobiology 6, 277-288.\\nPartridge, B. L. 1980 The effect of school size on the structure and dynamics of\\nminnow schools. Animal Behaviour 28, 68-77.\\nPitcher, T. J. & Partridge, B. L. 1979. Fish school density and volume. Marine\\nBiology. 54, 383-394.\\nPomeroy, H. & Heppner, F. 1992. Structure of turning in airborne rock dove (Columba\\nLivia) flocks. The Auk 109, 256-267.\\nRackham, H. 1933. Pliny’s Natural History. Cambridge, MA: Harvard University\\nPress.\\nRadokov, D.V. 1973. Schooling and Ecology of Fish. New York: Wiley.\\nReynolds, C. W. 1987. Flocks, herds, and schools: a distributed behavioral model.\\nComputer Graphics 21, 25-33.\\nSakai, S. 1973. A model for group structure and its behaviour. Biophysics 13, 82-90.\\nSelous, E. 1931. Thought-Transference (or What?) in Birds.  London: Constable.\\nSugg, R.W. 1965. An investigation into bird densities which might be encountered by\\naircraft during take off and landing. S&T Me mo 6/65. Technical Information and\\nLibrary Services, Ministry of Aviation.\\nVan Long, L., Aoyama, T. & Inagaki, T. 1985. A stereo photographic method for\\nmeasuring the spatial position of fish. Bullettin of the Japanese Society of Scientific\\nFisheries 51, 183-190.\\n31\\nvan Tets, G.F. 1966. A photographic method of estimating densities of bird flocks in\\nflight. CSIRO Wildlife Research 11, 103-110.\\nVicsek, T., Czirok, A., Ben-Jacob, E. Cohen, I. & Shochet, O. 1995. Novel type of\\nphase transition in a system of self-driven particles. Physical Review Letters 75, 1226-\\n1229.\\n32\\nFIGURES\\nFigure 1. Pinhole diagram\\nIn the pinhole model of the camera a photograph simply consists in the projection of the target\\nonto the CCD plane through the optical centre O, which for geometric convenience is\\nconventionally located behind the CCD. The distance between the focal point and the CCD\\nplane is the focal length f. A target of width L at distance z from the camera acquires, trough the\\nprojection, a width l that is related to the other parameters by simple proportions, L : (z+f)= l : f,\\nso that l/L=f/(z+f). The quantities f, L and z are all expressed in meters, whereas the CCD width\\nis measured in number n of pixels. The focal length too is sometimes measured in units of pixels,\\nin which case is indicated by W.\\n33\\nFigure 2. Effects of the radial distortion\\nA three-dimensional target was photographed with a Canon EOS 1D Mark II, with Canon 35mm\\nf2.0 lens. The photograph was then corrected for radial distortion by Mencisoftware s.r.l.. The\\nfigure shows the difference between the original photograph and the corrected one: the intensity\\nof each pixel in this photograph is equal to the modulus of the difference between the intensities\\nof the corrected and uncorrected photographs. If the two photographs were exactly the same, i.e.\\nif there were no radial distortion, the photograph obtained by the difference would look completely\\nblack. Instead, there is a clear effect of the radial distortion in that image points are shifted radially\\nby several pixels. This effect is proportional to the distance from the centre of the photograph. In\\nparticular, at the border of the field of view the effect of the shift caused by the radial distortion is\\nas large as 22 pixels. This shift would cause major errors in the 3D reconstruction if it were not\\ncorrected. [Image courtesy of Luca Menci, Mencisoftware s.r.l.]\\n34\\nFigure 3. Default vs. calibrated focus\\nLeft photo: the lens’ focusing ring has bees set on the default infinity mark. Right photo (taken at\\nthe same instant of time with a different camera): the lens’ focusing ring has been previously\\ncalibrated for optimal focus. The difference is striking: the left photo is significantly out of focus,\\nso much as to jeopardize the segmentation in a denser flock.\\n35\\nFigure 4. Elementary stereometry\\nThis is the simplest stereometric setup: the two focal planes are perfectly parallel, and the two\\ncameras are identical, but shifted along the x direction by a length d, called the baseline. The\\nfocal length W is measured in pixels. Due to the baseline between the two cameras, the two\\nimages of the target fall at different positions, uL and uR with respect to the centres of the CCD.\\nThe difference between these two positions defines the stereoscopic shift, or disparity. Simple\\ngeometric relations connect the disparity to the focal length, the baseline and the target’s\\ndistance z.\\n36\\nFigure 5. The alignment method\\nThe two cameras are mounted on two rigid bars, and a thin line connects the two bars. The line\\npasses close to a double orthogonal gauge, mounted on each bar. In this way both the yaw and\\nthe roll angle can be measured accurately (see text). The yaw angle a is fixed to the nonzero\\nvalue of 0.22rad in order to maximize the common field of view of the two cameras for targets at\\n100m, with baseline 25m.\\n37\\nFigure 6. The three Euler angles\\nThis is a front view of the two stereo cameras with perfectly parallel reference frames. The x-axis\\nruns parallel to the CCD gives the direction of displacement; a rotation of camera B around this\\naxis defines the relative pitch angle b. The z-axis runs through the lens, orthogonal to the CCD;\\na rotation of camera B around this axis defines the relative roll angle g. Finally, the y-axis runs\\nparallel to the CCD, but it is orthogonal to the displacement direction x; a rotation of camera B\\naround the y-axis defines the relative yaw angle a.\\n38\\nFigure 7. Interlaced shooting\\nIn order to increase the refresh rate we mount two cameras on each bar. Each camera is fired\\nevery 200ms. However, they are shifted half a period in time, so that they fire in interlaced mode.\\nThe net effect is to have a system shooting every 100ms, i.e. at 10 frames per second. The two\\ncameras have slightly shifted reference frames, and this shift must be corrected for after the 3D\\nreconstruction (see text).\\n39\\nFigure 8. Segmentation and blob splitter\\nIdentifying isolated birds is quite easy, and once this is done we can compute the centre of mass\\nof the object, defined as the average position of all pixels belonging to the object. A naïve\\nalgorithm, however, is unable to separate objects very close to each other: due to the small\\ndistance, the minimum intensity between two objects is larger than the initial threshold. When\\nthis happens a blob is formed, containing many different objects. In order to split the blob we can\\nraise the threshold until it becomes larger than the minimum intensity between the objects. At\\nthis point the objects separate, and their centre of mass position can be computed.\\n40\\n Figure 9. The matching problem\\nThese are two photographs taken by a stereoscopic pair of cameras, with a 25m baseline. The\\nflock (European starlings) is at about 100m from the cameras. In order to perform the 3D\\nreconstruction we have to match birds on the two photos. In other words, given a bird on the\\nright photo, we must find its corresponding image on the left photo. This is the matching, or\\ncorrespondence, problem. Due to the long baseline, the mutual positions of birds in the two\\nphotographs are radically different. Moreover, the features of the birds do not help in doing the\\nmatching, since at this distance they are little more than black dots. Coloured boxes indicate 3\\ncorresponding (matched) birds. In previous empirical studies the matching was performed by\\neye. Clearly, even for a relatively small flock as the one in the photo, it is very difficult to solve\\nthe matching problem by eyes. Some birds may be matched at the border (as those in the\\nboxes), but it is close to impossible to match birds by eye within the flock. Moreover, it would\\nquite time consuming. The flock in the figure is a typical example of the groups successfully\\nanalysed by STARFLAG.\\n41\\nFigure 10. Epipolar lines\\nGiven a point on the right photograph, we can compute a line that passes through the\\ncorresponding point on the left photograph. This is the epipolar line. The epipolar line is given by\\nthe intersection between the epipolar plane and the left CCD plane. The epipolar plane is the\\nplane passing through the target, its right image and its left image. Therefore, give point 1 in the\\nright photo, in order to find its match in the left photo, we have to look along epipolar line 1. In\\npresence of noise the point is slightly off the epipolar line, so that in a dense aggregation many\\nother points are close to the line, and it is not clear what is the correct matching.\\n42\\nFigure 11. Trifocal setup\\nThe trifocal setup of STARFLAG. The system of cameras 1 is synchronized, as well as the\\nsystem of cameras 2. However, cameras 1 and 2 fire in interlaced mode. Thus, each point of\\nview (C, A, and B) consists of two cameras (1 and 2) and has an effective refresh rate of 10fps.\\nThe stereoscopic pairs, used for 3D reconstruction, are A1-C1 and A2-C2 (interlaced). These\\npairs have a long baseline (25m), in order to obtain maximum 3D accuracy, and they are\\ncarefully aligned. Cameras B are needed for solving the matching problem, and they are not\\nused for 3D reconstruction. For this reason they are only loosely aligned to cameras A. An\\nelectronic timer built on purpose governs the whole camera system.\\n43\\nFigure 12. Epipolar lines in a trifocal system\\nThe two right cameras A and B are close to each other (2.5m), so that the stereo disparity is quite\\nsmall (right panels). For this reason we can assume that the matching between A and B is done.\\nConsider point 1A and its correspondent point 1B: these points generate two epipolar lines (1A\\nand 1B) on the left photo C. The correspondent point on C must lie along both epipolar lines, so\\nthat it must be located at the crossing point of these two lines. As usual, due to noise, the actual\\nposition of the correspondent point on C will be slightly off the estimated one. However, there will\\nbe very few points around the crossing of the two epipolar lines, thus making the matching\\nrelatively easy.\\n44\\nFigure 13. Pattern matching\\nThe human brain performs the matching by detecting similar, weakly distorted patterns on the two\\nphotos, and then identifying the vertexes of the patterns. The figure shows a close-up of a real\\nflock from two stereo cameras with a 25m baseline.\\n45\\nFigure 14. Match zero\\nThe pattern of neighbours around a given point is only weakly deformed in the left and right\\nphoto. In particular, if all neighbours were at the same distance from the cameras as Frank (the\\ncentral point), there would be no deformation at all of the pattern. Once the two centres, FA and\\nFB, are shifted to a common reference frame (lower panel), the pairs of neighbours’ images have\\na small relative shift, which is proportional to the depth difference between the neighbour and the\\ncentre. If focal planes are nearly parallel and they are shifted along the x direction, then also this\\nresidual shift is mainly along the x direction. Thus, correspondent neighbours are likely to be\\nlocated within a small and elongated box of size bx x by, with bx >> by. If this happens for many\\npairs of neighbours, then the overlap of the two patterns is large (see text).\\n\"),\n",
       " Document(metadata={'Published': '2008-02-12', 'Title': 'The STARFLAG handbook on collective animal behaviour: Part II, three-dimensional analysis', 'Authors': 'Andrea Cavagna, Irene Giardina, Alberto Orlandi, Giorgio Parisi, Andrea Procaccini', 'Summary': 'The study of collective animal behaviour must progress through a comparison between the theoretical predictions of numerical models and data coming from empirical observations. To this aim it is important to develop methods of three-dimensional (3D) analysis that are at the same time informative about the structure of the group and suitable to empirical data. In fact, empirical data are considerably noisier than numerical data, and they are subject to several constraints. We review here the tools of analysis used by the STARFLAG project to characterise the 3D structure of large flocks of starlings in the field. We show how to avoid the most common pitfalls i the quantitative analysis of 3D animal groups, with particular attention to the problem of the bias introduced by the border of the group. By means of practical examples, we demonstrate that neglecting border effects gives rise to artefacts when studying the 3D structure of a group. Moreover, we show that mathematical rigour is essential to distinguish important biological properties from trivial geometric features of animal groups.', 'entry_id': 'http://arxiv.org/abs/0802.1674v1', 'published_first_time': '2008-02-12', 'comment': 'To be published in Animal Behaviour', 'journal_ref': 'Animal Behaviour 76 (1), 237-248 (2008)', 'doi': None, 'primary_category': 'q-bio.QM', 'categories': ['q-bio.QM', 'cond-mat.stat-mech', 'q-bio.PE'], 'links': ['https://arxiv.org/abs/0802.1674v1', 'https://arxiv.org/pdf/0802.1674v1']}, page_content='The STARFLAG handbook on collective animal behaviour:\\nPart II, three-dimensional analysis\\nANDREA CAVAGNA1,2, IRENE GIARDINA1,2, ALBERTO ORLANDI1, GIORGIO\\nPARISI1,3, ANDREA PROCACCINI1,3\\n1 Centre for Statistical Mechanics and Complexity (SMC), CNR-INFM\\n2 Istituto dei Sistemi Complessi (ISC), CNR\\n 3 Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’\\nCorrespondence to Andrea Cavagna and Irene Giardina:\\nSMC, CNR-INFM, Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’, Piazzale\\nAldo Moro 2, 00185 Roma, Italy\\nandrea.cavagna@roma1.infn.it\\nirene.giardina@roma1.infn.it\\nAlberto Orlandi:\\nSMC, CNR-INFM, Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’, Piazzale\\nAldo Moro 2, 00185 Roma, Italy\\nGiorgio Parisi and Andrea Procaccini:\\nDipartimento di Fisica, Universita’ di Roma ‘La Sapienza’, Piazzale Aldo Moro 2,\\n00185 Roma, Italy\\nAbstract\\nThe study of collective animal behaviour must progress through a comparison between\\nthe theoretical predictions of numerical models and data coming from empirical\\nobservations. To this aim it is important to develop methods of three-dimensional (3D)\\nanalysis that are at the same time informative about the structure of the group and\\nsuitable to empirical data. In fact, empirical data are considerably noisier than numerical\\ndata, and they are subject to several constraints. We review here the tools of analysis\\nused by the STARFLAG project to characterize the 3D structure of large flocks of\\nstarlings in the field. We show how to avoid the most common pitfalls in the quantitative\\nanalysis of 3D animal groups, with particular attention to the problem of the bias\\nintroduced by the border of the group. By means of practical examples, we demonstrate\\nthat neglecting border effects gives rise to artefacts when studying the 3D structure of a\\ngroup. Moreover, we show that mathematical rigour is essential to distinguish important\\nbiological properties from trivial geometric features of animal groups.\\nKeywords: Collective animal behaviour; bird flocking; fish schooling; 3D statistics.\\nCollective animal behaviour is a fascinating phenomenon that has interested scientists\\nfor a long time (see, for a review, Krause & Ruxton 2002). How does global co-\\nordination emerge in a flock of thousands birds wheeling over the roost? How do fish in\\na school organize in a milling pattern? Are these self-organized phenomena, or is there a\\ncentralized control mechanism? What is the evolutionary function of collective\\nbehaviour? What are the common features in different species? What is the precise\\nnature of the interaction among the individuals?\\nThe best way to tackle these questions is through a feedback between theoretical\\nmodels and empirical observations. However, obtaining three-dimensional (3D) data\\nabout large groups of animals in the field has been very difficult up to now, and for this\\nreason the vast majority of studies focused on 3D numerical models. There has been\\nlittle space for a comparison between experiments and theory. In fact, comparisons have\\nmostly been made between different kinds of models (Parrish et al. 2002), rather than\\nbetween models and empirical data. The consequence of this fact is that most of the\\nquantitative tools normally used to characterize collective animal behaviour are tailored\\nto numerical models, and in some cases are unsuitable to 3D empirical data sets. Many\\nof the parameters that are varied when studying a numerical model, cannot be changed\\nduring empirical observations. The typical example is the amount of noise each\\nindividual is subject to when deciding its direction of motion: this is just an easy knob to\\nturn in a numerical model, but it is definitely not possible to do so for empirical\\nobservations. Even those quantities that do change in the field, as group size, velocity,\\nnearest neighbour distance and so on, unfortunately do not change on the observer’s\\ndemand. Moreover, empirical data are necessarily noisier than numerical data, and this\\nmay force us to discard some quantities that are on the other hand useful when studying\\nmodels.\\nRecently, the STARFLAG project developed some new techniques able to\\nreconstruct the 3D positions of individual birds in large flocks of starlings in the field\\n(Cavagna et al. 2008). The empirical methods introduced by the STARFLAG project can\\nbe used in many other instances of collective animal behaviour, and it is therefore\\nreasonable to expect that a new generation of 3D data is about to come. In these\\nconditions, it will be possible to make a direct comparison between empirical data and\\nnumerical models. Such a comparison will be based on the measurement of well-defined\\nquantities in the two different data sets, empirical and theoretical. Such quantities must\\nbe at the same time informative about the properties of the group, and suitable to\\nempirical data. The aim of this work is to present in full detail the quantitative tools of\\nanalysis that have been applied to the STARFLAG data set (Ballerini et al. 2008a, b).\\nWe stress that these tools can be used on any 3D data set, including those coming from\\nnumerical models. We hope this is a step towards standardizing the 3D methods of\\nanalysis in collective animal behaviour, making thus smoother the comparison between\\ntheory and experiments.\\nThe paper is divided into two parts. In the first part we will deal with the problem\\nof the bias introduced by the individuals at the border of the group. There are well-\\nknown statistical methods to solve this problem. Such methods, however, may not be\\nfamiliar to a biological audience, and thus we will review these methods in a very\\npedagogical way. In the second part of the paper we will present the STARFLAG tools\\nfor the analysis of 3D animal groups. As physicists, we have a very different background\\nto most researchers working in collective animal behaviour, and we think that a cross-\\nfertilization of different concepts and tools may be useful. In this second part we will\\nalso illustrate, by using practical example, how the border bias can completely modify a\\nmeasure, and even introduce some artefacts that can be mistakenly interpreted as real\\nbiological features.\\nThe Problem of the Border\\nOne of the major problems we face when analysing the 3D structure of a group of\\nanimals is that of the border. Whatever species is observed, animals on the border of the\\ngroup will inevitably introduce a bias into any measurements taken, unless they are\\nproperly treated. This problem has been mostly disregarded by previous empirical\\nobservations and numerical studies alike. It is only with a very large number of points,\\nsay beyond 106 in 3D, that the ratio between surface and volume becomes small enough\\nthat border effects become irrelevant. Such numbers are far beyond the reach of\\nempirical observations, and difficult to attain even in numerical simulations. Thus,\\nborder effects must adequately be taken into account in any 3D analysis.\\nTo explain in what consists the problem of the border, let us start with a simple\\nexample. Consider a school of fishes circling clockwise around an empty core, a pattern\\nknown as milling (Fig.1). Now imagine that one wishes to compute the spatial\\ndistribution of nearest neighbours. The (many) individuals on the external border lack\\nneighbours to their left, whereas the (few) individual on the internal border (the core)\\nlack neighbours to their right. If individuals on the border are included in the analysis,\\none obtains a distribution indicating that fishes have, on average, fewer neighbours to\\ntheir left. This result is not, of course, a general consequence of the local interaction\\nrules among the fish: in fact, the completely opposite result would be obtained for a\\nschool milling counter-clockwise! This is a typical case where disregarding border\\neffects results in the conflation of two levels of analysis that should remain separate;\\nspecifically, the morphological level (the mill-like shape of the school), and the\\nbehavioural level (individual interactions and nearest neighbour distribution). This\\nproblem is always present, whatever the shape of the group, and it can bias our analysis\\nheavily if it is not treated properly. Fortunately the problem is well known in statistics\\nand there are standard methods to cope with it (Stoyan & Stoyan 1994).\\nIn order to cure the bias of the border, the first essential step is to define, or\\nrecognize, those animals that belong to the border of the group. Moreover, identifying\\nthe border is also crucial to measuring the volume, and thus the density, of the group. In\\ngeneral, defining the volume of an irregular and discrete set of points is not a trivial\\nproblem. However, we often want to measure the volume and density, since many\\nbiological issues are related to these quantities. In this section we will explain how it is\\npossible to define the border in a rigorous way, and how to take care of the bias it\\nintroduces.\\nBefore we begin, we should note one very important fact. As we have said, the\\nweight of the border in a group is proportional to the ratio between surface and volume.\\nThis ratio scales as 1/N1/3 with the number N of animals in the group: for small groups\\nthis ratio is large, and relatively many animals belong to the border, whereas in very\\nlarge groups only a minority of animals is on the border.  For example, in a flock of\\nabout 20 birds in 3D, all birds belong to the border. In a flock of 100 birds, only ~3 birds\\nare at the interior of the flock.  Thus, working with small groups is very inconvenient,\\nand rather uninformative, because, once the birds on the border have been eliminated\\nfrom the analysis, no animals are left with which to build statistics. Small groups are\\ncompletely dominated by border effects, and it is therefore very difficult to get unbiased\\nresults out of them. For this reason, collecting data on large groups was one of the main\\nobjectives of the STARFLAG project. Of course, this does not mean that smaller groups\\ncannot be studied: many animals live in small groups, and it is certainly useful to study\\nthem. However, in these cases one should be particularly careful about the problem of\\nthe border, and realize that not all quantities can be measured in an unbiased way.\\nConvex Hull, Non-Convex Groups and a-Shapes\\nThe simplest tool to define the border of a discrete arrangement of points is the so-called\\nConvex Hull (CH). As its name suggests, the CH is an algorithm that gives the list of\\npoints that belong to the convex envelope of the original group (see Fig. 2), either in two\\nor in three dimensions. Apart from giving the border’s points, the algorithm also\\nprovides the volume of the hull, so that an estimate of the density can readily be\\ncalculated. The great advantage of the CH method is that it does not need any adjustable\\nparameters in order to get results and it is, therefore, fast and unambiguous. Moreover,\\nthere are several ready-to-use CH routines available on the Internet.\\nThe downside of the method is, of course, that animal groups are not\\nnecessarily convex. For example, starling flocks often display concavities in their shape.\\nA school of milling fishes is another obvious example. In non-convex cases, the CH is\\nunable to detect the concavities and its volume includes substantial regions void of\\npoints (see Fig. 2). This is very bad for the analysis and, in particular, it introduces a\\nsignificant bias into all indicators related to the volume and density of the group, and to\\nnearest neighbour distance. The calculated volume may be considerably larger than the\\nreal one, so that density will be badly underestimated, and the average nearest neighbour\\ndistance overestimated.\\nWe thus need a method that can detect concavities in an aggregation. The first\\nthing to understand when analysing this problem is that there is no absolute way to\\ndefine the non-convex border of a set of points. Rather, we need to specify the minimum\\nsize, or radius R, of the concavities we want to eliminate from the surface of the\\naggregation. Consider Fig. 2: there are two very obvious concavities, one with radius R1\\nand one with radius R2< R1. In order to define a non-convex border, we have to specify\\nwhether we want to ‘carve’ out of the surface of the smaller concavity alone, or\\neliminate both of them.\\nOnce we have specified the minimum size of the concavity we want to detect,\\nwe can use an algorithm called a-Shape  (AS in the following). The basic idea of the AS\\nalgorithm (Edelsbrunner & Mucke 1994) is to carve the set of 3D (2D) points with\\nspheres (discs) of radius, R, where the scale, R, is a parameter specified by the user.\\nWhenever the surface (perimeter) of a sphere (disc) hits three (two) points, these points\\nare included in the border. No points are permitted to be within the carving sphere (disc).\\nIn this way, all concavities larger than R are detected (see Fig. 2), and the effect is to\\nobtain the exact set of points belonging to the non-convex border on scale R. Note that\\nthe convex hull can be obtained as a limiting case of the AS, simply by setting the radius\\nR to infinity (Fig. 2).\\nFixing the Scale of the Concavities\\nThe good thing about the AS algorithm is that it is very intuitive to use. The\\nonly problem is how to choose the scale, R, of the concavities. Clearly, too large a value\\nwill neglect some concavities, whereas too small a value will make the spheres penetrate\\ninto the aggregation, with the rather odd result that all points will belong to the border.\\nHow do we choose R? One possibility is to use a biological criterion: for example, R\\nshould be significantly larger than the nearest neighbour distance, otherwise the group\\nfragments into many sub-parts. However, these kinds of criteria are normally not\\nsufficient, and they can only provide some guidelines. A less ambiguous way must thus\\nbe found.\\nA good way to fix R is to check the resulting density of the aggregation as a\\nfunction of R. Let B be the number of points belonging to the border and I the number of\\npoints in the interior of the aggregation, such that B+I=N, the total number of points.  Let\\nV be the volume contained within the border defined by the AS algorithm. When R is\\ndecreased, more points are included in the border, so that B grows while I decreases. At\\nthe same time the volume, V, decreases, because more concavities are carved out of the\\nsurface. If we plot the density of internal points, I/V, as a function of R, we discover\\nsomething interesting: whenever R becomes smaller than the typical scale of a concavity,\\nthe internal density makes a sudden jump, because whenever we cut a bona fide\\nsuperficial void, the volume decreases much more steeply than the number of internal\\npoints (Fig. 3). From this plot, we can obtain a very clear idea of the sizes R1< R2< R3<…\\nof the concavities in the aggregation, and we can accordingly fix R=R1, i.e. to the\\nsmallest genuine concavity. This was the STARFLAG method to fix R.\\nEven though measuring the internal density as a function of R for all 3D\\ngroups in the data set may be time consuming, there are no better alternatives. One point\\nworth noting, however, is that, most of the time, the range of possible values of R can be\\nreduced drastically by using biological constraints and experience, so that trying a\\nhandful of values of R is normally good enough to eliminate the most obvious\\nconcavities. Moreover, in many instances, the shape of an animal group changes rather\\nsmoothly across time. This means that, in a given time series, R does not change sharply,\\nso that it can be updated after a long interval. However, whenever the density of the\\ngroup shows some unexpected or sudden variation, it is always wise to check whether or\\nnot this is due to a concavity, whose size is below the chosen value of R.\\nHow to Cure the Border’s Bias\\nOnce we have defined animals belonging to the border, we must treat them properly, in\\norder to avoid border effects in our final measurement.  In statistics, this amounts to\\ncomputing  ‘edge-corrected’ estimators for the observables one is interested in  (for a\\ndeeper perspective on this problem see Stoyan  & Stoyan 1994).\\nThe basic origin of the border’s bias is that border points have on average a\\nvery different neighbours statistics, compared to inner points. This is simply due to the\\nfact that a large proportion of the space around them is empty. This is why, for example,\\nthe typical distance of the nearest neighbour of a point on the border is significantly\\nlarger than the average nearest neighbour distance of points in the interior. Moreover, as\\nthe milling fish example illustrates, border points carry a strong signature of the group’s\\nshape, which will bias all orientation quantities. Neighbour-based analyses enter to many\\ndifferent observables, which is why border effects are so widespread and so dangerous.\\nThe most elementary method for dealing with border points is the following:\\nconsider them as neighbours of inner points, but discard them as focal (or reference)\\npoints (Fig. 4). For example, assume that point P1 belongs to the border and that it is at\\ndistance 0.8m from point P2, which is an inner point. Assume also that P2 is the first\\nnearest neighbour of P1, and that P1 is the second nearest neighbour of P2 (nearest\\nneighbour relationships are not necessarily symmetric). Then, the value 0.8m is included\\nin the statistics of second nearest neighbours (when considering P2 as focal point), but it\\nis not included in the statistics of first nearest neighbours. This method is quite rough,\\nbecause it very crudely separates those points that belong to the border from those that\\ndo not. In reality, points that are very close to the border without, however, belonging to\\nit, may show biased statistics as well. This is not taken into account by the method just\\ndescribed.\\nA more precise technique was proposed by Hanisch (1984) in the context of\\nthe nearest neighbour distance statistics (see also Stoyan & Stoyan 1994). In order to use\\nthis method we must be able to compute, for each given point i, the distance between\\nthis point and the border, let us call it Di. This is defined as the distance between i and\\nthe closest geometric face of the border (Fig. 4). Consider a neighbour k of point i and\\ncompute their mutual distance dik. The Hanisch method states that if dik> Di, then point k\\nmust not be included in the statistics of the neighbours of i (Fig. 4). The basic idea is that\\nif k is more distant from i than the border, then their mutual neighbouring relationship is\\nbiased by the presence of the border. This method is very simple to employ, it gives very\\ngood results, and can be safely used for most observables. When it comes to the\\ndistribution of nearest neighbour distances, however, this method slightly overweights\\nvery close neighbours (see later). This small problem can be cured by appropriately\\nreweighting all distances. This is the weighted Hanisch method (Hanisch 1984; Stoyan\\n& Stoyan 1994), which was the method used by STARFLAG.  Both the simple and the\\nweighted Hanisch estimators are proven to be asymptotically (i.e. for large number of\\npoints) unbiased estimators for the nearest neighbour distance distribution. The results\\nobtained with the different methods are given below.\\nAlthough correcting for border effects is quite technical and time consuming, it\\nis very important to understand that disregarding these effects often produces disastrous\\nand misleading results. It is impossible to overemphasize this point. In order to illustrate\\nthe dangers related to this problem, and how it can produce some very nasty artefacts,\\nwe provide some examples of border bias later in the paper.\\nWhat to Measure and How to Do It\\nIn this section we list some useful observables, and illustrate some of the subtle technical\\npitfalls one may encounter. It is clear, however, that the decision of what to measure\\ndepends on the scientific aims of a study. In no way do we wish to suggest here that\\nthere is a unique, nor a best, way to perform the analysis of 3D data.\\nAngular Distributions and the problem of the Jacobian\\nOne of the most interesting things to observe in an animal group is the average angular\\ndistribution of nearest neighbours with respect to the direction of motion. Neighbour\\norientation has been used in the past to characterize 3D fish schools (Cullen et al. 1965;\\nPartridge et al. 1980), and, more recently, it has been used in 3D bird flocks as an\\nindirect way to study the range of the interaction among the individuals (Ballerini et al.\\n2008a). In both fish and birds, the clearest indicator of a non-trivial spatial structure of\\nindividuals was directly encoded in the angular distribution of neighbours.\\nThe measurement proceeds as follows. We identify the nearest neighbour of a\\nfocal individual, and, irrespective of the distance between the two, we define the nearest\\nneighbour’s vector, u. We also define the vector of the velocity V. In the following, both\\nu and V will be normalized, unitary vectors. Note that the velocity, V, can be either the\\nactual velocity of the focal individual, or the global (centre of mass) velocity:\\nfluctuations in the orientation of the individuals are normally so small that this does not\\nmake any practical difference for what follows. Then we measure the orientation of the\\nnearest neighbour’s vector u with respect to the velocity V. There are two ways of doing\\nthis, and we explain both of them below. The important point, however, is that the\\npractical outcome of measuring the orientation of u with respect to the V is an angle (or\\na quantity directly related to it). Such angle is then measured for all focal points in the\\ngroup, and the probability distribution (i.e. the normalized frequency) of this angle is\\ncalculated. Finally, this probability distribution must be compared to some null case, and\\nthe simplest one is the case where there is no structure at all, and the arrangement of\\npoints is completely isotropic in space. This is the case of a spatially random set of\\npoints (spatial Poisson process). A significant difference between the measured\\nprobability and the Poissonian case indicates that there is non-random, anisotropic\\nstructure in the group.\\nAs we said, there are two different ways to measure the orientation of the\\nnearest neighbour’s vector u, with respect to the velocity, V. First, we can consider the\\nangle, q, between these two directions, typically by computing the inner (or scalar)\\nproduct between these two unitary vectors,\\ncos(q) = Vxux + Vyuy + Vzuz      .    (1)\\nThe angle q is sometimes mistakenly called ‘bearing’ angle, but this is inaccurate. We\\nshall see below that the correct bearing angle is, in fact, defined differently. The crucial,\\nand sometimes misleading, point about q is that, even in a completely random\\narrangement of 3D points, its probability distribution is not constant. This is due to the\\npresence of a Jacobian factor (in fact, a determinant) in the transformation between\\nCartesian and spherical coordinates (Apostol 1969). To understand the origin of the\\nJacobian factor in the probability distribution of q, let us consider a sphere of radius one,\\nwith the unitary vector V corresponding to the north pole. Now consider the circle\\ndefined by all points on the sphere that lie at an angle q with V. The size of such circle is\\nproportional to sin(q): this is the Jacobian factor. The Jacobian is simply telling us that,\\neven in the random case, where the sphere is uniformly covered by points, there are\\nmany more points on the equator (q=p/2, sin(q)=1) compared to the poles (q=0, q=p, \\nsin(q)=0). The probability distribution of q in the random case is thus given by:\\nprandom(q) = 1\\n2 sin(q)     ;                     q Œ [0 :p]         ,      (2)\\nwhere sin(q) is the Jacobian. It is therefore highly inconvenient to compare the measured\\ndistribution of q with the random one, since this is not a constant. The comparison\\nbetween two non-constant and certainly noisy functions can be difficult.\\nThe best thing to do is to deal directly with the cosine of q, cos(q)=y. Indeed,\\nthe distribution of y, unlike that of q, is flat, since there are two Jacobian factors that\\ncancel each other out:\\nprandom(y) = prandom(q) dq\\ndy = 1\\n2 sin(q)\\n1\\nsin(q) = 1\\n2      ,       (3)\\nwith:\\n \\ny = cos(q) Œ [-1:1]    ,       (4)\\nand it can be conveniently compared to the empirical data. In fact, in many biological\\ncases p(y) is non-constant, signifying a non-isotropic arrangement of neighbours.\\nA different, and more complete, way to characterize the orientation of the\\nnearest neighbour vector u in space is to fix a 3D reference frame, and to associate two\\nangles to the nearest neighbour vector u, rather than simply one. To explain how this is\\ndone, we take birds as a reference system. First, we consider the plane containing the\\nvelocity V and the wings of the focal bird, i.e. the coronal plane (Fig. 5). Second, we\\nconsider the direction orthogonal to the coronal plane, i.e. the dorsal-ventral direction:\\nwe call G the unit vector along such direction. (In most situations, and especially in fish\\nand non-turning birds, this direction is almost parallel to gravity, but this is not essential\\nfor its definition). Now we can define two angles (Fig. 5): the bearing angle, a, is the\\nangle between the velocity, V, and the projection of u onto the coronal plane; the\\nelevation, f, is the angle between u and its projection onto the coronal plane.  It is\\nimportant to understand that the bearing angle, a, is different from the angle q described\\nabove. In fact, unlike q, the probability distribution of the bearing angle is constant,\\nprandom(a) = 1\\n2p          ;                      a Œ [-p :p]        .   (5)\\nOn the other hand, the distribution of the elevation angle f contains a Jacobian, and it is\\ntherefore not a constant:\\nprandom(f) = 1\\n2 cos(f)          ;                f Œ [-p /2 :p /2]         .     (6)\\nThe bottom line of this section is that, whenever we want to investigate the angular\\ndistribution of neighbours (nearest neighbours, but also second nearest, third nearest and\\nso on), by plotting the probability distribution of some quantity, we must make sure to\\nchoose a quantity whose distribution is constant in the trivial random case. The best\\nchoice is either cos(q), where q is the angle between velocity and neighbour, or the\\nbearing angle a , which is the angle between the velocity and the projection of the\\nneighbour’s direction onto the coronal plane. Both these quantities have a constant\\nprobability distribution, so that any non-constant trend of their distribution is a clear\\nindication of non-isotropic structure of neighbours.\\nQuantifying the Anisotropy\\nAll empirical studies that have investigated the orientation of neighbours in animal\\ngroups have detected a strong anisotropic structure (Cullen 1965; Partridge et al. 1980;\\nBallerini et al. 2008a, b). This means that the direction of motion breaks the rotational\\nsymmetry of the group’s structure. In other words, by taking the velocity as the reference\\ndirection, animals are located with respect to one another in some preferential direction.\\nThe angular distributions discussed above show this fact very clearly. However, it is\\nsometimes useful to quantify the strength of this anisotropy by means of a single scalar\\nquantity, rather than a graph, or a function. To this end, STARFLAG introduced a novel\\ntool, the anisotropy factor g (Ballerini et al. 2008a).\\nGiven a certain group, consider the set of all nearest neighbour vectors,\\n{ui}i=1…N where N is the number of individuals, and each vector is normalized to unity.\\nFor each one of these vectors, ui, we can build the following projection matrix, \\nMi =\\nux\\niux\\ni\\nux\\niuy\\ni\\nux\\niuz\\ni\\nuy\\niux\\ni\\nuy\\niuy\\ni\\nuy\\niuz\\ni\\nuz\\niux\\ni\\nuz\\niuy\\ni\\nuz\\niuz\\ni\\nÊ \\nË \\nÁ \\nÁ \\nÁ \\nˆ \\n¯ \\n˜ \\n˜ \\n˜ \\n    or, equivalently,       Mab\\ni\\n= ua\\ni ub\\ni    ,     (7)\\nwhere a,b=x,y,z. The effect of applying this matrix to an arbitrary vector v is to produce\\na new vector, which has the same direction as ui and modulus equal to the projection of\\nv along the direction of ui. Thus, the matrix projects along the direction of ui. Indeed,\\nMab\\ni\\nvb\\nbÂ\\n= ua\\ni\\nub\\ni vb\\nbÂ\\n         ;                         a,b = x,y,z      .    (8)\\nFor this reason Mi is called projection matrix. We can now average this matrix over all\\nneighbours in the system, to obtain a matrix that projects along the average direction of\\nthe nearest neighbour:\\nMab = 1\\nN\\nua\\ni ub\\ni\\ni=1\\nN\\nÂ\\n       .        (9)\\nThe important point here is that the three eigenvectors of the average projection matrix,\\nM, correspond to the most relevant directions in space. In particular, the eigenvector\\ncorresponding to the smallest eigenvalue of M indicates the direction along which there\\nis the smallest number of nearest neighbours vectors, and it therefore indicates the\\ndirection along which, on average, one is less likely to find an animal’s nearest\\nneighbour. Let us call this (unitary) eigenvector, w. In a random, isotropic arrangement\\nof points, w lies in a random direction: there will always be a direction along which, by\\nrandom fluctuations, the density of nearest neighbours will be slightly smaller than the\\naverage. Therefore, in an isotropic group, we do not expect w to have any particular\\ncorrelation with the velocity V. Conversely, whenever there is some anisotropic, non-\\nrandom structure of neighbours, the minimal crowding direction w will have a\\npreferential direction with respect to the global velocity V. This can be quantified by\\nmeasuring the square inner product between these two vectors,\\ng =\\nVa wa\\naÂ\\nÊ \\nË \\nÁ \\nˆ \\n¯ \\n˜ \\n2\\n       ;                       g Œ [0 :1]       .             (10)\\nThis is the anisotropy factor: a value of g close to 1 indicates that the direction of\\nminimal crowding of the nearest neighbours is almost parallel to the velocity. A value of\\ng close to 0 indicates that the direction of minimal crowding is orthogonal to the velocity\\n(this does not imply that the velocity is the direction of maximal crowding). To fix a\\nscale, let us compute the value of g for the case where there is no correlation whatsoever\\nbetween the two directions. To average g in the random case we can use spherical\\ncoordinates, with the velocity V along the z-axis,\\ng random = 1\\n4p\\ndx dy dz\\nsphereÚ\\n(V ⋅w)2 = 1\\n4p\\ndj\\ndq sin(q) cos2(q) =\\n0\\np\\nÚ\\n0\\n2p\\nÚ\\n1\\n3        (11)\\nThus, if we measure g  many times for two uncorrelated directions, we get, on average,\\nthe value 1/3. This value can be taken as a benchmark for a complete decorrelation. Of\\ncourse, even in the random case, in a single instance, the value of g can take any positive\\nvalue; however, if we average it over many instances, we will get 1/3, signifying that\\nthere is no anisotropy in the system. On the contrary, if g is significantly larger than the\\nisotropic, random value of 1/3, then there is correlation between global velocity and\\ndirection of minimal neighbour crowding. Within STARFLAG, we discovered that g is\\nsystematically larger than 1/3 for nearest neighbours, indicating that there is a\\nsignificantly lower probability to find nearest neighbours along the direction of motion\\n(Ballerini et al. 2008a).\\nThe useful thing about the anisotropy factor, g, is that it can be calculated as\\neasily for nearest neighbours, as for second nearest neighbours, third nearest neighbours,\\nand so on. Of course, in doing this, one should always be very careful to take care of\\nborder effects, and use the Hanisch method to eliminate this bias. We calculated the\\nanisotropy factor as a function of the distance of the neighbour from the focal individual,\\nand we found that g decreases with this distance. This allowed us to define a range of the\\nanisotropy, as the point where g becomes close to 1/3. This range can be computed either\\nin units of metres (metric distance), or in units of individuals (topological distance).\\nWhat we found that, for starlings, the most appropriate distance measure is topological,\\nnot metric: the topological range is the same in flocks with different densities, whereas\\nthe metric range depends on density, scaling linearly with the average nearest neighbour\\ndistance (Ballerini et al. 2008a).\\nNote that the anisotropy factor we have defined refers to the direction of\\nminimal crowding. Of course, one can also consider the direction of maximal crowding,\\nby computing the square inner product between the velocity and the eigenvector relative\\nto the largest eigenvalue. In the case of starlings, the direction of maximal crowding,\\nunlike the minimal crowding direction, does not have any clear correlation with velocity.\\nFor this reason, we defined the anisotropy factor using the direction of minimal\\ncrowding. However, it may be that, in different systems, the direction of maximal\\ncrowding gives a sharper description of the group’s anisotropy. In general, one should\\nalways compute the inner products of the velocity with all three eigenvectors of the\\nanisotropy matrix.\\nNearest Neighbour Distance and Exclusion Zone\\nThe second most investigated observable, which is not related to orientation properties,\\nis nearest neighbour distance (NND); in particular its probability distribution. Apart\\nfrom its obvious biological significance, this function is very useful for providing a\\nquantitative estimate of the so-called “exclusion zone” around each individual.\\nAlmost all models of collective animal behaviour assume that, surrounding\\neach individual, there is a zone where other individuals are not found. This can be\\nmodelled in different ways, either with a strong short-range repulsion, giving rise to\\nwhat is known as a soft-core, or alternatively it can be modelled with a bona fide hard-\\ncore, i.e. an impenetrable region surrounding each individual. (In this last case, if we call\\nr0 the hard-core, the minimum distance between points is, of course, 2r0.) The existence\\nof an exclusion zone is a reasonable biological assumption, and one would like to\\nmeasure the radius of this zone in real biological systems and then feed this value into\\nmodel simulations. This can be done using the distribution of nearest neighbour\\ndistances.\\nThe first thing to note here is that, like everything else, real exclusion zone\\nsizes fluctuate across individuals. This means that, in order to obtain the typical value of\\nthe exclusion zone,  r0.  we cannot simply measure the minimum nearest neighbour\\ndistance, dmin, within a large group and claim that this is twice the exclusion zone: the\\nvalue of dmin invariably refers to a pair of animals that are extremely close to each other,\\ndue to some rare fluctuation. In other words, in real biological groups, the exclusion\\nzone is not strictly hard, so we cannot simply measure the minimum distance.\\nA simple way to find the typical size of the exclusion zone is to fit the nearest\\nneighbour distance distribution using the exact equations describing the same quantity\\nfor an arrangement of totally impenetrable hard spheres (“hard-spheres”). Hard spheres\\nare a simple and popular test-bed system in physics: the only interaction they have with\\neach other is due to their hard exclusion zone of radius r0, i.e. the hard-core. Thus, they\\nbehave very much like a set of billiard balls hitting each other. Of course this is a quite\\ncrude approximation of animal groups. However, the hard-spheres system has the great\\nvirtue of simplicity, with two important by-products: first, a hard-sphere system is\\ndescribed by only two parameters, i.e. density r and hard-core r0; second, there are\\nnumerous exact results for hard-spheres, that can be directly compared to measured\\nquantities. Therefore, as long as we use hard-spheres only to describe the short distance\\nproperties of the group, and in particular to fit the exclusion zone, we should not be\\nconcerned about oversimplifying our analysis.\\nRather than dealing with the nearest neighbour distance distribution, P(r), it is\\nmore convenient to use the cumulative distribution, P>(r), defined as the probability that\\ntwo nearest neighbours have mutual distance larger than r. The former is simply the\\nderivative of the latter. Note that, by definition, P>(r)=1 for r < 2r0 in hard-spheres. For\\nr > 2r0 the exact expression of P>(r) in a hard-spheres systems with density r and hard-\\ncore radius r0, is given by the following (equation 5.96 of Torquato 2002):\\nP>(r) = exp -A\\nr\\n2r0\\nÊ \\nË Á \\nˆ \\n¯ ˜ \\n3\\n-1\\nÈ \\nÎ \\nÍ \\nÍ \\n˘ \\n˚ \\n˙ \\n˙ \\n+ B\\nr\\n2r0\\nÊ \\nË Á \\nˆ \\n¯ ˜ \\n2\\n-1\\nÈ \\nÎ \\nÍ \\nÍ \\n˘ \\n˚ \\n˙ \\n˙ \\n- C\\nr\\n2r0\\nÊ \\nË Á \\nˆ \\n¯ ˜ -1\\nÈ \\nÎ \\nÍ \\n˘ \\n˚ \\n˙ \\nÏ \\nÌ Ô \\nÓ Ô \\n¸ \\n˝ Ô \\n˛ Ô \\n,        (12)\\nwhere,\\nA = 8 f(1+ 4f)     ,\\n \\nB =18 f 2       ,\\nC = 24 f 3     ,\\nand,\\nf = 4\\n3 p r0\\n3 r    ,          (13)\\nis the so-called packing fraction, i.e. the ratio between the total volume occupied by the\\nspheres and the volume of the group. The expressions of the constants A, B, and C are\\ngiven at the first order in f, and thus are valid only in the limit of small packing\\nfractions, which is often verified in many animal groups (for example, in starling flocks;\\nBallerini et al. 2008b). Equation (12) can then be used to fit the cumulative nearest\\nneighbour distribution (in fact, it is technically more convenient to fit the logarithm of\\nP>(r)). The parameters of the fit are the density r and the radius of the exclusion zone r0.\\nNote that a random set of points with given density (spatial Poisson point process) can\\nbe obtained in the obvious limit r0=0, so that formula (12) can also be used to fit the data\\nto the Poisson case, where the cumulative distribution takes the simpler form\\nP>(r)=exp{- (4/3) pr r3}. Finally, we note that the hard-core fit can also be used to give\\nan estimate of the compactness of the aggregation. In order to do so, one has to look at\\nthe packing fraction, f ,of the aggregation: small values of f correspond to sparse\\nsystems, while large values indicate compact ones. As reference values, for hard spheres\\nthe crystalline arrangement corresponds to f = 0.49, and the random close packing to f ~\\n0.79 (Torquato 2002).\\nIn Ballerini et al. (2008b) we present data for the nearest neighbour\\ndistribution for starling flocks, and show that a hard-spheres fit is significantly better\\nthan the Poisson case fit. In particular, it is evident that the empirical distribution has a\\nlack of nearest neighbours at very short distances, compared to the Poisson. Clearly,\\nhowever, the hard-sphere fit is, generally speaking, far from perfect, because, as already\\nmentioned, the exclusion zone is not strictly hard. However, it does a much better job\\nthan the Poissonian case, so that, as a first approximation it can be used as a reliable\\nestimate of r0. More refined models can be used, of course, with the advantage of\\nincreased realism, but the disadvantage of a larger number of parameters: as always, one\\nmust carefully balance the trade-off between simplicity and realism.\\nWhat Happens When the Border is Disregarded: Instructive Examples\\nIn this section, we will present three practical examples of how disregarding the border\\ncan produce completely misleading results. In all cases. we consider aggregations of\\npoints that are randomly generated in 3D with a given density: what is known as a\\nspatial Poisson point process (for a more complete description of Poisson point fields see\\nStoyan & Stoyan 1994). The Poisson point process is the prototype for a spatially\\nhomogeneous and isotropic arrangement of points. All observables are in this case\\nknown analytically, so that any measurement taken must be consistent with the exact\\nformulas. For this reason, Poissonian aggregations represent an ideal null model with\\nwhich to test our analysis tools.\\nIn previous studies, both empirical and simulations, the average NND of\\nanimal groups is reported as decreasing with an increasing number of individuals within\\nthe group (see, for example, Partridge et al. 1980). This implies that animals are more\\nclosely packed in a large aggregation than in small ones. STARFLAG did not find any\\nevidence of this behaviour in our large starling flocks (Ballerini et al. 2008b). In fact, we\\nbelieve that this result is a pure artefact due to a border bias. In Figure 6, we plot the\\naverage NND of a spherical aggregation of random points vs. the number of points N.\\nAll aggregations of different sizes are generated with the same density, and thus they\\nshould all display the same NND. However, as we have already noted, points on the\\nborder have, on average, more distant nearest neighbours for purely geometric reasons,\\nso that they bias the NND towards larger values if they are included in the statistics. We\\ndid not compensate for this border effect in Fig. 6, and we computed the NND using all\\npoints: the net effect is that NND strongly decreases with N, at least for values of N\\nsmaller than 100. Only for very large values of N is the surface-to-volume ratio small\\nenough to suppress the bias introduced by the border points, and the NND converges to\\nits correct value. Thus, this packing effect as a function of aggregation size has no\\nbiological origin. It is simply an artefact of failing to account for the bias introduced by\\nborder points.\\nAs a second example, we consider the cumulative distribution of the nearest\\nneighbour distance, P>(r). For a Poisson aggregation, the exact value is given by\\nequation (12), with f=r0=0, that is P>(r) = exp {- (4/3) pr r3}. In Fig. 7 we plot the\\nlogarithm of P>(r) as a function of r. It is clear that including border points in the\\ncalculation gives a very bad estimate of P>(r), compared to the exact case. The rough\\nmethod for eliminating border points (described above) significantly improves the\\nestimate. The simple Hanisch method improves the empirical estimate even further, and\\nwith the weighted Hanisch method one gets an excellent consistency with the exact\\nresult.\\nAs a final example, we can show that the orientational properties of groups can\\nbe badly biased by border effects. We considered a Poisson point process of random\\npoints generated within a relatively thin 3D slab, with an aspect ratio similar to a typical\\nstarling flock (1:3:7). We put this in motion first along its shortest axis, and then along\\nits longest axis (Fig. 8). Given that the points are random, there should be no correlation\\nbetween the velocity and the eigenvectors of the anisotropy matrix, so that by repeating\\nthe numerical experiment many times we should find that the anisotropy factor g = 1/3,\\nnot only for the first nearest neighbours, but also for the second, third and so on.\\nHowever, when border effects are disregarded, this is not the case (Fig. 8), simply\\nbecause, depending on the direction of motion, border points will have more or less\\nneighbours in the direction of the velocity. Only by using the Hanisch method can we\\nrecover the exact value of 1/3. As with milling fish schools, a failure to account for\\nborder points means that, effectively, we are conflating two very different results:\\nstructure (the fact that there is no preferential directions of neighbours) and morphology\\n(the fact that the slab has very uneven dimensions). These two levels, however, must be\\nkept separated.\\nA final remark is in order. We do not claim here that border effects are\\nexclusively produced by geometry (or physics), and not by biology. In fact, it is\\nreasonable to expect that individuals at the border of the group do behave differently\\ncompared to others, for a number of biological reasons. It is exactly to detect in the\\nclearest possible way these biological effects, however, that one must first get rid of the\\ntrivial geometric effects. If this is not done, important biological properties can be\\nhidden or distorted by trivial geometric features.\\nMain Axes, Dimensions and Aspect Ratios\\nEven though this work was dedicated to obtaining and analysing the 3D positions of\\nindividual animals within a group, there are numerous global quantities that are worthy\\nof investigation. In particular, the morphological properties of a group and its orientation\\nin space with respect to gravity and velocity are of obvious relevance.\\nThe first step is to define the main axes of the group. Our method is simple.\\nWe start by fitting the 3D group of points to a 2D plane, let us call it P; the direction\\northogonal to this plane defines the first axis, I1. We then project all points onto the plane\\nP, and we fit such a 2D set of points to a line, let us call it l; the line belonging to P and\\northogonal to l defines the second axis I2. Finally, l itself defines the third axis I3. This\\nis a set of orthogonal directions in space, and its definition simply requires a linear fit,\\nand it is thus very fast. Of course, if the group is spherical, the two fits are ill defined, so\\nthat both the plane and the line will be arbitrarily defined. This result is, in fact, correct,\\nbecause in the case of a sphere there are no preferential axes, and one can choose an\\narbitrary set of orthogonal vectors.\\nWhenever the typical shape of a group is non-spherical, it is important to\\ncheck whether there is any particular orientation in space. There are two obvious\\npreferential directions in 3D animal groups, namely gravity G and velocity V, and it is\\nthus natural to measure the inner products between the three axes I1 I2 I3, and both G and\\nV. For example, in starling flocks the shortest axis, I1, is always more or less parallel to\\ngravity and orthogonal to the velocity, whereas the axes I2 and I3 are approximately\\northogonal to gravity, but show no particular correlation with the velocity. Finally, it is\\ninteresting to measure the angle between gravity and velocity: intuitively we expect\\nanimals to fly, or swim, while remaining as level as possible, in order not to waste\\npotential energy. This is confirmed in starlings by the fact that velocity and gravity are\\nalways quite orthogonal to one another. Of course, the situation may be different under\\nexternal stimuli, especially predation, when the mutual orientation of gravity and\\nvelocity may change drastically.\\nThe second important global observable is the dimension of the group along\\neach axis: I1 is the smallest dimension, I2 the intermediate one, and I3 the largest\\ndimension, I1< I2< I3. There are several ways to do this, the easiest of which is to use an\\nalgorithm that finds the Minimal Bounding Box (MBB) of the set of points (Fig. 9a).\\nNote that the naive bounding box, computed using a set of arbitrary Cartesian axes is\\nabsolutely unreliable (Fig. 9b). The lengths of the sides of the MBB give a reasonable\\nestimate of the group’s dimensions for convex shapes. For consistency, one must check\\nthat the main axes defined above are similar to the orientations of the MBB.\\nUnfortunately, whenever the shape of the group is non-convex, the MBB gives\\nan inaccurate estimate of the dimensions (Fig. 9c). Non-convex animal groups are not at\\nall uncommon, and they are frequent among starling flocks. Thus, within STARFLAG,\\nwe used a different method to define these dimensions, which requires knowledge of the\\na-shape. The idea here is that the smallest dimension of a set of 3D (2D) points is given\\nby the diameter of the largest sphere (disc) entirely contained in the set (Fig. 9d). This\\nprinciple can be applied provided that we are able to distinguish between what is within\\nthe group from what is outside the group, and this can only be achieved once the a-\\nshape algorithm has computed the faces of the polyhedron, i.e. the surface of a group.\\nWe then proceed in the following way: we compute the smallest distance between each\\ninner point and the surface: this is the radius of the largest sphere centred on that point\\nand contained entirely in the group. We then select the largest among these smallest\\ndistances, and use twice this value as an estimate of I1. We then project all points onto\\nthe plane P defined above, and find the minimum dimension I2 of such a 2D group using\\nthe same principle (with discs, in place of spheres). We finally project the 2D group onto\\nthe direction I3 and identify the dimension I3 as the maximal extension along this line.\\nThe dimensions along the three axes give a measure of the aspect ratio of the\\ngroup. For example, if the group has an approximately spherical shape the three axes\\nhave similar values, I1~I2~I3. In starlings, on the other hand, we found that I1 is always\\nconsiderably shorter than the other two dimensions (Ballerini et al. 2008b). To quantify\\nthe aspect ratios we simply measure the two major dimensions in units of the smallest\\none. Thus, for starlings we say that, on average, the aspect ratio is 1:3:6, meaning that\\nI2/I1= 3 and I3/I1= 6. A very interesting result in starling flocks is that, even though the\\ndimensions of a group may vary considerably, its aspect ratio is relatively stable,\\nsuggesting that there is a mechanism operating within flocks that produces an almost\\ninvariant aspect ratio (Ballerini et al. 2008b). Similar results have been found in fish\\nschools (Partridge et al. 1980). It would be very interesting to investigate aspect ratios in\\nother species.\\nConclusions\\nIn all the methods discussed in this work we put the emphasis on the mathematical rigour.\\nThe reason why we did that is not simply because we are physicists, nor because we think\\nthat mathematics, or physics, must be predominant in the study of collective animal\\nbehaviour. On the contrary, we believe that in order for the fundamental biological\\nproperties of animal groups to emerge in the clearest way, it is necessary to wipe out a\\nmeasurement from the trivial geometrical effects. As we explained through several\\npractical examples, a lack of mathematical rigour can lead to erroneous conclusions, or,\\nin the best case scenario, make our measurements very difficult.\\nLet us go back to the nearest-neighbour distance. We have proved that, even in a\\ncompletely random arrangement of points, when border effects are neglected the nearest\\nneighbour distance shows a strong dependence on the group size. The fact that this\\ndependence is present in the random case proves that its origin is exclusively\\ngeometrical. Such effect is thus biologically spurious, and it must be taken away in order\\nto appreciate the true biological aspects. Note that, by doing this, we are by no means\\nexcluding that there may be also a biological dependence of nearest-neighbour distance\\non group’s size. However, to investigate the biological contribution it is first essential to\\nget rid of the purely geometrical contribution. An identical logical pattern emerges with\\nthe problem of the Jacobian: the angular distributions do have a nontrivial shape that has\\na deep biological origin and meaning. In fact, this is one of the most relevant features of\\nanimal groups. However, the Jacobian factor, if disregarded, makes it very difficult to\\ndistinguish what is a biological feature (present only in animal groups, and therefore\\nimportant), from what is a purely geometrical feature (present also in random groups, and\\ntherefore irrelevant).\\nOur approach is therefore not motivated by a pedantic, and pointless, obsession\\nfor the mathematical details. We know only too well that empirical data are rough, noisy,\\nscarce, and very difficult to obtain. In fact, empirical data are pure gold in the field of\\ncollective animal behaviour. Mathematical rigour is necessary to extract from such data\\nthe maximum amount of biological information.\\nAcknowledgements\\nWe warmly thank Frank Heppner for many useful suggestions and for carefully reading the manuscript.\\nThe STARFLAG handbook was conceived during the participation of one of us (A.C) to the 2007\\nInternational Ethological Conference in Halifax, Nova Scotia. A.C. wants to thank the participants to this\\nconference for many stimulating discussions and for the exciting scientific environment. This work was\\nfinanced by a grant from the European Commission under the FP6-STARFLAG project.\\nBIBLIOGRAPHY\\n Apostol, T. M. 1969. Calculus, 2d ed., volume 2. Waltham, Massachusetts: Blaisdell.\\nBallerini, M., Cabibbo, N., Candelier, R., Cavagna, A., Cisbani, E., Giardina, I.,\\nLecomte, V., Orlandi, A.,  Parisi, G., Procaccini, A., Viale, M., & Zdravkovic, V.\\n2008. Interaction Ruling Animal Collective Behaviour Depends on Topological rather\\nthan Metric Distance: Evidence from a Field Study. Proceedings of the National\\nAcademy of Sciences USA 105, 1232-1237.\\nBallerini, M., Cabibbo, N., Candelier, R., Cavagna, A., Cisbani, E., Giardina, I.,\\nOrlandi, A.,  Parisi, G., Procaccini, A., Viale, M., & Zdravkovic, V. 2008. Empirical\\ninvestigation of starling flocks: a benchmark study in collective animal behaviour.\\nAnimal Behaviour, in press.\\nCavagna, A., Giardina, I., Orlandi, A.,  Parisi, G., Procaccini, A., Viale, M., &\\nZdravkovic, V. 2008. The STARFLAG handbook on collective animal behaviour: Part\\nI, empirical methods. Animal Behaviour, in press.\\nCullen, J. M., Shaw, E. & Baldwin, H.B. 1965. Methods for measuring the three-\\ndimensional structure of fish schools. Animal Behaviour 13, 534-543.\\nEdelsbrunner H. and Mucke E. P. 1994. Three-dimensional alpha shapes.  ACM\\nTransactions on Graphics 13, 43-72.\\nHanisch, K.-H. 1984. Some remarks on estimators of the distribution function of\\nnearest-neighbour distance in stationary spatial point patterns. Statistics 15, 409-412.\\nKrause, J. & Ruxton, G.D. 2002. Living in Groups. Oxford: Oxford University Press.\\nParrish, J. K., Viscido, S.V. & Grunbaum, D. 2002. Self-organized fish schools: an\\nexamination of emergent properties. Biological Bullettin 202, 296-305.\\nPartridge, B. L., Pitcher, T., Cullen, J. M. & Wilson, J. 1980. The three-dimensional\\nstructure of fish schools. Behavioral Ecology and Sociobiology 6, 277-288.\\nStoyan, D.  & Stoyan, H. 1994. Fractals, random shapes and point fields. Chichester:\\nJohn Wiley and Sons Ltd.\\nTorquato, S. 2002. Random Heterogeneous Materials. New York: Springer-Verlag.\\nFIGURES\\nFigure 1. Border bias in a milling configuration\\nThe yellow area represents a school of fish in a milling, or toroidal, configuration: all animals\\nrotate clockwise around an empty core. Animals on the external border lack neighbours on their\\nleft, whereas animals on the internal border lack neighbours on their right. However, animals on\\nthe external border are much more numerous than animals on the internal border. As a\\nconsequence, if we include all animals in the statistics of nearest neighbours, we find that on\\naverage animals have a lower probability to have their nearest neighbour to their left. This result\\nwould be the opposite for a school rotating counter-clockwise, showing that it is not a\\nfundamental feature of the inter-individual interaction, but rather an artefact due to the undue\\nmixture of structural (nearest neighbour location) and morphological (toroidal shape) results.\\nFigure 2. Convex hull and alpha shape\\nThis 2D aggregation of points displays two clear concavities of different sizes. The convex hull of\\nthe aggregation includes all points of the convex envelope, and it is by construction unable to\\ndetect the concavities. Thus, the convex hull badly overestimates the volume occupied by the\\naggregation, introducing thus a strong bias in the density of points. Moreover, such a convex\\nborder would be unable to properly cure the border effects, since many points effectively on the\\nborder are in fact considered as internal points. The alpha shape carves the aggregation with\\ndiscs of a certain radius (fixed by the user): when a disc touches two points, they are included in\\nthe border. In this way the alpha shape detects concavities that have roughly the same size as\\nthe chosen radius of the discs.\\n Figure 3. Fixing the scale of the concavities\\nA 2D aggregation of random points with two clear concavities with different sizes R1 =3 and R2=8.\\nWhen we plot the density of internal points as a function of the radius R used by the alpha shape\\nalgorithm, we find two sudden jumps in correspondence of the radius of the two concavities. In\\nthis way it is possible to fix the proper value of R right below the value of the smallest concavity.\\n Figure 4. Methods for curing the border’s bias\\nPoints on the border have a modified statistics of their neighbours. In particular, the nearest\\nneighbour of a border point is, on average, at a larger distance compared to points at the interior.\\nThis is simply due to the fact that half the space around a border point is empty, so that it has\\nfewer neighbours in general. The most basic method to cure the border’s bias consists in not\\nusing as focal points all points belonging to the border (red in the figure). This means, for\\nexample, that when we compute the distances of the neighbours of point 1 (focal point) we can\\ninclude point B; however, we must not include the distances of the neighbours of point B (focal\\npoint) in the global statistics. A more reliable technique is the Hanisch method: given a focal point\\n(1 in the figure), we compute its distance to the border, D1, and only include in the statistics those\\nneighbours of 1 that are closer than D1. Therefore in the neighbours’ statistics of focal point 1 we\\ninclude 2, but not 3. The idea is that on the scale of the distance 1-2, point 1 is for all practical\\npurpose an inner point, whereas on the scale of the distance 1-3, point 1 is influenced by the\\npresence of the border.\\n Figure 5. Definition of bearing and elevation\\nThe coronal plane includes the velocity V and, in the case of birds, the wings. The orthogonal\\ndirection G to this plane normally is very close to gravity, even though this is inessential to the\\ndefinition of the reference frame. The nearest neighbour vector u is projected onto the coronal\\nplane: the angle a between this projection and the velocity defines the bearing angle. The angle\\nF between u and its projection onto the coronal plane defines the elevation angle. A third angle,\\nnot to be confused with bearing nor elevation, is the angle q between u and V (not indicated in the\\nfigure).\\nFigure 6. Border artefact I: NND vs. number of points\\nWe generated random aggregations with increasing number of points N using a Poisson point\\nprocess. All aggregations have a 3D spherical shape, and they are all generated with the same\\ndensity r=1, corresponding, by construction, to an average nearest neighbour distance (NND)\\nequal to 0.554 (for the exact relation between density and average NND in a Poisson point\\nprocess see Stoyan & Stoyan 1994). In other words, the ratio between the number of points and\\nthe volume of the sphere within which they are randomly drawn is the same for all sizes. We\\nmeasure the NND by averaging over all the points of the aggregation, as normally done in the\\nliterature for biological groups (from 100 to 10000 samples for each value of N; error bars\\nrepresent standard error). For small groups size the resulting NND is strongly biased to larger\\nvalues. The net effect is that, due to the border’s bias, the NND decreases as a function of N, and\\nonly for very large aggregations it reaches the asymptotic correct value. In particular, for N < 100\\n(the typical range of previous empirical studies) there is no hint of an asymptotic value. The\\ndecrease of the NND with N has nothing to do with any biological feature. It is simply an artefact\\nof having included the border’s points in the statistics.\\nFigure 7. Border artefact II: distribution of nearest neighbours\\nWe generated spherical 3D aggregations of 1000 random points using a Poisson point process,\\nand we compute the cumulative probability of the nearest neighbour distance, P>(r), i.e. the\\nprobability that the NND is larger than r. In the Poissonian case the logarithm of P>(r) is a power\\nlaw with exponent 3 (see text), so that if we plot P>(r) vs. r3 in semilog scale the exact result\\ncorresponds to a straight line. The black full line is the exact analytic case, which must be\\nrecovered by the empirical measure. The blue circles are what we get when we include all points\\nin the statistics, disregarding border effects. The deviation from the exact case is substantial. The\\ngreen squares are obtained by using the most basic method of including only internal points (see\\ntext). This improves a lot the result, even though a significant deviation still persists, especially in\\nthe tails. The light blue triangles are obtained by using the simple Hanisch method. The estimate\\nis better than in the previous case, but one can note a tendency to underestimate the distances.\\nThe red diamonds are obtained by using the weighted Hanisch method. The result in this case is\\nin very good agreement with the exact curve. Each curve is obtained by averaging over 100\\nindependent samples; error bars represent standard errors.\\n Figure 8. Border artefact III: anisotropy of a moving slab\\nAnisotropy factor as a function of the order of the neighbour (nearest, second nearest, third\\nnearest, etc.). We generate 1200 random points within a 3D slab of aspect ratio 1:3:7 (similar to\\nrealistic flocks of stalings). The  spatial distribution of points is random so that there is no intrinsic\\nanisotropy in the distribution of nearest neighbours. We then put in motion the slab along one of\\nits three axes with a velocity V and compute the anisotropy factor g as a function of the order of\\nneighbours. If border effects are disregarded, the value of g strongly depends on the direction of\\nmotion. When the slab moves along on of the shortest axis (case 1), the majority of border points\\nlack neighbours along the direction of motion; as a consequence g is larger than the random case\\n1/3 (red circles). On the contrary, when the slab moves along the longest axis (case 2), the\\nmajority of border points lack neighbours in the directions perpendicular to V; as a consequence g\\nis smaller than the random case 1/3 (blue squares). None of these two results is correct, since\\nthey both mix some morphological information (the slab proportions) to structural information (the\\nanisotropy factor). When the border’s bias is properly cured by using the Hanisch method, the\\nexact value 1/3 is recovered (black diamonds). Points are averaged over 50 samples; error bars\\nrepresent standard error.\\nFigure 9. Definition of an aggregation’s dimensions\\nThe Minimal Bounding Box (MBB) (a) gives a reliable definition of the dimensions when the\\naggregation is convex. An arbitrary Cartesian bounding box, on the other hand, is in general\\ncompletely unreliable (b). For non-convex aggregations the MBB fails to detect the real\\ndimensions. For example, in the figure it overestimates the aggregation’s thickness, i.e. the\\ndimension of the shortest axis (c). In order to give an accurate measure of thickness one needs to\\nmeasure the diameter of the largest sphere entirely contained in the aggregation (d).\\n'),\n",
       " Document(metadata={'Published': '2008-02-12', 'Title': 'An empirical study of large, naturally occurring starling flocks: a benchmark in collective animal behaviour', 'Authors': 'Michele Ballerini, Nicola Cabibbo, Raphael Candelier, Andrea Cavagna, Evaristo Cisbani, Irene Giardina, Alberto Orlandi, Giorgio Parisi, Andrea Procaccini, Massimiliano Viale, Vladimir Zdravkovic', 'Summary': 'Bird flocking is a striking example of collective animal behaviour. A vivid illustration of this phenomenon is provided by the aerial display of vast flocks of starlings gathering at dusk over the roost and swirling with extraordinary spatial coherence. Both the evolutionary justification and the mechanistic laws of flocking are poorly understood, arguably because of a lack of data on large flocks. Here, we report a quantitative study of aerial display. We measured the individual three-dimensional positions in compact flocks of up to 2700 birds. We investigated the main features of the flock as a whole - shape, movement, density and structure - and discuss these as emergent attributes of the grouping phenomenon. We find that flocks are relatively thin, with variable sizes, but constant proportions. They tend to slide parallel to the ground and, during turns, their orientation changes with respect to the direction of motion. Individual birds keep a minimum distance from each other that is comparable to their wingspan. The density within the aggregations is non-homogeneous, as birds are packed more tightly at the border compared to the centre of the flock. These results constitute the first set of large-scale data on three-dimensional animal aggregations. Current models and theories of collective animal behaviour can now be tested against these results.', 'entry_id': 'http://arxiv.org/abs/0802.1667v1', 'published_first_time': '2008-02-12', 'comment': 'To be published in Animal Behaviour', 'journal_ref': None, 'doi': None, 'primary_category': 'q-bio.QM', 'categories': ['q-bio.QM', 'cond-mat.stat-mech', 'q-bio.PE'], 'links': ['https://arxiv.org/abs/0802.1667v1', 'https://arxiv.org/pdf/0802.1667v1']}, page_content='1\\nAn empirical study of large, naturally occurring\\nstarling flocks: a benchmark in collective animal\\nbehaviour\\nMICHELE BALLERINI1,2, NICOLA CABIBBO3,4, RAPHAEL CANDELIER3,\\nANDREA CAVAGNA1,5, EVARISTO CISBANI2, IRENE GIARDINA1,5, ALBERTO\\nORLANDI1, GIORGIO PARISI1,3, ANDREA PROCACCINI1,3, MASSIMILIANO\\nVIALE3 & VLADIMIR ZDRAVKOVIC1\\n1Centre for Statistical Mechanics and Complexity (SMC), CNR-INFM; 2Istituto\\nSuperiore di Sanita’(ISS); 3 Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’;\\n4Istituto Nazionale di Fisica Nucleare, 5Istituto dei Sistemi Complessi (ISC), CNR;\\nCorrespondence to:   \\nIrene Giardina\\nSMC, CNR-INFM, Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’,\\nP.le Aldo Moro 2, 00185 Roma, Italy\\nirene.giardina@roma1.infn.it\\n2\\nM. Ballerini, A. Cavagna, A. Orlandi, A. Procaccini, V. Zdravkovic:\\nSMC, CNR-INFM, Dipartimento di Fisica, Universita’ di Roma ‘La Sapienza’,\\nP.le Aldo Moro 2, 00185 Roma, Italy\\nN. Cabibbo, G. Parisi:\\nDipartimento di Fisica, Universita’ di Roma ‘La Sapienza’, P.le Aldo Moro 2,\\n00185 Roma, Italy\\nE. Cisbani:\\nISS, viale Regina Elena 299, 00161 Roma, Italy\\nM. Viale (current address):\\nDipartimento di Fisica, Universita’ di Roma 3, via della Vasca Navale 84, 00146\\nRoma, Italy   \\nR. Candelier (current address):\\nGIT / SPEC / DRECAM, Bat. 772, Orme des Merisiers, CEA Saclay, 91191 Gif\\nsur Yvette, France\\n3\\nAbstract:\\nBird flocking is a striking example of collective animal behaviour. A vivid\\nillustration of this phenomenon is provided by the aerial display of vast flocks of\\nstarlings gathering at dusk over the roost and swirling with extraordinary spatial\\ncoherence. Both the evolutionary justification and the mechanistic laws of flocking are\\npoorly understood, arguably because of a lack of data on large flocks. Here, we report a\\nquantitative study of aerial display. We measured the individual three-dimensional\\npositions in compact flocks of up to 2700 birds.  We investigated the main features of\\nthe flock as a whole - shape, movement, density and structure - and discuss these as\\nemergent attributes of the grouping phenomenon. We find that flocks are relatively thin,\\nwith variable sizes, but constant proportions. They tend to slide parallel to the ground\\nand, during turns, their orientation changes with respect to the direction of motion.\\nIndividual birds keep a minimum distance from each other that is comparable to their\\nwingspan. The density within the aggregations is non-homogeneous, as birds are packed\\nmore tightly at the border compared to the centre of the flock. These results constitute\\nthe first set of large-scale data on three-dimensional animal aggregations.  Current\\nmodels and theories of collective animal behaviour can now be tested against these\\nresults.\\nKeywords: collective behaviour; flocking; Sturnus vulgaris; emergent properties.\\n4\\nThe aerial display of large flocks of birds is a stunning example of collective\\nbehaviour in animal aggregations (Emlen 1952). A paradigmatic case is provided by\\nEuropean starlings (Sturnus vulgaris) (Feare 1984). These birds can be observed in\\nmany cities, where they establish their roosting sites. Shortly before sunset starlings\\nreturn to their roost and, prior to retiring for the night, they form sharp-bordered flocks,\\nranging from a few hundred to tens of thousands of birds, which wheel and turn over the\\nroosting site until darkness falls. Flocks exhibit strong spatial coherence and are capable\\nof very fast, highly synchronized manoeuvres, either spontaneously, or as a response to\\npredator attacks.  Many features of bird flocking are present in other instances of\\ncollective animal behaviour. Fish schools, mammal herds and insect swarms represent\\nother examples of animal aggregations that have fascinated biologists for many years\\n(Gueron et al. 1996; Parrish & Edelstein-Keshet 1999; Krause & Ruxton 2002; Couzin\\n& Krause 2003).  Like starlings, individuals form cohesive groups that are able to\\nsustain remarkable coordination.\\nDiverse instances of collective behaviour are found in many different fields of\\nscience, from the spontaneous ordering of magnetic moments in  physics (see, e.g.,\\nCardy 1996), the coordination of an ensemble of artificial agents with distributed\\nintelligence in robotics(Cao et al. 1997; Jadbabaie et al. 2003), the emergence of\\nherding behaviour in financial markets in economics (Cont & Bouchaud 2000), to the\\nsynchronized clapping in a concert hall (Neda et al. 2000; Michard & Bouchaud 2005)\\nor the Mexican wave in a stadium (Farkas et al. 2002). In all these examples, collective\\nbehaviour emerges as the result of the local interactions between the individual units,\\nwithout the need for centralized coordination. The tendency of each agent to imitate its\\nneighbours (allelomimesis), can, by itself, produce a global collective state. Whenever\\nthis happens, we are in the presence of self-organized collective behaviour.\\nAlthough self-organization is undoubtedly a general and robust mechanism, its\\nuniversality is an open issue. In physics, for example, universality is a well-defined\\nconcept: the same model and theory can be used to describe quantitatively very different\\nphysical systems, provided that they all share the same fundamental symmetries. The\\nsituation is more complicated in biology because the individuals that form a group are\\nmuch more complex than particles or spins. For example, despite the fact that fish\\nschools and bird flocks behave similarly, certain collective patterns are present in one\\ncase and not in the other (Krause & Ruxton 2002).  At some level, the specificities of\\nthe individuals and of the environment must make a difference. Therefore, in view of\\nthe highly interdisciplinary nature of self-organized collective behaviour, it is important\\nto distinguish the general from the specific.\\nModels play a crucial role in this respect. Indeed, it was modelling exercises that\\nrevealed the general principles of how collective behaviour can emerge from self-\\norganization. When it comes to modelling real instances of collective behaviour\\nhowever we need to be more detailed. In this case, models must specify the minimal\\nconditions necessary to reproduce the empirical observations, so that we can distinguish\\nbetween general phenomena and those specific to the system.\\nThe field of collective animal behaviour boasts a wealth of models (Aoki 1982;\\nHeppner & Grenader 1990, Reynolds 1987; Huth & Wissel 1992; Couzin et al. 2002;\\nInada & Kawaki 2002,  Kunz & Hemelrijk 2003; Vicsek et al. 1995; Gregoire & Chaté\\n2004). Some of these were developed for fish schools, some for bird flocks, and some\\n5\\nwith a non-specific biological target. In all cases, however, the models agree on three\\ngeneral behavioural rules: move in the same direction as your neighbours; remain close\\nto them; avoid collisions. These rules are modelled using three distinct contributions to\\nthe interaction among the individuals, i.e. alignment of velocities, attraction and short-\\nrange repulsion. In all cases, the models produce cohesive aggregations that look\\nqualitatively similar to natural cases. However, each model has its own way of\\nimplementing the rules, dictated by the differing opinions as to which are the relevant\\nconditions, and by the different biological targets (e.g. fish vs. birds). Of course,\\nselection among different models can only be achieved by comparing their results with\\nempirical evidence. Only empirical observations can tell us whether or not the collective\\nproperties of a model are in quantitative agreement with the natural case. Moreover, the\\nfeedback between models and empirical data must confirm whether or not a certain\\ncondition is truly necessary to reproduce a specific biological feature.\\nEmpirical data, then, are necessary both as a crucial input of the modelling\\napproach, and as a quantitative guideline for answering more fundamental questions\\nabout groups, their global features and biological function (see e.g. Parrish & Hammer\\n1997). Unfortunately, 3D data on even moderately large groups of animals are very hard\\nto obtain, and quantitative empirical data are scarce and limited to very small groups (a\\nfew tens of individuals). Testing of the models has therefore been sporadic so far. At the\\nsame time, speculation on the microscopic origin and biological function of collective\\nbehaviour has outgrown empirical groundwork.\\nEmpirical 3D studies on fishes have been performed in laboratory tanks (Cullen et\\nal. 1965; Partridge et al. 1980; Partridge 1980; Van Long et al. 1985; Tien at al. 2004).\\nData for birds, in contrast, have been obtained in field observations (Miller and Stephen\\n1966; van Tets 1966; Major & Dill 1978; Pomeroy & Heppner 1992; Budgey 1998).\\nThese past studies have however two major limitations: the number of individuals is\\nsmall (limited to few tens) and the group arrangements were loose, at variance with the\\nhuge, highly cohesive groups characteristic of collective behaviour. Both these\\ndrawbacks stem from a single technical problem: in order to reconstruct the 3D position\\nof an object, all optical techniques (stereometry, orthogonal method, shadow method)\\nrequire different images to be placed in correspondence (i.e. to be matched, see\\nFig.1a,b) (Osborn 1997; Hartley & Zisserman 2003). For large and compact sets of\\nfeatureless points, this problem is so severe that it has been suggested that these\\ntechniques are fundamentally inadequate to handle 3D biological scenes (Aloimonos et\\nal. 1991).\\nUsing statistical physics, optimization theory and computer vision techniques we\\nhave managed to solve the correspondence problem. We developed an experimental\\ntechnique capable of reconstructing the individual 3D positions in cohesive\\naggregations of several thousands of animals in the field (see Cavagna et al. 2008a). We\\nused this technique to collect quantitative empirical data on large flocks of starlings\\nduring aerial display. In this paper, we present quantitative and systematic data on the\\ntwo main attributes of the groups: global properties (shape, size, orientation and\\nmovement) and internal structure (density profile and neighbours distributions).  Our\\naim in doing so is two-fold:\\n6\\ni. We wish to provide a detailed analysis of the mechanistic laws of flocking, at\\nthe global and structural level. This enables us to set a new empirical benchmark for\\ntesting existing models of self-organized collective behaviour.\\nii. \\nWe wish to characterize the attributes of flocks as emergent properties of\\nthe grouping phenomenon. To this end, we attempt to place our results in the context of\\nthe biological function of grouping, individual fitness consequences, interaction with the\\nenvironment and mutual interaction between individuals.\\nMETHODS\\nBoth the experimental technique employed to obtain  3D data and the statistical\\ntools used to analyse the data are reported in detail elsewhere (Cavagna et al. 2008 a,b).\\nIn the present section, therefore, we give only a brief summary of the most essential\\npoints.\\nLocation and Materials\\nLarge colonies of European Starlings (Sturnus vulgaris) spend the winter in\\nRome, Italy, where several roosting sites are located within the city urban area.  In our\\nanalysis we do not consider migrating flocks, or travelling flocks, i.e. flocks that move\\nfrom feeding to roosting site. Instead, we focus only on those engaged in aerial display:\\nshortly before dusk, very cohesive flocks of starlings swirl over the roost wandering in a\\nwide but confined area. We took digital images of starling aerial display at the roosting\\nsite of Termini railway station, between December 2005 and February 2006, from the\\nterrace of Palazzo Massimo, Museo Nazionale Romano. The apparatus was located 30m\\nabove ground level. Wind speed never exceeded 12ms-1. Average distance of birds from\\nthe cameras was 100m. We used Canon EOS 1D Mark II digital cameras (3504×2336\\npixels), mounting Canon 35-mm lenses. The aperture was set between f2.0 and f4.0;\\nshutter speed between 1/1000s and 1/250s; ISO between 100 and 800; and cameras’ tilt-\\nup between 35% and 40%.\\nExperimental technique.\\nWe used stereo photography to collect data on flocks (Longuet-Higgins 1981;\\nOsborn 1997). More specifically, we employed the trifocal technique (Hartley &\\nZisserman 2003), with three different points of view. The distance between the two\\nstereo cameras (the baseline) was d=25m. Given two targets at 100m from the\\napparatus, with a relative distance of 1m, our tests gave an error of δz<0.04m on the\\nrelative distance, and an error of Δz<0.92m on the absolute distance of the targets. The\\nerror is thus below 5% for both the relative and absolute distance. Our apparatus shot\\nseries of photographs at a rate of 10fps (frames-per-seconds), for a maximum of 8\\nseconds (80 consecutive photographs). After that, the cameras’ buffer filled up and we\\nhad to wait a few seconds before another series of photographs could be taken. If the\\nflock under consideration exited from the field of view of any of the cameras, the series\\nwas interrupted. The algorithms used to solve the correspondence problem and perform\\n7\\nthe 3D reconstruction are described in Cavagna et al. (2008a). In the cases we analysed,\\nwe reconstructed on average 88% of the birds, and never less than 80% of the flock.\\nEvents selection\\nA flocking event was defined as a series of up of 80 consecutive photographs shot\\nat 10fps of a single flock. We collected on average 15 events every session, and we thus\\ngathered approximately 500 flocking events over 40 sessions. Not all of these events,\\nhowever, could be processed. The vast majority of them were not in the common field\\nof view of all cameras, a necessary condition to perform the 3D reconstruction. In\\naddition, the distance of the flock had to be smaller than 250m (due to our photographic\\nresolution); the total number of objects in the photograph had to be smaller than 8000\\nbirds (due to software constraints); and exposure and contrast had to be appropriate.\\nOnly 50 events met these criteria. We then selected 20 of these, choosing flocks with\\nsharp borders, strong spatial cohesion and a large number of birds (N>400). Finally, our\\nmatching algorithm put a limit to the flock’s maximum density on the photographs,\\nleaving only 10 events suitable for analysis. Using synthetic data, we checked that the\\nreconstruction software did not introduce any significant bias in the flocks’ shape and\\nstructure.\\nBiological target of investigation\\nQualitatively, we could distinguish between two classes of aerial display. Some\\nflocks flew very high above the roost (>200m), were typically very large (tens of\\nthousands of birds) with a columnar-like shape and performed the most striking changes\\nin formation patterns. These flocks were visible from very far away, due to their size,\\naltitude and change in density. It has been suggested that these flocks may act as beacon\\nto signal the location of the roost to co specifics (Feare 1984). These groups were too\\nfar away to allow us to use our reconstruction procedure effectively, as optical\\nresolution is the main bottleneck. Moreover, the pattern of these flocks seems to be\\nspecific to starlings, whereas we were interested in more general features of collective\\nbehaviour. Therefore, we do not address these kinds of flocks here. The other kind of\\nflock flew at a lower altitude (<100m), right over the roost. They were usually smaller\\nin size, ranging from a few hundred to several thousand birds and were often very\\ncompact, with sharp-bordered edges. These flocks performed a random walk above the\\nroost, keeping the same global features for quite long time scales. In other words, they\\nexhibited a form of steady-state behaviour, which made them a good target for statistical\\ninvestigation.  These flocks are the ones we were able to reconstruct accurately and the\\npresent study therefore focuses on the properties of these flocks.\\nMethods of analysis\\nThe reconstruction provided us with the 3D coordinates of each individual bird in\\nthe flock. The next step was to identify the border of the flock. Standard tools, such as\\nthe convex hull, are inadequate given the presence of concavities at different scales.\\nRather, we used the α-shape algorithm (Edelsbrunner & Mucke 1994), a method that\\nallows the pinpointing of concavities of the boundary down to a given scale α.  This\\n8\\nalgorithm identifies a non-convex border that appropriately follows the external\\nconcavities and faithfully reproduces the external shape. This procedure allowed us to\\nidentify birds that belonged to the border and all the others were therefore identified as\\n‘internal’ birds. Typically, birds on the border, or close to it, have statistical properties\\nthat are different from internal birds. When computing structural features (like\\nneighbour distribution, average distances and so on) one needs to take into account\\nthese boundary effects in order to produce unbiased data. In all our analysis, we used\\nappropriate techniques to deal with this problem, as explained in Cavagna et al. (2008b).\\nOnce the border was defined, the volume of the flock was computed via the\\nDelaunay triangulation, identifying the sub-ensemble of the triangulation internal to the\\nborder.  Flocks typically have asymmetric shapes: they are thinner in one direction and\\nmore extended along the other two. To characterize this shape quantitatively, we\\ndefined the three main dimensions and axes of the flock (see Cavagna et al. 2008b). The\\nshortest dimension, the thickness I1, was defined as the diameter of the largest sphere\\ncontained within the flock’s boundary. We then fitted a plane to the flock and defined\\nthe axis orthogonal to this plane as the yaw axis relative to I1. We refer to this axis by\\nmeans of a unit vector I1 parallel to it. Since the flock extends in space mostly\\northogonally to I1, we could exploit the fitted plane to identify the two largest\\ndimensions. The flock was then projected onto this plane. Following a similar strategy\\nas before, we defined the second dimension I2 as the diameter of the smallest circle\\ncontained in flock’s projection on the plane.  We then fitted the 2D projection of the\\nflock to a line, which identified the axis I3 along the flock and the relative largest\\ndimension I3. The orthogonal direction to I3 on the plane defined the axis I2 relative to\\nthe intermediate dimension I2. The three unitary axes I1, I2 and I3 are by construction\\northogonal to each other and can be used to describe the orientation of the flock in\\nspace.  For example, the main axis I3 represents the direction along which the flock is\\nmore extended: by measuring the mutual orientation of I3 with the velocity we can\\nidentify whether the flock is elongated along the direction of motion.\\nThe three dimensions I1, I2 and I3 give a quantitative description of the flock\\nshape. For example, if I1, I2 and I3 assume values comparable one to the other this means\\nthat the flock has an approximately symmetrical shape (in the case of a sphere or of a\\ncube, one finds I1= I2 = I3). If I1 is much smaller than I2 and I3 then the group is very\\nthin along the smallest axes and extends more perpendicularly to it. From these\\nexamples it is clear that what matters are the relative and not the absolute values of\\nthese dimensions. For this reason we considered ratios between the dimensions in our\\nanalysis (see Results).\\nRESULTS\\nMorphology\\nA typical example of a flock, together with its 3D reconstruction, is shown in\\nFig.1 a-f.  By rotating the point of view, the striking thinness of the flock is immediately\\n9\\napparent (Fig. 1c). To characterize the shape and thinness of the flocks, we consider its\\nthree dimensions I1<I2<I3 and measure the aspect ratios, i.e. the ratios of the two longer\\ndimensions to the shortest one: I2/I1 and I3/I1 (Table 1). Despite some fluctuations across\\nflocks, these ratios are generally stable, and do not show any significant dependence on\\nthe number of birds or the volume of the flocks (see Fig. 2b). The average over all\\nevents gives: I2/I1=2.8±0.4 and I3/I1=5.6±1.0 (confidence intervals of 95% throughout\\nthe paper), confirming the visual impression that flocks are generally thin.\\nIn contrast, the flocks’ thickness I1, i.e. the shortest dimensionis highly variable.\\nWe can explain this by noting that, if the aspect ratios I2/I1 and I3/I1 are constant, the\\nthickness I1 must be linearly correlated to V1/3, where V is the flock’s volume. This is\\naccurately verified by our data, as shown in Fig. 2a  (Pearson correlation test: R2=0.97,\\nn=10, p < 0.001). Therefore, while the number of individuals and flock volume changes\\nsignificantly across flocks, flocks proportion remains approximately constant.\\nThe concavity of a flock can be quantified as the relative difference between the\\nvolume of the convex hull (smallest convex polyhedron enclosing the flock) and the\\nvolume of the aggregation computed with the α-shape method. The results are reported\\nin Table 1. Some flocks do exhibit a nonzero concavity, which indicates that defining\\ntheir border with the convex hull would be inappropriate, since this method would\\ninclude large portions of space devoid of animals. Unlike the aspect ratio, however,\\nthere is no typical value for the concavity.\\nOrientation\\nGiven the highly non-spherical shape of flocks, we can ask whether or not they\\nhave a preferential orientation in space. To investigate this, we looked at the flock axes\\nI1, I2, and I3 . We define the unit vector G as parallel to gravity and compute the inner\\nproduct of I1 and G, i.e. the cosine between yaw and gravity. This quantity is close to 1\\nin most flocks (Table 1 and Fig. 3) and, on average, we find |I1⋅G|=0.93±0.04. This\\nshows that the yaw axis is approximately parallel to gravity, and therefore that the\\nflock’s plane is parallel to the ground. It is also interesting to study the relationship\\nbetween orientation in space and global movement of the flock. We define the centre of\\nmass as the average position of all birds in the flock, and then measure its direction of\\nmotion. Such direction is identified by a unit vector V, representing the normalized\\nvelocity. The orientation of flight can be described by the scalar product of the unitary\\nvelocity V with gravity and yaw (Table 1). Both these quantities are small for most\\nevents (see Fig. 3). We find, on average, |V⋅G|=0.13±0.05 and |V⋅I1|= 0.19±0.08.\\nVelocity is thus approximately perpendicular to gravity and yaw. This result, together\\nwith the fact that yaw and gravity are nearly parallel to each other, implies that most of\\nthe time flocks slide horizontally, flying approximately parallel to the ground.\\nFinally, we can look at the orientation of the flock with respect to the other two\\naxes, I2 and I3. The scalar product of the velocity with each one of these two vectors\\nchanges significantly from flock to flock, and, in some cases, it also changes across\\ntime. Some flocks appear to be elongated in the direction of motion (i.e. the velocity\\ndirection), having a large value of |V⋅I3|, whereas for other flocks it is the opposite, with\\n10\\na large value of |V⋅I2|. We do not find, therefore, any correlation between direction of\\nmotion and elongation of the aggregation.\\nTurns\\nAn interesting problem to consider is how birds coordinate their movements\\nduring turns. This requires analysis of events that are long enough to display a turn.\\nEvent 32-06 is almost 4 seconds long, it moves parallel to the ground (Fig.4b) and the\\nprojection of its trajectory on the plane perpendicular to gravity (the horizontal plane)\\nshows a clear turn (Fig. 4a). The unit vectors I2 and I3 associated to the dimensions I2\\nand I3 identify the two orthogonal axis of the flock on the horizontal plane. The aspect\\nratio of this flock on the plane remains approximately constant throughout the turn (I3/I2\\n~ 4/3). We find that the turn is accompanied by a rotation of the velocity V with respect\\nto I2 and I3. This can be seen clearly from the time evolution of the angles between the\\nprojection of the velocity V on the plane, and the axes I2 and I3: for example, the angle\\nbetween the projection of V and I3 is slightly larger than 90 degrees at the beginning of\\nthe curve, whereas it is 0 degrees at the end of the turn (Fig. 4c). Hence the flock’s\\norientation changes with respect to the velocity, but it remains approximately constant\\nwith respect to an absolute reference frame (Fig. 4d). We find the same in three other\\nflocks that displayed turns (events 17-06, 25-08, 28-10).\\nAnother interesting behaviour can be observed in event 32-06. Before the turn the\\nflock performs standard level flight, sliding horizontally. When the turn begins,\\nhowever, the flock tilts the yaw axis I1 with respect to the vertical, and thus the angle\\nbetween I1 and G increases (Fig. 4e). At the same time, I1 acquires a nonzero\\ncomponent in the direction of motion, and therefore the angle between I1 and V\\ndecreases. These two results indicate that when the turn is initiated the yaw axis of the\\nflock rolls (or banks) in the direction of the turn (velocity and gravity remain orthogonal\\nto each other). As a consequence, during a turn, the flock’s plane does not slide parallel\\nto the ground, but undergoes a finite drag. This manoeuvre is reminiscent of an\\naircraft’s banking turn. However, the aircraft rotates in the direction of the turn (the\\nfront of an aeroplane always remains the same) and the mutual orientation of the\\nvelocity and the main aircraft axes (I2 and I3) does not change. In the case of a flock, as\\nwe have seen, the velocity rotates with respect to the main axes (what was the front\\nbecomes the side and vice versa). Moreover, there is no global lift on the flock, as it is\\nnot a unique rigid body. Therefore, even though individual birds will bank during the\\nturn, it is unclear why flock as a whole also does so, rather than simply sliding. A\\npossible explanation is that it allows birds to keep their neighbours on the same visual\\nplane, preserving visual information on their neighbours’ positions.\\nAverage Density and Nearest Neighbour Distance\\nWhen looking at flocks, one of the first questions concerns the density, or the\\ndegree of compactness of the aggregations. We estimate the density ρ as the number of\\ninternal birds divided by the volume of flock, as defined with the α-shape (see\\nMethods). The values obtained for the ten flocking events are reported in Table 1 and\\nshow that density varies substantially across flocks.  We find that ρ  does not depend on\\n11\\nthe number of birds belonging to the flock, nor on the velocity (P =0.39 and 0.16\\nrespectively). An alternative measure of compactness (or rather of sparseness) is given\\nby the average nearest neighbour distance r1. We find that r1 is sharply related to the\\ndensity, ρ being proportional to r1\\n-3, as expected for an homogeneous arrangement of\\npoints (Stoyan & Stoyan 1994)  (Fig.5a).  The nearest neighbour distance r1 also does\\nnot depend on the size of the group, contrary to the pattern observed in fish schools\\n(Partridge at al. 1980) and in computer simulations (Kunz & Hemelrijk  2003). The\\nvalues of r1 are shown in Figure 5b and once again reveal that the flocks we analysed\\nwere quite heterogeneous: nearest neighbour distance varied more than 100% between\\ndensest and sparsest events.\\nDensity Variations Within the Aggregation\\nA further important question concerns density variations through the group: is\\nthere any significant density gradient in the flock? To answer this question we\\ninvestigated how density or, alternatively, sparseness, changed as one moved from the\\nborder to the centre of the flock. We divided the flock into shells of thickness δ, moving\\nfrom the border to the centre. One shell is defined as the subset of birds that are found at\\na distance d < k δ from the border, with k = 1, 2, 3…. Within each shell, we measured\\nthe average nearest neighbour distance. In this way, we obtained the nearest neighbour\\ndistance r1 as function of the distance from the border, d. The behaviour of r1(d) for our\\nflocks is radically different from the one of a homogeneous system  (Fig. 6a). It shows\\nthat the nearest neighbour distance increased from the border to the centre and indicates\\nthat flocks had a higher density close to the border (small r1) and were more sparse in\\nthe centre (large r1).  We also looked directly at a density measure by defining a\\ngradient function from the border to the centre. For a given value of δ, we erased all the\\nbirds with d < δ, i.e. the birds close to the original border. We were then left with a\\nreduced set of birds where the outer shell had disappeared.  We computed the border of\\nthis new set using the α-shape method, its volume and, finally, its density. We\\niteratively re-computed the density until we reached the core of the flock. For δ=0 this\\nbasically corresponds to progressively peeling away layers of the flock. As can be seen\\nin Fig. 6b, this measure shows that density is higher close to the border. This density\\ngradient is larger in some flocks than in others but it is qualitatively present in all 10\\nflocks we analysed. To investigate whether there is also a front to back density\\nimbalance, we computed the balance shift, defined as the relative difference along the\\ndirection of motion between the position of the centre of mass and the geometrical\\ncentre of the flock. Positive values of the balance shift mean that the centre of mass is\\nlocated more toward the front of the flock with respect to the geometrical centre, and\\nthat there is therefore a neat imbalance of birds in the front. The values of the balance\\nshift for all the analysed events are reported in Table 1. Sometimes the birds were\\nconcentrated more toward the front of the flock (4 flocks), sometimes more toward the\\nback (2 flocks), and sometimes they were uniformly distributed in the outer shell.\\nNearest Neighbours Radial Distribution and Exclusion Zone\\nTo gain an insight into the inter-individual structure of the flock, we looked at the\\nprobability distribution of the nearest neighbour (n.n.) distance, P(r) (Fig. 7). The shape\\n12\\nof this function is reminiscent of a random (Poisson) set of points. There are, however,\\nimportant differences. Most notably we observed that P(r) displayed a drop at low\\nvalues of r and, correspondingly, that the cumulative probability P > (r) displayed a\\nshift compared to the Poisson case. The biological origin of this drop is rather intuitive:\\nbirds maintained a certain distance from each other in order to avoid collisions. For this\\nreason, most numerical models of collective behaviour assume the existence of short-\\nrange repulsion, which gives rise to an “exclusion zone” around each individual. This is\\nequivalent to hard-core repulsion in particle systems. In order to measure the exclusion\\nzone, we fitted the probability distribution of nearest neighbour distance to that of a\\nhard-core system (Torquato 2002). We found that the size of the exclusion zone was\\nvery stable from flock to flock, and did not correlate with sparseness r1 (P=0.56). The\\naverage over all flocks gives rh=0.19m±0.02m SE. Therefore, the average minimum\\nbird-to-bird distance (2rh~0.38) is larger than starling’s typical body-length (BL=0.2m),\\nbut is comparable to the typical wingspan (WS=0.4m).\\nNearest Neighbours Angular Distribution\\nFinally, we considered the angular distribution of the nearest neighbours. To do\\nthis, for each individual bird, we considered the vector to its nearest neighbour and\\nmeasured the angle θ between this vector and the direction of motion of the flock. The\\ndistribution of cos(θ) should be constant and equal to 0.5 for an isotropic system. For\\nexample, for a completely random arrangement of birds the neighbours can be found\\nanywhere around the focal one. The probability of finding a neighbour at angle θ can be\\ncomputed by assuming a constant bird density in space. If we choose the direction of\\nmotion as the polar axis, then this probability is simply given by\\nPrandom(θ)dθ =1 2sin(θ)dθ , where the factor sin(θ) is the Jacobian from Cartesian to\\npolar coordinates. Thus, even in a random isotropic system the distribution of θ is not\\nconstant. On the other hand, a simple change of variables gives\\nPrandom(cos(θ))dcos(θ) =1 2, and the distribution of cos(θ) is therefore constant for a\\nrandom isotropic system (see Cavagna et al. 2008b for more details). It is therefore\\nmuch more convenient to look at this distribution to distinguish anisotropic from\\nisotropic arrangements of neighbours. We plotted the distribution of cos(θ) for four\\ndifferent flocks (Fig. 8). Ιn some flocks the distribution peaks at cos(θ)= 0  indicating\\nthat the nearest neighbour was more likely to be found on the plane perpendicular to the\\nvelocity (Fig. 8 c,d); in other cases (Fig. 8 a,b) there were two well defined peaks at\\nintermediate angles indicating a more structured distribution in space. However, for all\\nthe analysed flocks, we found that there was a lack of neighbours along the direction of\\nmotion (P(cos(θ))  <<  0.5 for cos(θ) ∼ ± 1). \\nDISCUSSION\\nOur data constitute the first large-scale study of collective animal behaviour in a\\nbiological system. Some of these results (e.g. the exclusion zone or the density range)\\ncan be used as input parameters for existing models. Most of the results, however,\\nshould be used to refine and extend these models, to verify and assess their assumptions,\\n13\\nand to identify the most appropriate theoretical framework. In addition, our results\\nobviously have relevance for several general biological issues.\\nMorphology\\nPerhaps the most interesting morphological result is that flocks seem to have a\\ncharacteristic shape, being thin in the direction of gravity and  more extended\\nperpendicular to it.  The proportions of the flock are well-defined, with only weakly\\nfluctuating aspect ratios, despite showing a wide range of sizes (Table 1). Our ability to\\nconclude this stems entirely from the fact that we were able to analyse several groups\\nwith very different sizes (dimensions and number of birds). Non-spherical shapes have\\nalso been observed in fish schools, with the average proportions I1:I2:I3 ranging from\\n1.0:1.7:2.1 in pilchards (Cullen et al. 1965) to 1.0:3.0:6.0 in saithes and 1:3:4 in herrings\\n(Partridge et al. 1983). These values are comparable to those we found for starlings:\\n1.0:2.8:5.6.  These studies, however, are limited in several respects: first, group size was\\nlimited to a few tens of individuals, so that it was not possible to check whether or not\\nthe proportions remained stable at different sizes; second, it is unclear to what extent the\\nshape of the tank and the water depth influenced group morphology.\\nIn our analysis, using a much larger statistical sample, we verified that, while the\\nsmallest dimension is strongly correlated with volume and number of individuals, the\\nflock is organized in a way that keeps its proportions constant. This raises a number of\\ninteresting questions: do these proportions serve any function? Is it the individuals\\nthemselves or some external stimulus that keeps the group’s proportions constant? If\\nthe individual birds that are responsible, how do they achieve this, starting from a purely\\nlocal perception of the aggregation?\\nThere may be a general reason why some specific proportions have been selected.\\nFor fish schools it has been suggested that a thin shape minimizes the chance of being\\nseen by distant predators (Partridge et al. 1983). Alternatively, one could argue that this\\nthin elongated shape is a direct consequence of the mechanisms leading to group\\nformation and does not have any adaptive function. This correlation between elongation\\nand velocity was suggested long ago on purely biological principles (Breder 1959;\\nRadakov 1973) and was later observed in fish schools (Pitcher 1980), and has been\\ndemonstrated in numerical simulations of fish schools, which show that aggregations\\nproduced with simple local interactions acquire an elongated shape in the direction of\\nthe velocity  (Kunz & Hemelrijk 2003). We did not, however, find any evidence of a\\nthis kind of pattern: the aspect ratios did not depend significantly on the absolute value\\nof the velocity and there was no correlation between the longest axis and the velocity\\ndirection. This suggests that this feature may differ between birds and fish and that, in\\nthe case of bird flocks, the global shape of the groups may not be fully explained by the\\ninteractions between individuals.\\nOne possible explanation for this difference is that aggregation dynamics between\\ngroups plays a role here. Recent models of aggregation-fragmentation (Okubo 1986;\\nBonabeau et al. 1999) have been used to predict successfully scaling in the size\\ndistribution of several species (from tuna fish to buffalo herds). These models may be\\nhelpful to investigate the origin of the shape of the groups. The rules of fission and\\nfusion between groups, however, need to be generalized to this purpose. They were\\nassumed to be isotropic in the past, while this may not be the case in reality. For\\n14\\nexample, studies on fish schools have shown that they become multilayered as the\\nnumber of individuals increases (Partridge et al., 1980).\\nGravity may also explain the thin shape that we observe: up/down space\\nfluctuations are energetically more costly than lateral ones, favouring the stretching of\\nthe aggregation along the plane perpendicular to gravity. Finally, from an aerodynamic\\npoint of view, a simple model shows that there is a net decrease in drag when the flock\\nextends further laterally than vertically (Higdon & Corrsin 1978).\\nOrientation\\nWe found that all the flocks we analysed have a well-defined orientation in space:\\nvelocity and gravity are approximately perpendicular, while yaw and gravity are nearly\\nparallel. In other words, flocks slide horizontally, flying almost parallel to the ground.\\nThis may seem odd to anyone who has observed flocks. The reason for this is that we\\ntypically watch from the ground, with our heads tilted-up, so we are unable to\\ndistinguish between closer and more distant birds. As a result, we project the flock onto\\nour tilted plane of vision, thus losing perception of its orientation and relatively thin\\naspect. It is possible, however, that the flight orientation is different in more dramatic\\nevents than the ones we considered (as in the case of a predator’s attack or for flocks at\\nhigher altitudes).\\nWhen looking for possible explanations of this behaviour, it is reasonable to\\nexpect gravity to play a strong role in the flock’s relative orientation. Migratory flocks\\nand flocks travelling from feeding to roosting sites exhibit level-flight parallel to the\\nground, and typically spread out horizontally rather than vertically (Major & Dill 1978).\\nSeveral kinds of fish also form almost two-dimensional schools that swim parallel to the\\nwater surface (Cullen et al. 1965; Partridge et al. 1983). Gravity directly determines\\nindividual kinematics which occurs at a lower cost orthogonal to gravity: starlings\\nperform steady level flight or intermittent undulating flight (temporarily gaining or\\nlosing altitude in correspondence of flapping and gliding periods) where, in any case,\\nthe optimal flight strategy corresponds to a mean flight path, which is level (Rayner et\\nal. 2001). In other words, birds tend to fly, on average, parallel to the ground due to\\nenergetic considerations.\\nTurns\\nWe observed that, during a turn, the flock’s orientation changes with respect to\\nvelocity: if the flock was originally moving in the same direction as the intermediate\\naxis, at the end of the turn it ends up oriented along the longest axis (see Fig. 4c). This\\nexplains why we did not find any statistically preferred orientation of flocks with\\nrespect to the direction of motion: flocks remain over the roost and continuously\\nperform turns, so that their relative orientation with respect to velocity changes\\ncontinuously.\\nPomeroy and Heppner (1992) described the mechanism of turning in a group of\\n12 rock doves (Columba livia), and showed that birds turn in equal-radius paths, rather\\nthan parallel paths. For example, in a 90-degree left-turn, birds at the front of the flock\\n15\\nend up on the right of it, while those on the left of the flock end up at the front of it (Fig.\\n4d). In this way what was the front-to-back axis before the turn becomes the side-to-side\\naxis after the turn. The mechanism described by Pomeroy and Heppner (1992) is\\nconfirmed by our empirical observations here: if birds turn along equal-radius paths, it\\nfollows that, as we show, the velocity must rotate with respect to the flock’s planar axes.\\nNote that, unlike equal radius paths, parallel paths would require very different\\naccelerations of the individuals during the turn, in order to retain the shape of the flock\\nunaltered, which is what we observed. Energetic considerations would suggest this is\\nunlikely to occur.\\nThis turning mechanism leads to several interesting issues. From the point of view\\nof the individual, group membership is advantageous for its anti-predator benefits.\\nHowever, not all the positions in the aggregation are equivalent, and, under some\\ncircumstances, some positions may be more advantageous than others. For example,\\nbirds at the boundary of the flock typically suffer a higher predation risk. If the\\ncost/benefit balance were negative for too many individuals or for too long, the group\\nwould eventually break up. Given that, in general, animal aggregations are rather stable,\\nthis implies that group structure and dynamics must allow for a systematic redistribution\\nof risk among its members. Individuals must be able to move through the flock and\\nexchange positions, while at the same time maintaining the integrity of the group.  We\\nfound that the front, sides and back of the border were not stable regions of the flock.\\nRather, they continuously switched. This indicates that there were no long-lasting\\nprivileged positions along the border, so that the risk/benefit of any boundary location is\\nperiodically readjusted. It would be interesting to know whether there is an analogous\\nturnover with respect to the border-centre direction. Future work on individual\\ntrajectories will allow us to answer this question.\\nAverage Density and Nearest Neighbour Distance\\nOur results on the internal structure of the groups showed that the density and the\\naverage nearest neighbour distance within flocks does not depend on the size of the\\ngroup. This is contrary to observations of fish schools (Partridge et al. 1980) and the the\\nresults of theoretical models (Kunz & Hemelrijk 2003), where the average nearest\\nneighbour distance was found to increase as the number of fishes in the group increased.\\nHowever, in both of these studies, the average nearest neighbour distance was computed\\nwithout taking into account the bias introduced by the border and, consequently, they do\\nnot give an accurate measure of group density. Individuals located on the border\\nmaintained an average nearest neighbour distance that was larger than that typical of\\ninternal individuals, simply because part of their surrounding volume was empty (see\\nCavagna et al. 2008b for a more detailed discussion of border effects). Given that the\\npercentage of individuals on the border decreases as group size gets larger, ignoring the\\nbias of the border means that the average nearest neighbour distance will inevitably\\ndecrease with group size, even if the density of groups remains the same at all sizes.\\nAnother interesting result, shown clearly in Table 1 and Figure 5b, is that density\\nvaries considerably across flocks and there is not a well-defined typical value. Of\\ncourse, this may be due to the limited number of flocks we analysed and a larger sample\\ncould reveal a distribution of densities that peaks at a characteristic value. Even if this\\nwere the case, however, it remains true that density fluctuations across flocks were very\\n16\\nlarge. In addition, density did not depend on the size of the flock (contrary to observat\\nsmall fish schools, Partridge 1980). This leaves open the question as to what exactly\\ndetermines the density of a flock. Clearly, there are some relevant biological factors to\\nconsider. For example, the presence of predators (falcons) or disturbance actions (e.g.\\nseagulls). When under attack, a flock exhibits fast expansions and contractions,\\nindicating that strong perturbations have a direct effect on density. This cannot explain\\nthe variation we see here because the events we analysed did not involve predator\\nattacks.\\nMany models of self-organized motion assume that the interaction between\\nindividuals depends on their mutual distance in space. In such cases, the average inter-\\nindividual distance (and therefore density) is determined by the nature of the\\nrelationship between inter-individual interactions with distance. An individual must be\\nattracted to its neighbours over a certain threshold distancein order to remain part of the\\nflock. At short distances, however, it is repelled, in order to avoid collisions. For a given\\ndistance, these forces of attraction and repulsion will compensate for each other and the\\nneat force experienced by the individual will be null: this value becomes the average\\nnearest neighbour distance of the model. In a recent work, however, (Ballerini et al.\\n2008), we have shown that interactions between starling individuals do not depend on\\ntheir metric distance in space, but rather on their topological distance (i.e. whether they\\nare first, second, third … neighbours). This means that there cannot be a well-defined\\ndistance in space where the force felt by one bird is null and therefore the global density\\nis not simply related to microscopic interactions.\\nDensity Variations Within the Aggregation\\nAll flocks exhibited a density gradient: density was higher at the border than in\\nthe centre. This result is quite surprising, given that some models of collective animal\\nbehaviour predict exactly the opposite (see e.g. Kunz & Hemelrijk 2003). The\\nexplanation for this is possibly to be found in the anti-predator response of the\\naggregations.\\nThe evolutionary motivation for grouping has been associated traditionally with\\nits anti-predator function (Pitcher & Parrish 1993; Vine 1971; Parrish 1992).  Belonging\\nto a group of similar individuals decreases the probability of being caught (dilution\\neffect). Moreover, moving together also reduces the ability of predator to focus on a\\nspecific individual and capture it.  The response to predators is likely to optimize this\\nconfusion effect, as can be seen in the very effective escape manoeuvres displayed by\\nstarling flocks under attack. The high density borders that we observed may represent a\\nfeature that enhances such anti-predatory tactics, creating a ‘wall’ effect to increase the\\npredator’s confusion. However, we do not at present understand how the density\\ngradient is produced in terms of inter individual dynamics within the group.\\nReconstruction of the individual dynamical trajectories, which is currently underway,\\nwill help to clarify this point. \\nFinally, some numerical models of fish schools reveal that density is higher at the\\nfront of the group (Kunz & Hemelrijk 2003; Hemelrijk & Kunz 2004; Hemelrijk &\\nHildenbrandt 2007), as also observed in natural shoals (Baumann et al. 1997). In our\\nanalysis however we did not find a clear and general correlation between the densest\\npart of the flock and the direction of motion.\\n17\\nNearest Neighbours Radial Distribution and Exclusion Zone\\nAn important result concerned the presence of a well-defined exclusion zone\\naround individual birds, which was very stable from flock to flock and whose diameter\\nwas comparable to the average wingspan. The finding that birds do not come too close\\none to the other is highly intuitive, given the need to avoid collisions, and is consistent\\nwith one of the main assumptions of numerical models. Models for flocking, which use\\nthe size of the exclusion zone as an input parameter, can now be fed with our empirical\\nvalue, in order to increase their accuracy.\\nWe stress that the value of the exclusion zone (~0.38m) does not depend on\\ndensity, and that it provides a metric scale characteristic of flocking behaviour. As\\nalready mentioned, a second study on the same data set (Ballerini et al. 2008) shows\\nthat the interaction between birds possesse a crucial topological character: each\\nindividual interacts with up to 6-7 neighbours, irrespective of their distance. It seems,\\ntherefore, that the interaction between birds can be understood at two levels: metric at\\nshort scales, and topological at larger ones. In other words, when a neighbour is too\\nclose, a bird takes into account its physical distance and tries to keep it outside the\\nexclusion zone; for more distant birds, however, the actual distances do not matter and\\nthe bird interacts up to its seventh neighbour,  wherever it is located in space. The\\nreasons for such behaviour are inherent in the very nature of flocking and aerial display.\\nOn the one hand, individuals in a group must avoid collisions and control the mutual\\ndynamics at short distances. On the other hand, the flock undergoes large density\\nvariations that can modify dramatically the distances of its nearest neighbours: to keep\\nthe same degree of cohesion, indispensable during predator attacks, each bird must keep\\ntrack of the same number of neighbours, even if their distance changes.\\nThe analysis of nearest-neighbours distribution using a hard-sphere model also\\nrevealed another quantitatively important feature: flocks are not as compact as they may\\nappear to an observer. For hard-spheres, the degree of compactness of the system can be\\nclassified through the so-called packing fraction: φ= 4/3 πρ rh\\n3 = v N /V, where ρ is the\\ndensity, rh the size of the hard-core (the exclusion zone), v is the volume of each sphere,\\nN is the total number of spheres, and V is the volume of the aggregation. The packing\\nfraction is simply the ratio between the total volume occupied by the spheres and the\\nvolume of the aggregation. It depends only on the hard-core value and on the density\\nand, by definition, it is a number between 0 and 1. Small values of φ correspond to very\\ndiluted systems (gas-like), while large values correspond to compact ones (liquid or\\ncrystals). To give some quantitative reference values, compact crystalline arrangements\\nof spheres correspond to φ > 0.490, while the value for the most compact possible\\narrangement is φ ~ 0.79 (the so-called random close packing, Torquato 2002).  All the\\nflocks we analysed showed packing fractions smaller than 0.012. This is a very small\\nvalue indeed. In hard-sphere language, flocks are extremely sparse systems, with much\\nlower densities than a crystalline arrangement. This sparseness is not evident to the eye,\\nsince the two-dimensional projection we perceive looks much denser. This sparseness\\nmakes good biological sense because, if the birds were too close one to the other,\\nexchange of positions and diffusion inside the flock would be difficult to achieve.\\nFurther analysis on individual birds trajectories will allow us to investigate\\n18\\nquantitatively the extent to which the individual freedom to move is related to the\\noverall flock density.\\nDespite being such sparse systems, flocks exhibit a non-trivial structure in space,\\nwhich is well described by the angular distribution of nearest neighbours. Neighbours\\nare less likely to be found along the direction of motion, and instead they concentrate\\nlaterally. While the comparison with particle systems can be extremely useful from a\\nmethodological point of view, one always has to keep in mind that animal groups are\\nvery different from particle systems. A hard-sphere system with the same hard-core and\\ndensity values as those we found here would be a gas without any structure, while\\nflocks exhibit a strongly anisotropic structure. Reproducing such features will be an on-\\ngoing challenge for models and theories.\\nNearest Neighbours Angular Distribution\\nA similar spatial anisotropy to the one found here for starlings has been reported\\nin fish schools (Cullen 1980), suggesting this is a typical feature of collective behaviour\\nin both birds and fish. An important question, therefore, is to understand the origin of\\nthis anisotropy. One possibility is that it is simply an effect of the existence of a\\npreferential direction of motion. However, the simplest numerical models of self-\\norganized motion, which assume isotropic interactions between individuals, give a non-\\nzero velocity for the aggregation, but fail to reproduce the angular anisotropy. This\\nsuggests that the anisotropy in the nearest neighbour’s distribution is an explicit\\nconsequence of the anisotropic character of the interaction itself, which models should\\nincorporate into their assumptions if they want to reproduce the observed behaviour.\\nThere are various reasons why interactions between individuals should be\\nanisotropic. First of all, we note that vision itself has an anisotropic nature in both birds\\nand fishes. In particular, starlings have lateral visual axes and a blind rear sector (Martin\\n1986). Thus, if vision is the main mechanism by which interactions are controlled, then\\nthe very structure of the eye may be responsible for the lack of nearest neighbours in the\\nfront-rear direction (Heppner 1974; Badgerow 1988; Speakman & Banks 1998). An\\nalternative idea is that the mutual position chosen by the animals is the one that\\nmaximizes the sensitivity to changes of heading and speed of their neighbours (Dill et\\nal. 1997). According to this hypothesis, even though the interaction is still vision-based,\\nit is an optimization mechanism to determine the anisotropy of neighbours. Another\\npossibility is that individuals try to keep a larger distance between themselves and the\\nindividuals in front of them, to avoid the risk of collisions in case of sudden changes of\\nvelocities. Finally, a radically different claim is that anisotropic structures in both bird\\nand fish aggregations saves energy thanks to aerodynamic (or hydrodynamic)\\nadvantages  (Lissaman & Shollenberger 1970; Weihs, 1973; Hummel 1995). The\\nenergy-saving principle has been challenged, however, for both birds (Badgerow &\\nHainsworth  1981) and fish (Partridge & Pitcher  1979). More importantly, at least for\\naerial display, the fact that inter-individual interactions depend on the order of\\nneighbour, rather than on their distance, rules out any aerodynamic arguments, because\\nthese imply a strong dependence on the metric distance (as discussed in Ballerini et al.\\n2008).\\n19\\nIn this paper we presented large-scale data on starling flocks during aerial display.\\nOur data were obtained in field observations of large, naturally occurring groups. This\\nis, we believe, a very important feature, which distinguishes our research from previous\\nones. Experiments in the laboratory may condition some important features of the\\ngroups, such as their shape and dynamics, due to the confined space.  Groups of small\\nsize, both in the laboratory and in the field, do not allow a reliable statistical analysis\\ndue to border effects. Thanks to our experimental procedure, we avoided these two\\nproblems and produced, for the first time, unbiased data on very large groups in the\\nfield.\\nWe investigated the main features of the flocks, shape, movement, density and\\nstructure, and characterized them as emergent attributes of the grouping phenomenon.\\nOur data provide with a new experimental benchmark for testing and improving\\ntheoretical models of self-organized motion. In this light, we discussed some of our\\nresults in connection with the known predictions of existing models. We hope that our\\nanalysis will help to clarify what are the fundamental microscopic mechanisms leading\\nto collective behaviour in animal groups, and how appropriate behavioural rules for the\\nindividuals can determine specific features of the aggregation at the group level.\\nAcknowledgements\\nWe thank E. Alleva, F. Bartumeus, C. Carere, G. Cavagna, I. Couzin, D.\\nGrunbaum, C. Hemelrijk, H. Hildelbrandt, D.Santucci, D. Stoyan for interesting\\ndiscussions. We particularly thank A. Cimarelli and F. Stefanini for technical help and\\nseveral discussions.\\n20\\nREFERENCES\\nAloimonos, Y. & Rosenfeld, A. 1991. A response to “Ignorance, Myopia, and Naiveté\\nin Computer Vision Systems” by Jain, R. C. & Binford, T. O. CVGIP Image\\nUnderstanding 53, 120-124.\\nAoki, I. 1982. A simulation study on the schooling mechanism in fish. Bulletin of the\\nJapanese  Society of  Scientific Fisheries 48, 1081-1088.\\nArmangué, X. & Salvi, J. 2003. Overall view regarding fundamental matrix\\nestimation. Image and Vision Computing  21, 205-220.\\nBallerini, M.,  Cabibbo, N., Candelier, R., Cavagna,  A., Cisbani, E., Giardina, I.,\\nLecomte, V., Orlandi, A., Parisi, G., Procaccini, A., Viale, M., & Zdravkovic, V.\\n2008. Interaction Ruling Animal Collective Behaviour Depends on Topological rather\\nthan Metric Distance: Evidence from a Field Study. Proceedings of the National\\nAcademy of Sciences USA 105, 1232-1237.\\nBadgerow, J. P. 1988. An analysis of function in the formation flight of Canada geese.\\nThe Auk 105, 749-755.\\nBadgerow, J. P. & Hainsworth F. R. 1981. Energy savings through formation flight?\\nA re-examination of the vee formation. Journal of  Theoretical Biology 93, 41-52 .\\nBonabeau E., Dagorn L. and Fréon P. 1999. Scaling in animal group-size\\ndistributions. Proceedings of the  National  Academy of  Sciences USA 96, 4472–4477.\\nBreder, C. M. 1959. Studies in social groupings in fish.  Bulletin of the American\\nMuseum of Natural  History 117, 397-481.\\nBudgey, R. 1998. The three dimensional structure of bird flocks and its implications for\\nbirdstrike tolerance in aircraft. International Bird Strike Committee Proceedings. 24,\\n207-220.\\nBuhl, J., Sumpter, D. J. T., Couzin, I. D., Hale, J. J., Despland, E., Miller, E. R. &\\nSimpson, S. J. 2006. From disorder to order in marching locusts. Science 312, 1402-\\n1406.\\nBumann, D., Krause, J. & Rubenstein, D. 1997. Mortality Risk of Spatial Positions in\\nAnimal Groups: the Danger of Being in the Front. Behaviour 134, 1063-1076.\\nCao, Y. U., Fukunaga, A. S. & Kahng, A. B. 1997. Cooperative mobile robotics:\\nantecedents and directions Autonomous Robots 4, 1-23.\\nCardy, J. 1996. Scaling and Renormalization in Statistical Physics. Cambridge Lecture\\nNotes in Physics No. 5, Cambridge: Cambridge University Press.\\nCavagna, A., Giardina, I., Orlandi, A., Parisi, G., Procaccini, A, Viale, M. & V.\\nZdravkovic. 2008. The STARFLAG Handbook to collective animal behaviour: Part I,\\nempirical methods. Animal Behaviour, in press.\\n21\\nCavagna, A., Giardina, I., Orlandi, A., Parisi, G. & Procaccini. 2008. The\\nSTARFLAG Handbook to collective animal behaviour: Part II, three-dimensional\\nanalysis. Animal Behaviour, in press.\\nCont, R. & Bouchaud, J-P. 2000. Herd behaviour and aggregate fluctuations in\\nfinancial markets. Macroeconomic Dynamics 4: 170-196.\\nCouzin, I. D. & Krause, J. 2003. J. Self-organization and collective behaviour in\\nvertebrates. Advances in the Study of Behaviour 32, 1-75.\\nCouzin, I. D., Krause, J., James, R., Ruxton, G. D. & Franks, N. R. 2002. Collective\\nmemory and spatial sorting in animal groups.  Journal of Theoretical Biology 218, 1-11.\\nCullen, J. M. , Shaw, E. and Baldwin, H.A. 1965. Methods for measuring the three-\\ndimensional structure of fish schools. Animal Behaviour 13, 534-543.\\nDill, L. M., Holling, C. S. & Palmer, L. H. 1997. Predicting the three-dimensional\\nstructure of animal aggregations from functional considerations: The role of\\ninformation. In: Animal groups in three dimensions. (Ed. By Parrish, J. K. & Hammer,\\nW. M.), pp. 207-224. Cambridge: Cambridge University Press.\\nDussutour, A., Fourcassie, V., Helbing, D. & Deneubourg, J.-L. 2004. Optimal\\ntraffic organization in ants under crowded conditions. Nature 428, 70-73.\\nEdelsbrunner H. and Mucke E. P. 1994. Three-dimensional alpha shapes.  ACM\\nTransaction Graphics 13, 43-72.\\nEmlen, J. T. 1952.  Flocking behaviour in birds. The Auk 69, 160-170.\\nFarkas, I., Helbing D. & Vicsek T. 2002. Mexican waves in an excitable medium.\\nNature 419, 131-132.\\nFeare, C. 1984. The Starling. Oxford: Oxford University Press.\\nGregoire, G. & Chate, H. 2004. Onset of collective and cohesive motion. Physical\\nReview Letters 92, 025702.\\nGueron, S., Levin, S. A. & Rubenstein, D. I. 1996. The dynamics of herds: from\\nindividuals to aggregations. Journal of Theoretical Biology 182, 85-98.\\nHartley, R. & Zisserman, A. 2003. Multiple View geometry in computer vision.\\nCambridge: Cambridge University Press.\\nHemelrijk, C. K. & Kunz, H. 2005. Density distribution and size sorting\\nin fish schools: an individual-based model. Behavioral Ecology 16, 178-187.\\nHemelrijk, C. K. & Hildenbrandt, H. 2008. Self-organized shape and frontal density\\nof fish schools. Ethology. In press.\\nHeppner, F. H. 1974. Avian flight formation. Bird Binding 45, 160-169.\\nHeppner, F. H. & Grenander, U. 1990. A stochastic nonlinear model for coordinated\\nbird flocks. In: The ubiquity of chaos (Ed. by Krasner, S.), pp. 233-238. Washington,\\nDC: American Association for the Advancement of Science.\\nHigdon J. J. L. & Corrsin S. 1978. Induced Drag of a Bird Flock, The American\\nNaturalist, 112, 727-744.\\n22\\nHummel, D. 1995. Formation flight as an energy-saving mechanism. Israel Journal of\\nZoology 41, 261-278.\\nHuth, A. & Wissel, C. 1992. The simulation of the movement of fish schools. Journal\\nof Theoretical Biology 156, 365-385.\\nJadbabaie, A., Lin, J. & Morse, S. 2003. Coordination of Groups of Mobile\\nAutonomous Agents using nearest neighbour Rules, IEEE Transactions on Automatic\\nControl 48, 988-1001.\\nKrause, J. & Ruxton, G. D. 2002. Living in groups. Oxford: Oxford University Press.\\nKunz, H. & Hemelrijk, C. K. 2003. Artificial fish schools: collective\\neffects of school size, body size, and body form. Artificial Life  9, 237-253.\\nInada, Y. & Kawachi, K. 2002. Order and flexibility in the motion of fish schools.\\nJournal of Theoretical Biology 214, 371-387.\\nLissaman, P. B. S. & Shollenberger, C. A. 1970. Formation flight of birds. Science\\n168, 1003-1005.\\nLonguet-Higgins, H.C.  1981. A computer algorithm for reconstructing  a scene from\\ntwo projections. Nature 293, 133-135.\\nMajor, P. F. & Dill, L. M. 1978. The three-dimensional structure of airborne bird\\nflocks. Behavioral Ecology and Sociobiology 4, 111-122.\\nMartin, G. R. 1986. The eye of a passeriform bird, the European starling (Sturnus\\nvulgaris): eye movement amplitude, visual fields and schematic optics. Journal of\\nComparative Physiology A 159, 545-557.\\nMichard, Q. & Bouchaud, J.-P. 2005. Theory of collective opinion shifts: from\\nsmooth trends to abrupt swings. European Physical Journal B 47, 151-159.\\nMiller, R.S. & Stephen, W. J. D. 1966. Spatial Relationships in Flocks of Sandhill\\nCranes (Grus Canadensis). Ecology 47, 323-327.\\nNeda, Z., Ravasz, E., Brechet, Y., Vicsek, t. & Barabasi, A. 2000. The sound of\\nmany hands clapping, Nature 403, 849-850.\\nOkubo, A. 1986. Dynamical aspects of animal grouping: swarms, schools, flocks, and\\nherds. Advances in Biophysics 22, 1–94.\\nOsborn, J. 1997. Analytical and digital photogrammetry. In: Animal groups in three\\ndimensions (Ed. by Parrish, J. K. & Hammer, W. M.), pp. 36-60. Cambridge:\\nCambridge University Press.\\nParrish, J. K. 1992. Do predators “shape” fish schools: interactions between predators\\nand their schooling prey.  Netherlands Journal of Zoology 42, 358-370.\\nParrish, J. K. and Edelstein-Keshet L. 1999. Complexity, Pattern, and Evolutionary\\nTrade-Offs in Animal Aggregation. Science 284, 99-101 (1999).\\nParrish, J. & Hammer, W. M. (ed.) 1997. Animal groups in three dimensions.\\nCambridge: Cambridge University Press.\\nPartridge, B. L. 1980. The effect of school size on the structure and dynamics of\\nminnow schools. Animal Behaviour 28, 68-77.\\n23\\nPartridge, B.L., Johansson, J. and Kalik, J. 1983. Structure of schools of giant\\nbluefin tuna in Cape Cod Bay. Environmental Biology of Fishes 9, 253-262.\\nPartridge, B. L. & Pitcher, T. J. 1979. Evidence against a hydrodynamic function for\\nfish schools. Nature 279, 418-419.\\nPartridge, B. L., Pitcher, T., Cullen, J. M. & Wilson, J. 1980. The three-dimensional\\nstructure of fish schools. Behavioral Ecology and Sociobiology 6, 277-288.\\nPitcher, T. J. 1980. Some ecological consequences of fish school volumes. Freshwater\\nBiology 10, 539-544.\\nPitcher, T. J. & Parrish, J. K. 1993. Functions of shoaling behaviour in teleosts. In:\\nBehaviour of Teleost Fishes (Ed. by Pitcher, T. J.), pp. 363-439. London: Chapman and\\nHall.\\nPomeroy, H. & Heppner, F. 1992. Structure of turning in airborne rock dove\\n(Columba livia) flocks. The Auk 109(2), 256-267.\\nRadakov, D.V. 1973. Schooling in the echology of fish, Israeli Scientific Translation\\nSeries. John Wiley and Sons Ltd, New York.\\nRayner, J. M. V., Viscardi, P. V., Ward, S., & R. Speakman, J. R. 2001\\nAerodynamics and Energetics of Intermittent Flight in Birds, American Zoologist 41,\\n188-204.\\nReynolds, C. W. 1987. Flocks, herds, and schools: a distributed behavioral model.\\nComputer Graphics 21, 25-33.\\nSpeakman, J. R. & Banks, D. 1998. The function of flight formations in Greylag\\nGeese Anser anser; energy savings or orientation?  Ibis 140, 280-287.\\nStoyan, D.  & Stoyan, H. 1994. Fractals, random shapes and point fields. Chichester:\\nJohn Wiley and Sons Ltd.\\nTien, J. H., Levin, S. A. & Rubenstein, D. I. 2004. Dynamics of fish shoals:\\nidentifying key decision rules. Evolutionary Ecology Research 6, 555-565.\\nTorquato, S. 2002. Random Heterogeneous Materials. New York: Springer-Verlag.\\nVan Long, L., Aoyama, T. & Inagaki, T. 1985. A stereo photographic method for\\nmeasuring the spatial position of fish. Bulletin of the Japanese Society of Scientific\\nFisheries 51, 183-190.\\nVan Tets, G. F. 1966.  A photographic method of estimating densities of bird flocks in\\nflight. CSIRO Wildlife Research 11, 103-110.\\nVicsek, T., Czirok, A., Ben-Jacob, E. Cohen, I. & Shochet, O. 1995. Novel type of\\nphase transition in a system of self-driven particles. Physical Review Letters 75, 1226-\\n1229.\\nVine, I. 1971. Risk of visual detection and pursuit by a predator and the selective\\nadvantage of flocking behaviour. Journal of Theoretical Biology 30, 405-422.\\nWeihs, D. 1973. Hydrodynamics of fish schooling. Nature 241, 290-291.\\n24\\nTable 1.  Global quantitative properties\\nThickness        Aspect ratios\\nOrientation Parameters\\nEvent\\nNumber \\nVolume\\nDensity\\nNnd\\nVelocity\\nConcavity\\nBalance \\n     \\nof birds\\n(m3)\\nρ  (m-3)\\n r 1 (m)\\n(ms-1)\\nshift  \\n(m)\\n32-06\\n781\\n930\\n0.8\\n0.68\\n9.6\\n0.03\\n0.08\\n5.33\\n2.97\\n4.02\\n0.89\\n0.06\\n0.20\\n28-10\\n1246\\n1840\\n0.54\\n0.73\\n11.1\\n0.34\\n-0.06\\n5.29\\n3.44\\n6.93\\n0.80\\n0.09\\n0.41\\n25-11\\n1168\\n2340\\n0.38\\n0.79\\n8.8\\n0.37\\n-0.1\\n8.31\\n1.90\\n5.46\\n0.92\\n0.12\\n0.14\\n25-10\\n834\\n2057\\n0.34\\n0.87\\n12.0\\n0.05\\n0.0\\n6.73\\n2.65\\n4.98\\n0.99\\n0.18\\n0.18\\n21-06\\n617\\n2407\\n0.24\\n1.00\\n11.2\\n0.04\\n0.0\\n7.23\\n2.56\\n4.53\\n0.96\\n0.09\\n0.11\\n29-03\\n448\\n2552\\n0.13\\n1.09\\n10.1\\n0.20\\n0.0\\n6.21\\n3.58\\n5.96\\n0.97\\n0.27\\n0.06\\n25-08\\n1360\\n12646\\n0.09\\n1.25\\n11.9\\n0.19\\n0.16\\n11.92\\n3.32\\n5.12\\n0.95\\n0.14\\n0.12\\n17-06\\n534\\n5465\\n0.08\\n1.30\\n9.1\\n0.18\\n0.5\\n9.12\\n2.76\\n6.94\\n0.91\\n0.09\\n0.32\\n16-05\\n2631\\n28128\\n0.06\\n1.31\\n15.2\\n0.15\\n0.0\\n17.14\\n2.46\\n8.36\\n0.90\\n0.19\\n0.25\\n31-01\\n1856\\n33487\\n0.04\\n1.51\\n6.9\\n0.24\\n0.17\\n19.00\\n2.44\\n4.07\\n0.95\\n0,09\\n0.13\\nI2 /I1\\nI3 /I1\\nI1 ⋅G\\nV ⋅G\\nV⋅I1\\nI1\\nFlocking events are labelled according to session number (each day of data-taking\\ncorresponding to a session) and position within the session (in temporal order). Each quantity is\\naveraged over the different shots of the event. Events are ordered by increasing values of the\\naverage nearest neighbour distance (Nnd), r1. The density is computed as the number of\\ninternal birds divided by the volume of the flock, as defined with the α-shape. Velocity refers to\\nthe centre of mass (the average position of all birds in the flock). Concavity is defined as the\\nrelative volume difference between actual and convex border of the flock. The balance shift is\\ndefined as the relative difference along the direction of motion between the position of the\\ncentre of mass and the geometrical centre of the flock: positive values indicate that the density\\nis larger on the front.  The thickness I1, the two larger dimensions I2 and I3, and the\\ncorresponding aspect ratios, are defined as described in the Methods section. The three last\\ncolumns report the scalar products (absolute values) between yaw (the axis relative to the\\nshortest dimension), gravity and velocity.\\n25\\nFIGURES\\nFigure 1. A typical flock and its 3D reconstruction\\nThis flock belongs to event 16-05, and consists of 2630 starlings, flying at approximately\\n240 m from the cameras. The cameras tilt-up was 40%. a,b, Left and right photographs of the\\nstereo pair, taken at the same instant of time, but 25 meters apart. To perform the 3D\\nreconstruction, each bird’s image on the left photo must be matched to its corresponding image\\non the right photo. c,d,e,f, 3D reconstruction of the flock in the reference frame of the right\\ncamera, under 4 different points of view. The thinness of the flock is evident. Panel d shows the\\nreconstructed flock with the same perspective as the right photograph (b).\\n26\\nFigure 2.  Dimensions and aspect ratios\\na. The flock’s thickness I1 is plotted against V1/3, where V is the volume, for all the ten\\nevents we analysed. The dash-dotted line represents a linear fit of the data, showing a clear\\nlinear correlation between these two quantities (Pearson, n=10, R2=0.97, P=1.6×10-7). b. The\\naspect ratios, i.e. the ratios of the two longer dimensions to the shortest dimension, I2/I1 and I3/I1,\\nare rather stable and show no significant correlation with volume (P=0.51 and P=0.54,\\nrespectively).\\n27\\nFigure 3.  Orientation\\nThe flock’s orientation in space can be described by the mutual orientation of yaw (I1),\\nvelocity (V) and gravity (G). The inner product (in modulus) between yaw and gravity ( |I1⋅G| ) is\\nclose to one in most flocks. The inner product (in modulus) between velocity and gravity ( |V⋅G|\\n), on the other hand, is small in most events. These results show that most of the times flocks\\nslide horizontally, flying approximately parallel to the ground.\\n28\\nFigure 4. A turning flock\\nEvent 32-06 lasts 4 seconds before the flock exits the camera’s field of view. a, The\\ntrajectory of the flock projected on the horizontal plane (orthogonal to gravity), clearly shows a\\nturn (direction of motion is left to right). Each point represents the (X,Z) position of the centre of\\nmass of the flock, at time intervals of 0.1s. b, The same trajectory, when projected on the\\nvertical plane (X,Y) (with Y parallel to gravity) looks very thin, showing that the flock flies parallel\\nto the ground. c, The angles between the planar orthogonal axes I2 and I3 of the flock, and the\\nprojection of the velocity V on the plane vs. time. The velocity rotates with respect to I2 and I3: at\\nthe beginning of the turn the velocity is approximately perpendicular to the longest axis I3,\\nwhereas V and I3 are parallel at the end of the turn. d, Schematic representation of the flock’s\\nturn on the horizontal plane; I2 (red), I3 (blue) and V (black) have mutual orientation as in the\\nreal event. Apart from a slight rotation at early times, the absolute orientation of the flock\\nremains constant throughout the turn. e, The yaw axis I1, orthogonal to the flock’s plane, is\\nalmost parallel to gravity and perpendicular to velocity before the turn. As the turn approaches,\\nthe angle between I1 and G increases, whereas the angle between I1 and V decreases. Hence,\\nthe yaw axis tilts in the direction of the turn, and the flock’s plane is no longer parallel to the\\nground. The centripetal acceleration (in m/s2) has been computed from a Gaussian SP-line\\ninterpolation of the discrete trajectory. As expected, the acceleration shows a peak associated\\nto the turn.\\n29\\nFigure 5. Nearest neighbour average distance\\na. The density ρ  is proportional to r1\\n-3 (Pearson correlation test: n=10, R2=0.8, P=0.0004)\\nand we can then use r1 as a convenient measure of a flock’s sparseness. b. The sparseness r1\\nvaries significantly from flock to flock, ranging from 0.7m to 1.5m, in the analysed events.\\nAverage body length and wingspan of starlings are respectively BL=0.2m and WS=0.4m, and\\nthus in the densest flocks r1~3.5BL and r1~1.7WS.\\n30\\nFigure 6. Density gradient\\nFlocks exhibit a clear density gradient from the border to the centre. a. Average nearest\\nneighbour distance as a function of the distance from the border, for 4 flocking events. Each\\ncurve is obtained by averaging over the consecutive shots of the corresponding event (error\\nbars indicate standard errors due to the corresponding fluctuations). In all the flocks the average\\nnearest neighbour distance increases when going from the border  to the centre, indicating that\\nflocks are denser at the border. We also report the behaviour of a Poisson ensemble of random\\npoints of density and dimensions comparable to event 25-08: for a homogeneous system the\\naverage nearest neighbour distance is constant and does not depend on the distance from the\\n31\\nborder. b. Density as a function of the shell number. To compute the density, the birds in a shell\\nof length δ=0.2 from the border are erased, then the border of the reduced set of birds is re-\\ncomputed, and finally its density obtained as the ratio of the number of internal birds by the\\nreduced volume. The procedure is repeated until no birds are left in the reduced set. The curves\\ncorrespond to single consecutive shots in event 25-08, where the density gradient is more\\nevident and the number of birds is large enough to have a single-sample good statistics. To\\ncompare with the random homogeneous case, and to be sure that small volume effects do not\\nbias the decrease in density, we generated a set of random points with the same border,\\nvolume and internal density as photo 166 of flock 25-08. In the other events the density gradient\\nis qualitatively similar.\\n32\\nFigure 7. Nearest neighbour distance distribution and hard core\\na, Event 32-06: probability distribution P(r) of the nearest neighbour distance. The\\naverage n.n. distance r1 is the average of r with this distribution. Circles are the data. The red\\nline is the hard-sphere fit, the blue line is the Poisson fit. Compared to Poisson, data show a\\ndrop of the probability at small values of r. Birds (on average) tend not to get closer than a\\ncertain minimum distance. In this sense, we say that birds have an effective hard-core rh. As a\\nconsequence, the n.n. probability distribution of a hard-sphere system, which is zero for r<2rh, is\\na better fit of the data. b,  The presence of a hard-core is particularly evident when one plots the\\nlogarithm of the cumulative probability P>(r) (the probability that the n.n. distance is larger than r)\\nas a function of r3. Circles are the data. The red line is the hard-spheres fit, the blue line is the\\nPoisson case. The hard-sphere cumulative probability is equal to 1 for r<2rh, where rh is the\\nhard-core. Thus, compared to Poisson, the hard-sphere curve has a shift due to the presence of\\nthe hard-core. This shift is clearly present also in the real data. For event 32-06 the hard-sphere\\nfit gives rh=0.17m (the value of 2rh is marked by the vertical dotted line). c,d, Event 25-11, same\\nsymbols as in a,b. Hard-core rh=0.20m .\\n33\\nFigure 8. Nearest neighbour angular distribution\\na-d, Probability distribution of the cosine of the angle θ between the global flock’s velocity\\nand the direction of the nearest neighbour, for events 32-06, 17-06, 25-10, and 25-11.\\ncos(θ)=+1 corresponds to the front of the reference bird, whereas cos(θ)=-1 corresponds to the\\nback. For an isotropic arrangement of points there is no correlation between velocity and\\nposition of the nearest neighbour, therefore all directions are equally probable, and this\\nprobability distribution is constant and equal to 1/2 (provided that boundary effects are\\nappropriately accounted for). All 10 flocks we analysed show a significant drop of the probability\\nat cos(θ)=±1, indicating that the arrangement is not isotropic and that the nearest neighbour of\\neach bird is less likely to be found around the direction of motion.\\n')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:34:38.433625Z",
     "start_time": "2025-12-13T11:34:38.427669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 문서 메타데이터 출력\n",
    "docs[0].metadata"
   ],
   "id": "2e1b4e1c1fa2a941",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2008-02-12',\n",
       " 'Title': 'The STARFLAG handbook on collective animal behaviour: Part I, empirical methods',\n",
       " 'Authors': 'Andrea Cavagna, Irene Giardina, Alberto Orlandi, Giorgio Parisi, Andrea Procaccini, Massimiliano Viale, Vladimir Zdravkovic',\n",
       " 'Summary': 'The most startling examples of collective animal behaviour are provided by very large and cohesive groups moving in three dimensions. Paradigmatic examples are bird flocks, fish schools and insect swarms. However, because of the sheer technical difficulty of obtaining 3D data, empirical studies conducted to date have only considered loose groups of a few tens of animals. Moreover, these studies were very seldom conducted in the field. Recently the STARFLAG project achieved the 3D reconstruction of thousands of birds under field conditions, thus opening the way to a new generation of quantitative studies of collective animal behaviour. Here, we review the main technical problems in 3D data collection of large animal groups and we outline some of the methodological solutions adopted by the STARFLAG project. In particular, we explain how to solve the stereoscopic correspondence - or matching - problem, which was the major bottleneck of all 3D studies in the past.',\n",
       " 'entry_id': 'http://arxiv.org/abs/0802.1668v1',\n",
       " 'published_first_time': '2008-02-12',\n",
       " 'comment': 'To be published in Animal Behaviour',\n",
       " 'journal_ref': 'Animal Behaviour 76 (1), 217-236 (2008)',\n",
       " 'doi': None,\n",
       " 'primary_category': 'q-bio.QM',\n",
       " 'categories': ['q-bio.QM', 'cond-mat.stat-mech', 'q-bio.PE'],\n",
       " 'links': ['https://arxiv.org/abs/0802.1668v1',\n",
       "  'https://arxiv.org/pdf/0802.1668v1']}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load_all_available_meta=False 인 경우 메타데이터는 전체가 아닌 일부만 출력됩니다.",
   "id": "5b822946c6dcb602"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:36:01.189156Z",
     "start_time": "2025-12-13T11:35:51.606704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Query 에 검색하고자 하는 논문의 주제를 입력합니다.\n",
    "loader = ArxivLoader(\n",
    "    query=\"ChatGPT\",\n",
    "    load_max_docs=2,  # 최대 문서 수\n",
    "    load_all_available_meta=False,  # 메타데이터 전체 로드 여부\n",
    ")\n",
    "\n",
    "# 문서 로드 결과출력\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 메타데이터 출력\n",
    "docs[0].metadata"
   ],
   "id": "39412be58ebbe79e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2023-01-18',\n",
       " 'Title': 'How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection',\n",
       " 'Authors': 'Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu',\n",
       " 'Summary': \"The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\"}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 요약(summary)\n",
    "\n",
    "논문의 전체 내용이 아닌 요약본을 출력하고자 한다면, get_summaries_as_docs() 함수를 호출하면 됩니다."
   ],
   "id": "2031d37e1e367c64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:36:34.397531Z",
     "start_time": "2025-12-13T11:36:33.713952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 문서 요약 로딩\n",
    "docs = loader.get_summaries_as_docs()\n",
    "\n",
    "# 첫 번째 문서 접근\n",
    "print(docs[0].page_content)"
   ],
   "id": "64513a1a9accf6d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## lazy_load()\n",
    "문서를 대량으로 로드할 때 모든 로드된 문서의 부분 집합에 대해 하류 작업을 수행할 수 있다면, 메모리 사용량을 최소화하기 위해 문서를 한 번에 하나씩 지연 로드할 수 있습니다."
   ],
   "id": "66bf2b5622668975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:36:51.393730Z",
     "start_time": "2025-12-13T11:36:48.711842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = []\n",
    "\n",
    "# 문서 지연 로드\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)"
   ],
   "id": "fe1f615c44e5fbbb",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:36:51.433269Z",
     "start_time": "2025-12-13T11:36:51.422141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 결과 출력\n",
    "docs\n"
   ],
   "id": "2abbc496d8e459b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-01-18', 'Title': 'How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection', 'Authors': 'Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu', 'Summary': \"The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\"}, page_content='How Close is ChatGPT to Human Experts?\\nComparison Corpus, Evaluation, and Detection\\nBiyang Guo1†∗, Xin Zhang2∗, Ziyuan Wang1∗, Minqi Jiang1∗, Jinran Nie3∗\\nYuxuan Ding4, Jianwei Yue5, Yupeng Wu6\\n1AI Lab, School of Information Management and Engineering\\nShanghai University of Finance and Economics\\n2Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen)\\n3School of Information Science, Beijing Language and Culture University\\n4School of Electronic Engineering, Xidian University\\n5School of Computing, Queen’s University, 6Wind Information Co., Ltd\\nAbstract\\nThe introduction of ChatGPT2 has garnered widespread attention in both academic\\nand industrial communities. ChatGPT is able to respond effectively to a wide range\\nof human questions, providing ﬂuent and comprehensive answers that signiﬁcantly\\nsurpass previous public chatbots in terms of security and usefulness. On one hand,\\npeople are curious about how ChatGPT is able to achieve such strength and how\\nfar it is from human experts. On the other hand, people are starting to worry about\\nthe potential negative impacts that large language models (LLMs) like ChatGPT\\ncould have on society, such as fake news, plagiarism, and social security issues.\\nIn this work, we collected tens of thousands of comparison responses from both\\nhuman experts and ChatGPT, with questions ranging from open-domain, ﬁnancial,\\nmedical, legal, and psychological areas. We call the collected dataset the Human\\nChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the\\ncharacteristics of ChatGPT’s responses, the differences and gaps from human\\nexperts, and future directions for LLMs. We conducted comprehensive human\\nevaluations and linguistic analyses of ChatGPT-generated content compared with\\nthat of humans, where many interesting results are revealed. After that, we conduct\\nextensive experiments on how to effectively detect whether a certain text is gener-\\nated by ChatGPT or humans. We build three different detection systems, explore\\nseveral key factors that inﬂuence their effectiveness, and evaluate them in differ-\\nent scenarios. The dataset, code, and models are all publicly available at https:\\n//github.com/Hello-SimpleAI/chatgpt-comparison-detection.\\n1\\nIntroduction\\nSince its dazzling debut in November 2022, OpenAI’s ChatGPT has gained huge attention and wide\\ndiscussion in the natural language processing (NLP) community and many other ﬁelds. According\\nto OpenAI, ChatGPT is ﬁne-tuned from the GPT-3.5 series with Reinforcement Learning from\\nHuman Feedback (RLHF; [7, 32]), using nearly the same methods as InstructGPT [25], but with\\nslight differences in the data collection setup. The vast amount of knowledge in GPT-3.5 and the\\nmeticulous ﬁne-tuning based on human feedback enable ChatGPT to excel at many challenging NLP\\n∗Equal Contribution.\\n†Project Lead. Corresponding to guo_biyang@163.com\\n+Each author has made unique contributions to the project.\\n2Launched by OpenAI in November 2022. https://chat.openai.com/chat\\narXiv:2301.07597v1  [cs.CL]  18 Jan 2023\\ntasks, such as translating natural language to code [5], completing the extremely masked text [15] or\\ngenerating stories given user-deﬁned elements and styles [40], let alone typical NLP tasks like text\\nclassiﬁcation, entity extraction, translation, etc. Furthermore, the carefully collected human-written\\ndemonstrations also make ChatGPT able to admit its mistakes, challenge incorrect premises and\\nreject even inappropriate requests, as claimed by OpenAI3.\\nThe surprisingly strong capabilities of ChatGPT have raised many interests, as well as concerns:\\nOn the one hand, people are curious about how close is ChatGPT to human experts. Different\\nfrom previous LLMs like GPT-3 [4], which usually fails to properly respond to human queries,\\nInstructGPT [25] and the stronger ChatGPT have improved greatly in interactions with humans.\\nTherefore, ChatGPT has great potential to become a daily assistant for general or professional\\nconsulting purposes [20, 21]. From the linguistic or NLP perspectives, we are also interested in\\nwhere are the remaining gaps between ChatGPT and humans and what are their implicit linguistic\\ndifferences [14, 18].\\nOn the other hand, people are worried about the potential risks brought by LLMs like ChatGPT.\\nWith the free preview demo of ChatGPT going virus, a large amount of ChatGPT-generated content\\ncrowded into all kinds of UGC (User-Generated Content) platforms, threatening the quality and\\nreliability of the platforms. For example, Stack Overﬂow, the famous programming question-\\nanswering website, has temporarily banned ChatGPT-generated content4, because it believes \"the\\naverage rate of getting correct answers from ChatGPT is too low, the posting of answers created by\\nChatGPT is substantially harmful to the site and to users who are asking and looking for correct\\nanswers\". Many other applications and activities are facing similar issues, such as online exams [33]\\nand medical analysis [20]. Our empirical evaluation of ChatGPT on legal, medical, and ﬁnancial\\nquestions also reveals that potentially harmful or fake information can be generated.\\nConsidering the opaqueness of ChatGPT and the potential social risks associated with model misuse,\\nwe make the following contributions to both the academy and society:\\n1. To facilitate LLM-related research, especially the study on the comparison between humans\\nand LLMs, we collect nearly 40K questions and their corresponding answers from human\\nexperts and ChatGPT, covering a wide range of domains (open-domain, computer science,\\nﬁnance, medicine, law, and psychology), named as the Human ChatGPT Comparison\\nCorpus (HC3) dataset. The HC3 dataset is a valuable resource to analyze the linguistic and\\nstylist characteristics of both humans and ChatGPT, which helps to investigate the future\\nimprovement directions for LLMs;\\n2. We conduct comprehensive human evaluations as well as linguistic analysis on\\nhuman/ChatGPT-generated answers, discovering many interesting patterns exhibited by\\nhumans and ChatGPT. These ﬁndings can help to distinguish whether certain content is\\ngenerated by LLMs, and also provide insights about where language models should be\\nheading in the future;\\n3. Based on the HC3 dataset and the analysis, we develop several ChatGPT detecting models,\\ntargeting different detection scenarios. These detectors show decent performance in our\\nheld-out test sets. We also conclude several key factors that are essential to the detector’s\\neffectiveness.\\n4. We open-source all the collected comparison corpus, evaluations, and detection models, to\\nfacilitate future academic research and online platform regulations on AI-generated content.\\n2\\nHuman ChatGPT Comparison Corpus (HC3)\\nChatGPT is based on the GPT-3.5 series, which is pre-trained on the super-large corpus, consisting of\\nweb-crawled text, books, and codes, making it able to respond to all kinds of questions. Therefore,\\nwe are curious how will a human (especially an expert) and ChatGPT respond to the same question\\nrespectively. Inspired by [1], we also want to evaluate whether ChatGPT can keep honest (not\\nfabricate information or mislead the user), harmless (shouldn’t generate harmful or offensive content),\\n3https://openai.com/blog/chatgpt/\\n4https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned\\n2\\nHC3-English\\n# Questions\\n# Human Answers\\n# ChatGPT Answers\\nSource\\nAll\\n24322\\n58546\\n26903\\nreddit_eli5\\n17112\\n51336\\n16660\\nELI5 dataset [10]\\nopen_qa\\n1187\\n1187\\n3561\\nWikiQA dataset [39]\\nwiki_csai\\n842\\n842\\n842\\nCrawled Wikipedia (A.1)\\nmedicine\\n1248\\n1248\\n1337\\nMedical Dialog dataset [6]\\nﬁnance\\n3933\\n3933\\n4503\\nFiQA dataset [23]\\nHC3-Chinese\\n# Questions\\n# Human Answers\\n# ChatGPT Answers\\nSource\\nAll\\n12853\\n22259\\n17522\\nopen_qa\\n3293\\n7377\\n3991\\nWebTextQA & BaikeQA [38]\\nbaike\\n4617\\n4617\\n4617\\nCrawled BaiduBaike (A.1)\\nnlpcc_dbqa\\n1709\\n1709\\n4253\\nNLPCC-DBQA dataset [8]\\nmedicine\\n1074\\n1074\\n1074\\nMedical Dialog dataset [6]\\nﬁnance\\n689\\n1572\\n1983\\nChineseNlpCorpus (A.1)\\npsychology\\n1099\\n5220\\n1099\\nfrom Baidu AI Studio (A.1)\\nlaw\\n372\\n690\\n505\\nLegalQA dataset (A.1)\\nTable 1: Meta-information of the HC3 dataset. The English (resp. Chinese) contains 5 (resp. 7) splits.\\nand how helpful (provide concrete and correct solutions to the user’s question) it is compared to\\nhuman experts.\\nTaking these into account, we decided to collect a comparison corpus that consists of both human and\\nChatGPT answers to the same questions. We believe such a comparison corpus can be a valuable and\\ninteresting source to study the nature of the language of both humans and language models.\\n2.1\\nHuman Answers Collection\\nInviting human experts to manually write questions and answers is tedious and unaffordable for us\\nto collect a large amount of data, therefore we construct the comparison dataset mainly from two\\nsources:\\n• Publicly available question-answering datasets, where answers are given by experts in speciﬁc\\ndomains or the high-voted answers by web users;\\n• Wiki text. We construct question-answer pairs using the concepts and explanations from wiki\\nsources like Wikipedia5 and BaiduBaike6.\\nThe split-data source mapping is shown in Table 1, and please refer to Appendix A.1 for further\\ndetailed information.\\n2.2\\nChatGPT Answers Collection\\nBased on the collected human question-answering datasets, we use ChatGPT to generate answers\\nto these questions. Since the ChatGPT is currently only available through its preview website, we\\nmanually input the questions into the input box, and get the answers, with the aid of some automation\\ntesting tools. Answers by ChatGPT can be inﬂuenced by the chatting history, so we refresh the thread\\nfor each question.\\nTo make the answer more aligned with human answers, we add additional instructions to ChatGPT\\nfor speciﬁc datasets. For example, the human answers from the reddit-eli5 dataset split are under\\nthe context of \"Explain like I’m ﬁve\", therefore we use this context to instruct ChatGPT by adding\\n\"Explain like I’m ﬁve\" at the end of the original question. More detail can be found in the Appendix.\\n5https://www.wikipedia.org/\\n6https://baike.baidu.com/\\n3\\nChatGPT can generate different answers given the same question in different threads, which is\\nperhaps due to the random sampling in the decoding process. However, we found the differences can\\nbe very small, thereby we only collect one answer for most questions.\\n2.3\\nHuman ChatGPT Comparison Corpus (HC3)\\nFor each question, there can be more than one human/ChatGPT answer, therefore we organize the\\ncomparison data using the following format:\\n1 {\\n2\\n\"question \": \"Q1\",\\n3\\n\"human_answers \": [\"A1\", \"A2\"],\\n4\\n\" chatgpt_answers \": [\"B1\"]\\n5 }\\nOverall, we collected 24, 322 questions, 58, 546 human answers and 26, 903 ChatGPT answers for\\nthe English version, and 12, 853 questions, 22, 259 human answers and 17, 522 ChatGPT answers\\nfor the Chinese version. The meta-information of each dataset split is illustrated in Table 1.\\n3\\nHuman Evaluation & Summarization\\nIn this section, we invite many volunteer testers and conduct extensive human evaluations from\\ndifferent aspects. After the human evaluation, we make our collected comparison corpus available\\nto the volunteers and ask them to manually conclude some characteristics. We then summarize the\\nfeedback from the volunteers combined with our observations.\\n3.1\\nHuman Evaluation\\nThe human evaluation is divided into the Turing test and the Helpfulness Test. The Turing Test\\n[34] is a test of a machine’s ability to exhibit intelligent behavior that is indistinguishable from a\\nhuman. We invite 17 volunteers, divided into two groups: 8 experts (who are frequent users of\\nChatGPT) and 9 amateurs (who have never heard of ChatGPT). This is because people who are\\nfamiliar with ChatGPT may have memorized some patterns exhibited by ChatGPT, helping them to\\neasily distinguish the role.\\nWe designed four types of evaluations, using different query formats or testing groups. We introduce\\nthe speciﬁc evaluation design and results in the following parts:\\nA. Expert Turing Test, Paired Text (pair-expert)\\nThe pair-expert test is conducted in the expert group. Each tester is required to do a series of\\ntests, each test containing one question and a pair of answers (one from humans and another from\\nChatGPT). The tester needs to determine which answer is generated by ChatGPT.\\nB. Expert Turing Test, Single Text (single-expert)\\nThe single-expert test is also conducted in the expert group. Each tester is required to do a\\nseries of tests, each test containing one question and a single answer randomly given by humans or\\nChatGPT. The tester needs to determine whether the answer is generated by ChatGPT.\\nC. Amateur Turing Test, Single Text (single-amateur)\\nThe single-amateur test is conducted in the amateur group. Each tester is required to do a series\\nof tests, each test containing one question and a single answer randomly given by humans or ChatGPT.\\nThe tester needs to determine whether the answer is generated by ChatGPT.\\nD. Helpfulness Test (helpfulness)\\nWe are also curious about how helpful are the answers from ChatGPT compared with humans’\\nanswers to one question. Note that helpfulness is a very subjective metric, which can be inﬂuenced\\nby many factors, including emotion, tester personality, personal preference, etc. Therefore, simply\\nproviding more accurate information or a more detailed analysis may not always lead to a more\\nhelpful answer.\\nThe helpfulness test is conducted in the expert group. Each tester is required to do a series of tests,\\neach containing one question and a pair of answers (one from human and another from ChatGPT).\\n4\\nHuman Evaluation (En)\\nPair-expert\\nSingle-expert\\nSingle-amateur\\nHelpfulness\\nAll\\n0.90\\n0.81\\n0.48\\n0.57\\nreddit_eli5\\n0.97\\n0.94\\n0.57\\n0.59\\nopen_qa\\n0.98\\n0.78\\n0.34\\n0.72\\nwiki_csai\\n0.97\\n0.61\\n0.39\\n0.71\\nmedical\\n0.97\\n0.97\\n0.50\\n0.23\\nﬁnance\\n0.79\\n0.73\\n0.58\\n0.60\\nHuman Evaluation (Zh)\\nPair-expert\\nSingle-expert\\nSingle-amateur\\nHelpfulness\\nAll\\n0.93\\n0.86\\n0.54\\n0.54\\nopen_qa\\n1.00\\n0.92\\n0.47\\n0.50\\nbaike\\n0.76\\n0.64\\n0.60\\n0.60\\nnlpcc_dbqa\\n1.00\\n0.90\\n0.13\\n0.63\\nmedicine\\n0.93\\n0.93\\n0.57\\n0.30\\nﬁnance\\n0.86\\n0.84\\n0.84\\n0.75\\npsychology\\n1.00\\n1.00\\n0.60\\n0.67\\nlaw\\n1.00\\n0.77\\n0.56\\n0.56\\nTable 2: Human evaluations of ChatGPT generated answers for both English and Chinese.\\nEach tester is asked to pretend that the question is proposed by him/herself, and needs to determine\\nwhich answer is more helpful to him/her.\\nSettings.\\nWe sample around 30 <question, human_answer, chatgpt_answer> triplets from\\neach split (i.e., reddit_eli5, wikipedia, medical, etc.) as the samples for the human evaluation. We\\nallocate 2-5 testers for each split and report their average results. For all Turing tests, we report the\\nproportion that ChatGPT-generated answer is correctly detected by testers. For the helpfulness test,\\nwe report the proportion that ChatGPT-generated answer is considered to be more helpful.\\nResults.\\nSeveral conclusions can be drawn from the results shown in Table 2. Comparing the\\nresults of pair-expert and single-expert, we can ﬁnd that it is easier to distinguish ChatGPT-\\ngenerated content when providing a comparison pair than only providing a single answer. Com-\\nparing the results of single-expert and single-amateur, we can ﬁnd that the accuracy of\\nexperts is much higher than that of amateurs. The helpfulness test gives the proportion of\\nquestions that volunteers think the ChatGPT answer is more helpful to them. Surprisingly, results\\nshow that ChatGPT’s answers are generally considered to be more helpful than humans’ in\\nmore than half of questions, especially for ﬁnance and psychology areas. By checking the speciﬁc\\nanswers in these domains, we ﬁnd that ChatGPT can usually provide more concrete and speciﬁc\\nsuggestions. However, ChatGPT performs poorly in terms of helpfulness for the medical domain\\nin both English and Chinese. The ChatGPT often gives lengthy answers to medical consulting in\\nour collected dataset, while human experts may directly give straightforward answers or suggestions,\\nwhich may partly explain why volunteers consider human answers to be more helpful in the medical\\ndomain.\\n3.2\\nHuman Summarization\\nAfter the above evaluations, we open our collected HC3 dataset to the volunteers where they can\\nfreely browse the comparison answers from humans and ChatGPT. All dataset splits are allocated\\nto different volunteers, and each volunteer is asked to browse at least 100 groups of comparison\\ndata. After that, we ask them to summarize the characteristics of both human answers and ChatGPT\\nanswers. Eventually, we received more than 200 feedbacks, and we summarize these ﬁndings as\\nfollows:\\n5\\nDistinctive Patterns of ChatGPT\\n(a) ChatGPT writes in an organized manner, with clear logic. Without loss of generality,\\nChatGPT loves to deﬁne the core concept in the question. Then it will give out detailed\\nanswers step by step and offers a summary at the end, following the deduction and summary\\nstructure;\\n(b) ChatGPT tends to offer a long and detailed answer. This is the direct product of the\\nReinforcement Learning with Human Feedback, i.e. RLHF, and also partly related to the\\npattern (a) unless you offer a prompt such as \"Explain it to me in one sentence\";\\n(c) ChatGPT shows less bias and harmful information. ChatGPT is neutral on sensitive\\ntopics, barely showing any attitude towards the realm of politics or discriminatory toxic\\nconversations;\\n(d) ChatGPT refuses to answer the question out of its knowledge. For instance, ChatGPT\\ncannot respond to queries that require information after September 2021. Sometimes\\nChatGPT also refuses to answer what it believes it doesn’t know. It is also RLHF’s ability to\\nimplicitly and automatically determine which information is within the model’s knowledge\\nand which is not.\\n(e) ChatGPT may fabricate facts. When answering a question that requires professional\\nknowledge from a particular ﬁeld, ChatGPT may fabricate facts in order to give an answer,\\nthough [25] mentions that InstructGPT model has already shown improvements in truthful-\\nness over GPT-3. For example, in legal questions, ChatGPT may invent some non-existent\\nlegal provisions to answer the question. This phenomenon warns us to be extra careful when\\nusing ChatGPT for professional consultations. Additionally, when a user poses a question\\nthat has no existing answer, ChatGPT may also fabricate facts in order to provide a response.\\nMany of the conclusions mentioned above like (b),(c),(d) are also discussed in [12] by Fu et al.\\nMajor Differences between Human and ChatGPT\\n(a) ChatGPT’s responses are generally strictly focused on the given question, whereas\\nhumans’ are divergent and easily shift to other topics. In terms of the richness of\\ncontent, humans are more divergent in different aspects, while ChatGPT prefers focusing on\\nthe question itself. Humans can answer the hidden meaning under the question based on\\ntheir own common sense and knowledge, but the ChatGPT relies on the literal words of the\\nquestion at hand;\\n(b) ChatGPT provides objective answers, while humans prefer subjective expressions.\\nGenerally, ChatGPT generates safer, more balanced, neutral, and informative texts compared\\nto humans. As a result, ChatGPT is excellent at interpreting terminology and concepts. On\\nthe other hand, human answers are more speciﬁc and include detailed citations from sources\\nbased on legal provisions, books, and papers, especially when providing suggestions for\\nmedical, legal, and technical problems, etc.;\\n(c) ChatGPT’s answers are typically formal, meanwhile humans’ are more colloquial.\\nHumans tend to be more succinct with full of oral abbreviations and slang such as \"LOL\",\\n\"TL;DR\", \"GOAT\" etc. Humans also love to apply humor, irony, metaphors, and exam-\\nples, whereas ChatGPT never uses antiphrasis. Additionally, human communication often\\nincludes the \"Internet meme\" as a way to express themselves in a speciﬁc and vivid way;\\n(d) ChatGPT expresses less emotion in its responses, while human chooses many punctu-\\nation and grammar feature in context to convey their feelings. Human uses multiple\\nexclamation mark(’!’), question mark(’?’), ellipsis(’...’) to express their strong emotion, and\\nuse various brackets(’(’, ’)’, ’[’, ’]’) to explain things. By contrast, ChatGPT likes to use\\nconjunctions and adverbs to convey a logical ﬂow of thought, such as \"In general\", \"on the\\nother hand\", \"Firstly,..., Secondly,..., Finally\" and so on.\\nOverall, these summarised features indicate that ChatGPT has improved notably in question-\\nanswering tasks for a wide range of domains. Compared with humans, we can imagine ChatGPT\\nas a conservative team of experts. As a \"team\", it may lack individuality but can have a more\\ncomprehensive and neutral view towards questions.\\n6\\nEnglish\\navg. len.\\nvocab size\\ndensity\\nChinese\\navg. len.\\nvocab size\\ndensity\\nhuman\\nAll\\n142.50\\n79157\\n2.33\\nAll\\n102.27\\n75483\\n5.75\\nChatGPT\\n198.14\\n66622\\n1.41\\n115.3\\n45168\\n3.05\\nhuman\\nreddit_eli5\\n134.21\\n55098\\n2.46\\nnlpcc_dbqa\\n24.44\\n10621\\n25.43\\nChatGPT\\n194.84\\n44926\\n1.38\\n78.21\\n11971\\n8.96\\nhuman\\nopen_qa\\n35.09\\n9606\\n23.06\\nopen_qa\\n93.68\\n40328\\n13.13\\nChatGPT\\n131.68\\n16251\\n10.40\\n150.66\\n26451\\n5.35\\nhuman\\nwiki_csai\\n229.34\\n15859\\n8.21\\nbaike\\n112.25\\n28966\\n5.59\\nChatGPT\\n208.33\\n9741\\n5.55\\n77.19\\n14041\\n3.94\\nhuman\\nmedicine\\n92.98\\n11847\\n10.42\\nmedicine\\n92.34\\n9855\\n9.94\\nChatGPT\\n209.61\\n7694\\n3.00\\n165.41\\n7211\\n4.06\\nhuman\\nﬁnance\\n202.07\\n25500\\n3.21\\nﬁnance\\n80.76\\n2759\\n5.05\\nChatGPT\\n226.01\\n21411\\n2.41\\n120.84\\n4043\\n4.94\\nhuman\\n-\\n-\\n-\\n-\\npsychology\\n254.82\\n16160\\n5.77\\nChatGPT\\n-\\n-\\n-\\n-\\n164.53\\n5897\\n3.26\\nhuman\\n-\\n-\\n-\\n-\\nlaw\\n28.77\\n2093\\n19.55\\nChatGPT\\n-\\n-\\n-\\n-\\n143.76\\n3857\\n7.21\\nTable 3: Average answer length, vocabulary size and density comparisons on our corpus.\\n4\\nLinguistic Analysis\\nIn this section, we analyze the linguistic features of both humans’ and ChatGPT’s answers, and try to\\nﬁnd some statistical evidence for the characteristics concluded in Section 3.\\n4.1\\nVocabulary Features\\nIn this part, we analyze the vocabulary features of our collected corpus. We are interested in how\\nhumans and ChatGPT differ in the choice of words when answering the same set of questions.\\nSince the number of human/ChatGPT answers is unbalanced, we randomly sample one answer from\\nhumans and one answer from ChatGPT during our statistical process. We calculated the following\\nfeatures: average length (L), which is the average number of words in each question; vocab size (V ),\\nthe number of unique words used in all answers; we also propose another feature called density (D),\\nwhich is calculated by D = 100 × V/(L × N) where N is the number of answers. Density measures\\nhow crowded different words are used in the text. For example, if we write some articles that add up\\nto 1000 words, but only 100 different words are used, then the density is 100 × 100/1000 = 10. The\\nhigher the density is, the more different words are used in the same length of text.\\nIn Table 3, we report the vocabulary features for both English and Chinese corpus. Looking at both\\nfeatures of average length and vocab size, we can see that: compared to ChatGPT, human answers\\nare relatively shorter, but a larger vocabulary is used. This phenomenon is particularly obvious\\nin the Chinese open_qa split and the medical splits in both languages, where the average length of\\nChatGPT is nearly twice longer than that of humans, but the vocab size is signiﬁcantly smaller.\\nThis phenomenon is also reﬂected by the density factor. The word density of humans is greater than\\nChatGPT’s in every split, which further reveals that humans use a more diverse vocabulary in\\ntheir expressions.\\n4.2\\nPart-of-Speech & Dependency Analysis\\nIn this part, we compare the occurrences of different part-of-speech (POS) tags and the characteristics\\nof the dependency relations.\\n4.2.1\\nPart-of-Speech\\nFigure 1 illustrates the comparisons between humans and ChatGPT in terms of POS usage. In\\nHC3-English, ChatGPT uses more NOUN, VERB, DET, ADJ, AUX, CCONJ and PART words, while using\\nless ADV and PUNCT words.\\n7\\nNOUN\\nPUNCT\\nVERB\\nADP\\nDET\\nPRON\\nADJ\\nAUX\\nADV\\nCCONJ\\nPROPN\\nPART\\nSCONJ\\nNUM\\nSPACE\\nSYM\\nX\\nINTJ\\n0\\n5\\n10\\n15\\n20\\n25\\nProportion (%)\\n18.7\\n12.4\\n11.2\\n9.1\\n8.4\\n8.3\\n6.8\\n6.3\\n4.9\\n3.1\\n3.0\\n2.8\\n2.4\\n1.4\\n0.4\\n0.3\\n0.2\\n0.2\\n21.1\\n9.6\\n11.7\\n9.3\\n9.1\\n7.3\\n7.9\\n7.0\\n3.3\\n4.1\\n2.2\\n3.5\\n2.6\\n0.6\\n0.6\\n0.1\\n0.1\\n0.0\\nPart-of-Speech Comparison (En)\\nHuman\\nChatGPT\\nNOUN\\nVERB\\nPUNCT\\nADV\\nPART\\nNUM\\nPRON\\nADP\\nPROPN\\nADJ\\nCCONJ\\nDET\\nSPACE\\nSCONJ\\nX\\n_\\nINTJ\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nProportion (%)\\n26.0\\n20.8\\n15.0\\n9.7\\n8.7\\n4.2\\n3.6\\n2.8\\n2.6\\n2.3\\n1.4\\n1.2\\n0.9\\n0.4\\n0.4\\n0.0\\n0.0\\n27.5\\n20.3\\n13.4\\n7.5\\n8.7\\n3.0\\n4.9\\n3.5\\n2.4\\n2.4\\n2.9\\n1.5\\n0.8\\n0.7\\n0.3\\n0.0\\n0.0\\nPart-of-Speech Comparison (Zh)\\nHuman\\nChatGPT\\nFigure 1: Part-of-Speech distribution comparison between ChatGPT and human answers. Results are\\nsorted by POS proportion of human answers. The upper ﬁgure is for the HC3-English dataset and the\\nlower is for the HC3-Chinese dataset.\\nA high proportion of nouns (NOUN) often indicates that the text is more argumentative, exhibiting\\ninformativeness and objectivity [24]. Accordingly, adposition (ADP) and adjective (ADJ) words\\nalso tend to appear more frequently [11]. The frequent co-occurrence of conjunctions (CCONJ)\\nalong with nouns, verbs, and adposition words indicates that the structure of the article and the\\nrelationships of cause-and-effect, progression, or contrast are clear. The above are also typical\\ncharacteristics in academic papers or ofﬁcial documents [29]. We believe the RLHF training process\\nhas a great inﬂuence on ChatGPT’s writing style, which partly explains the difference in the POS\\ntags distribution.\\n4.2.2\\nDependency Parsing\\nDependency parsing is a technique that analyzes the grammatical structure of a sentence by identifying\\nthe dependencies between its words. We parse the answers in the corpus and compare the proportion\\nof different dependency relations and their corresponding dependency distances. Figure 2 shows the\\ncomparison between humans and ChatGPT in HC3-English. Due to the limited space, the Chinese\\nversion is placed in the Appendix A.2.\\nThe comparison of dependency relations exhibits similar characteristics to that of POS tags, where\\nChatGPT uses more determination, conjunction, and auxiliary relations. In terms of the dependency\\ndistance, ChatGPT has much longer distances for the punct and dep relations, which is perhaps due\\nto the fact that CharGPT tends to use longer sentences. However, ChatGPT has obviously shorter\\nconj relations. According to the analysis of POS tags, ChatGPT usually uses more conjunctions than\\nhumans to make the content more logical, this may explain why the conj relations of ChatGPT are\\nrelatively shorter than humans.\\n4.3\\nSentiment Analysis\\nHumans are emotional beings, it is natural that our emotions are reﬂected in our words, to some\\nextent. ChatGPT is learned on large-scale human-generated text, but it is further ﬁne-tuned with\\nhuman instructions. Therefore we are curious how \"emotional\" ChatGPT is compared with humans.\\nWe use a multilingual sentiment classiﬁcation model7 ﬁne-tuned on Twitter corpus [2] to conduct\\nsentiment analysis for both English and Chinese comparison data. Note that deep learning-based\\nmodels can be greatly inﬂuenced by some indicating words (such as \"but\" and \"sorry\" can easily\\n7https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment\\n8\\npunct\\nprep\\ndet\\nnsubj\\npobj\\nadvmod\\namod\\ndobj\\nROOT\\naux\\nconj\\ncompoundcc\\nadvcl\\nmark\\nccomp\\nposs\\nattr\\nxcomp\\nrelcl\\nacomp\\nneg\\nauxpass\\nnummod\\nnsubjpass\\npcomp\\ndep\\nappos\\nacl\\nprt\\nnpadvmod\\nnmod\\nexpl\\ncase\\nagent\\nquantmod\\noprd\\nintj\\ndative\\ncsubj\\npredet\\nparataxis\\npreconj\\nmeta\\ncsubjpass\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nProportion (%)\\n12.5\\n8.3\\n8.3\\n8.1\\n7.7\\n5.5\\n5.3\\n4.9\\n4.9\\n4.4\\n3.4\\n3.1\\n3.0\\n1.9\\n1.8\\n1.7\\n1.3\\n1.3\\n1.2\\n1.2\\n1.2\\n1.0\\n0.9\\n0.8\\n0.8\\n0.6\\n0.6\\n0.6\\n0.5\\n0.5\\n0.5\\n0.4\\n0.3\\n0.2\\n0.2\\n0.2\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.0\\n0.0\\n0.0\\n9.7\\n8.7\\n9.0\\n7.5\\n8.2\\n3.9\\n5.8\\n5.2\\n3.9\\n5.6\\n4.5\\n3.0\\n4.1\\n2.0\\n1.9\\n1.5\\n1.5\\n1.2\\n1.9\\n1.7\\n1.6\\n0.6\\n1.3\\n0.4\\n1.1\\n0.7\\n0.7\\n0.3\\n0.5\\n0.3\\n0.2\\n0.2\\n0.2\\n0.3\\n0.2\\n0.1\\n0.2\\n0.0\\n0.1\\n0.1\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\nDependency Relation Comparison (En)\\nHuman\\nChatGPT\\npunct\\nprep\\ndet\\nnsubj\\npobj\\nadvmod\\namod\\ndobj\\nROOT\\naux\\nconj\\ncompoundcc\\nadvcl\\nmark\\nccomp\\nposs\\nattr\\nxcomp\\nrelcl\\nacomp\\nneg\\nauxpass\\nnummod\\nnsubjpass\\npcomp\\ndep\\nappos\\nacl\\nprt\\nnpadvmod\\nnmod\\nexpl\\ncase\\nagent\\nquantmod\\noprd\\nintj\\ndative\\ncsubj\\npredet\\nparataxis\\npreconj\\nmeta\\ncsubjpass\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\nProportion (%)\\n7.3\\n2.4\\n1.6\\n2.0\\n2.1\\n2.3\\n1.4\\n2.3\\n0.0\\n1.4\\n7.0\\n1.1\\n4.6\\n8.6\\n3.7\\n7.2\\n1.5\\n2.9\\n2.5\\n3.6\\n1.7\\n1.2\\n1.2\\n1.4\\n3.5\\n1.9\\n3.6\\n4.5\\n2.6\\n1.2\\n3.9\\n2.4\\n1.1\\n1.0\\n1.4\\n1.5\\n2.8\\n4.0\\n1.7\\n5.6\\n2.3\\n7.2\\n1.8\\n5.1\\n7.1\\n9.4\\n2.3\\n1.5\\n2.0\\n2.1\\n2.3\\n1.3\\n2.2\\n0.0\\n1.2\\n5.5\\n1.1\\n3.6\\n8.8\\n3.7\\n6.7\\n1.6\\n3.0\\n2.7\\n3.7\\n1.4\\n1.2\\n1.2\\n1.4\\n3.2\\n1.6\\n9.9\\n4.4\\n2.5\\n1.2\\n3.4\\n2.8\\n1.1\\n1.0\\n1.2\\n1.3\\n2.4\\n3.8\\n2.1\\n6.5\\n2.3\\n6.4\\n1.6\\n5.2\\n8.4\\nDependency Distance Comparison (En)\\nHuman\\nChatGPT\\nFigure 2: Top-30 dependency relations (upper) and corresponding dependency distances (lower)\\ncomparison between human and ChatGPT answers in HC3-English. Results are sorted by relations\\nproportion of human answers.\\nNeutral\\nPositive\\nNegative\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nProportion (%)\\n54\\n4\\n44\\n72\\n3\\n25\\nSentiment Distribution (English)\\nHuman\\nChatGPT\\n(a) Sentiment distribution of HC3-English\\nNeutral\\nPositive\\nNegative\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nProportion (%)\\n67\\n5\\n27\\n78\\n4\\n18\\nSentiment Distribution (Chinese)\\nHuman\\nChatGPT\\n(b) Sentiment distribution of the HC3-Chinese\\nFigure 3: Proportions of three kinds of sentiments (neutral, positive, and negative) in our corpus.\\nfool the classiﬁer to predict the \"negative\" label), making the predictions biased [16]. Therefore, the\\nsentiment given by the classiﬁer is only a reference to the true sentiment behind the text.\\nFigure 3 shows the comparison of the sentiment distribution of humans and ChatGPT. Several ﬁndings\\ncan be drawn from the results: First, we ﬁnd that the proportion of neutral emotions is the largest for\\nboth humans and ChatGPT, which is in line with our expectations. However, ChatGPT generally\\nexpresses more neutral sentiments than humans. Then, the proportion of negative emotions is\\nsigniﬁcantly higher than that of positive emotions. Notably, humans express signiﬁcantly more\\nnegative emotions than ChatGPT. The proportion of humans’ positive emotions is also slightly\\nhigher than that of ChatGPT. Overall, ChatGPT is less emotional than humans, though it is not\\ncompletely emotionless.\\n4.4\\nLanguage Model Perplexity\\nThe perplexity (PPL) is commonly used as a metric for evaluating the performance of language\\nmodels (LM). It is deﬁned as the exponential of the negative average log-likelihood of the text under\\n9\\n1\\n25\\n50\\n75\\n110\\n150\\n190\\n0.00\\n0.01\\n0.02\\n0.03\\n0.04\\nProportion\\nlabel\\nChatGPT\\nHuman\\n(a) English text ppl\\n1\\n25\\n50\\n75\\n110\\n150\\n190\\n0.000\\n0.005\\n0.010\\n0.015\\n0.020\\n0.025\\n0.030\\nProportion\\nlabel\\nChatGPT\\nHuman\\n(b) English sent ppl\\n1 2 3 4 5 6 7 8\\n10\\n12\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nProportion\\nlabel\\nChatGPT\\nHuman\\n(c) Chinese text ppl\\n1 2 3 4 5 6 7 8\\n10\\n12\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nProportion\\nlabel\\nChatGPT\\nHuman\\n(d) Chinese sent ppl\\nFigure 4: PPL distributions on both English and Chinese data, as well as both text and sentence levels.\\nthe LM. A lower PPL indicates that the language model is more conﬁdent in its predictions, and\\nis therefore considered to be a better model. The training of LMs is carried out on large-scale text\\ncorpora, it can be considered that it has learned some common language patterns and text structures.\\nTherefore, we can use PPL to measure how well a text conforms to common characteristics.\\nWe use the open-source GPT-2 small8 (Wenzhong-GPT2-110M9 for Chinese) model to compute the\\nPPL (both text-level and sentence-level10 PPLs) of the collected texts. The PPL distributions of text\\nwritten by humans and text generated by ChatGPT are shown in Figure 4.\\nIt is clearly observed that, regardless of whether it is at the text level or the sentence level, the content\\ngenerated by ChatGPT has relatively lower PPLs compared to the text written by humans. ChatGPT\\ncaptured common patterns and structures in the text it was trained on, and is very good at reproducing\\nthem. As a result, text generated by ChatGPT have relatively concentrated low PPLs.\\nHumans have the ability to express themselves in a wide variety of ways, depending on the context,\\naudience, and purpose of the text they are writing. This can include using creative or imaginative\\nelements, such as metaphors, similes, and unique word choices, which can make it more difﬁcult for\\nGPT2 to predict. Therefore, human-written texts have more high-PPL values, and show a long-tailed\\ndistribution, as demonstrated in Figure 4.\\n5\\nChatGPT Content Detection\\nAI-generated content (AIGC) is becoming increasingly prevalent on the internet, and it can be\\ndifﬁcult to distinguish it from human-generated content, as shown in our human evaluation (sec 3.1).\\nTherefore, AIGC detectors are needed to help identify and ﬂag content that has been created by a\\nmachine, to reduce the potential risks to society caused by improper or malicious use of AI models,\\nand to improve the transparency and accountability of the information that is shared online.\\nIn this section, we conduct several empirical experiments to investigate the ChatGPT content detection\\nsystems. Detecting AI-generated content is a widely studied topic [19, 27]. Based on these [30, 13,\\n27], we establish three different types of detection systems, including machine learning-based and\\ndeep learning-based methods, and evaluate them on different granularities and data sources. Detailed\\nresults and discussions are provided.\\n5.1\\nMethods\\nDetection of machine-generated text has been gaining popularity as text generation models have\\nadvanced in recent years[19, 27]. Here, we implement three representative methods from classic\\nmachine learning and deep learning, i.e, a logistic regression model trained on the GLTR Test-2[13]\\nfeatures, a deep classiﬁer for single-text detection and a deep classiﬁer for QA detection. The deep\\nclassiﬁers for both single-text and QA are based on RoBERTa [22], a strong pre-trained Transformer\\n[35] model. In fact, algorithms for OOD detection or anomaly detection [17] can also be applied to\\ndevelop ChatGPT content detectors, which we leave for future work.\\n8https://huggingface.co/gpt2\\n9https://huggingface.co/IDEA-CCNL/Wenzhong-GPT2-110M\\n10For English text, we used NLTK[3] for sentence segmentation (HarvestText for Chinese).\\n10\\nFigure 5: The experiment design for the training and testing of detectors. Different dataset versions\\nare generated through ﬁltering or splitting.\\nGLTR.\\n[13] studied three tests to compute features of an input text. Their major assumption is that\\nto generate ﬂuent and natural-looking text, most decoding strategies sample high probabilities tokens\\nfrom the head of the distribution. We select the most powerful Test-2 feature, which is the number\\nof tokens in the Top-10, Top-100, Top-1000, and 1000+ ranks from the LM predicted probability\\ndistributions. And then a logistic regression model is trained to ﬁnish the classiﬁcation.\\nRoBERTa-sinlge.\\nA deep classiﬁer based on the pre-trained LM is always a good choice for\\nthis kind of text classiﬁcation problem. It is also investigated in many studies and demo systems\\n[30, 9, 27]. Here we ﬁne-tune the RoBERTa [22] model.\\nRoBERTa-QA.\\nWhile most content detectors are developed to classify whether a single piece of\\ntext is AI-generated, we claim that a detector that supports inputting both a question and an answer\\ncan be quite useful, especially for question-answering scenarios. Therefore, we decide to also build\\na QA version detector. The RoBERTa model supports a text pair input format, where a separating\\ntoken is used to join a question and its corresponding answer.\\n5.2\\nImplementation Details\\nFor the LM used by GLTR, we use gpt2-small [28] for English, and Wenzhong-GPT2-110M released\\nby [36] for Chinese, it is the same with sec. 4.4. For RoBERTa-based deep classiﬁers, we use\\nroberta-base11 and hfl/chinese-roberta-wwm-ext12 checkpoints for English and Chinese,\\nrespectively. All the above models are obtained from huggingface transformers [37].\\nWe train the logistic regression model by sklearn [26] on the GLTR Test-2 features from trainset, and\\nsearch hyper-params following the code of [27]. The RoBERTa-based detectors are trained by the\\nfacilities of transformers. Speciﬁcally, we use the AdamW optimizer, setting batch size to 32 and\\nlearning rate to 5e −5. We ﬁnetune models by 1 epoch for English, and 2 epochs for Chinese.\\n5.3\\nExperiment Design\\nThe HC3 dataset consists of questions and their corresponding human/ChatGPT answers. We\\nextracted all the <question, answer> pairs, and assigned label 0 to pairs with human answers and\\nlabel 1 to pairs with ChatGPT answers.\\nSimply using the original answers from humans and ChatGPT to train a binary classiﬁer is the most\\nstraightforward way. However, there might be some issues by doing so:\\n• First, based on the observations in Section 3, both human answers and ChatGPT answers\\nmay contain some obvious indicating words that may inﬂuence the effectiveness of models;\\n• Second, users may want to detect whether a single sentence is generated by ChatGPT,\\ninstead of the full text. This can be quite difﬁcult for a classiﬁer that is only trained on full\\ntexts;\\n• Third, taking the corresponding question of the answer into account may help the detector\\nto make a more accurate judgment, compared with only considering the answer itself. This\\n11https://huggingface.co/roberta-base\\n12https://huggingface.co/hfl/chinese-roberta-wwm-ext\\n11\\ncan be widely applied to many QA platforms (like Quora, Stack Overﬂow, and Zhihu) to\\nﬁnd out which answer below a certain question is generated by AI.\\nTherefore, we design different groups of experiments to study these key questions:\\n• How will the indicating words inﬂuence the detector?\\n• Is it more challenging for the ChatGPT detectors to detect sentence-level content? Is it harder to\\ntrain a sentence-level classiﬁer?\\n• Can the corresponding question help detectors detect the origin of the answer more accurately?\\nFigure 5 shows how we generate different types of training and testing sets. Speciﬁcally, we use\\nthe collected raw corpus to construct the ﬁrst train-test sets (the \"full text (raw)\" in the ﬁgure),\\nwhich we call the raw-full version. Then we ﬁlter away the indicated words in the text to obtain the\\nﬁltered-full version. By splitting the full text into sentences, we obtain the raw-sent version and the\\nﬁltered-sent version. We also combine the full text and the sentences into a mixed version, namely\\nthe raw-mix and ﬁltered-mix version. Overall, we have six different versions of training and testing\\nsets. Evaluating a model’s performance on version B’s testing set which is trained on version A’s\\ntraining set can be seen as an out-of-distribution (OOD) generalization evaluation, which is more\\nchallenging since it requires the model to be robust when facing sample style changes.\\n5.4\\nResults\\nFollowing the above experiment design, we conduct comprehensive empirical studies on all kinds of\\nderived corpus. Table 4 shows the test F1 scores.\\nEnglish\\nChinese\\nTest →\\nraw\\nﬁltered\\nAvg.\\nraw\\nﬁltered\\nAvg.\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nTrain ↓\\nRoBERTa\\nraw\\nfull\\n99.82\\n81.89\\n84.67\\n99.72\\n81.00\\n84.07\\n88.53\\n98.79\\n83.64\\n86.32\\n98.57\\n82.77\\n85.85\\n89.32\\nsent\\n99.40\\n98.43\\n98.56\\n99.24\\n98.47\\n98.59\\n98.78\\n97.76\\n95.75\\n96.11\\n97.68\\n95.31\\n95.77\\n96.40\\nmix\\n99.44\\n98.31\\n98.47\\n99.32\\n98.37\\n98.51\\n98.74\\n97.70\\n95.68\\n96.04\\n97.65\\n95.27\\n95.73\\n96.35\\nﬁltered\\nfull\\n99.82\\n87.17\\n89.05\\n99.79\\n86.60\\n88.67\\n91.85\\n98.25\\n91.04\\n92.30\\n98.14\\n91.15\\n92.48\\n93.89\\nsent\\n96.97\\n97.22\\n97.19\\n99.09\\n98.43\\n98.53\\n97.91\\n96.60\\n92.81\\n93.47\\n97.94\\n95.86\\n96.26\\n95.49\\nmix\\n96.28\\n96.43\\n96.41\\n99.45\\n98.37\\n98.53\\n97.58\\n97.43\\n94.09\\n94.68\\n97.66\\n95.61\\n96.01\\n95.91\\nTrain ↓\\nGLTR Test-2\\nraw\\nfull\\n98.26\\n71.58\\n76.15\\n98.22\\n70.19\\n75.23\\n81.61\\n89.61\\n44.02\\n53.72\\n85.89\\n43.58\\n53.62\\n61.74\\nsent\\n86.26\\n88.18\\n87.96\\n87.72\\n88.23\\n88.19\\n87.76\\n84.49\\n71.79\\n74.01\\n84.06\\n70.29\\n72.90\\n76.26\\nmix\\n95.97\\n86.45\\n87.81\\n96.13\\n86.24\\n87.73\\n90.06\\n86.45\\n70.85\\n73.59\\n84.94\\n69.14\\n72.14\\n76.19\\nﬁltered\\nfull\\n98.31\\n70.91\\n75.65\\n98.30\\n69.48\\n74.72\\n81.23\\n89.46\\n58.69\\n64.52\\n86.51\\n55.45\\n62.18\\n69.47\\nsent\\n84.00\\n88.25\\n87.71\\n85.68\\n88.35\\n87.99\\n87.00\\n84.56\\n71.85\\n74.07\\n84.22\\n70.59\\n73.18\\n76.41\\nmix\\n95.36\\n86.73\\n87.97\\n95.60\\n86.56\\n87.92\\n90.02\\n86.30\\n71.00\\n73.70\\n84.98\\n69.45\\n72.40\\n76.31\\nTable 4: F1 scores (%) of different models on each testset, average of each language are reported.\\n5.4.1\\nWhich detector(s) is more useful? ML-based or DL-based? and Why?\\nAccording to Table 4, we can derive following conclusions:\\nFirstly, the robustness of RoBERTa-based-detector is better than GLTR. The F1-scores of\\nRoBERTa decrease slightly (1.5-2% in English datasets and 2-3% in Chinese datasets) when sentences\\nare split by comparing the leading diagonal elements in raw→raw and ﬁltered→ﬁltered. In contrast,\\nthe GLTR reduces signiﬁcantly by over 10% in English datasets, and above 15% in Chinese datasets.\\nAbove all, the RoBERTa-based-detector is more robust with anti-interference character. In contrast,\\nthe GLTR reduces signiﬁcantly by over 10% in English datasets, above 15% in Chinese datasets.\\nAbove all, the RoBERTa-based-detector is more robust with anti-interference character.\\nSecondly, RoBERTa-based-detector is not affected by indicating words.\\nThe F1-scores of\\nRoBERTa only slightly decreased by 0.03% in English full dataset, and 0.65% in Chinese full dataset,\\nas seen in the minus of relevant leading diagonal elements in raw→raw versus ﬁltered→ﬁltered. On\\nthe contrary, evaluations based on GLTR decrease by up to 3.1% on Chinese datasets, though tiny\\nrise on English datasets, indicating that GLTR is sensitive to indicating words, easily inﬂuenced by\\nthe patterns of ChatGPT.\\n12\\nLastly, RoBERTa-based-detector is effective in handling Out-Of-Distribution scenarios. When\\ncompared to the original model, it demonstrates a signiﬁcant decrease in performance on GLTR’s\\nOOD test datasets, with a drop of up to 28.8% on English datasets(ﬁltered-full→ﬁltered-full −\\nﬁltered-full→ﬁltered-sent) and 45.5% on Chinese datasets(raw-full→raw-full −raw-full→raw-sent).\\nHowever, RoBERTa maintains consistent performance with F1-scores varying by no more than 19%.\\n5.4.2\\nHow will the indicating words inﬂuence the detector?\\nWe ﬁrst collected a bunch of indicating words for both humans and ChatGPT. For example, ChatGPT’s\\nindicating words (or phrases) include \"AI assistant\", \"I’m sorry to hear that\", and \"There’re a few\\nsteps...\", etc. and humans’ indicating words may include \"Hmm\", \"Nope\", \"My view is\", etc. In the\\nﬁltered version, we remove all sentences in the answers that contain the indicating words for both\\nhumans and ChatGPT.\\nAccording to Table 4, removing the indicating words helps the models trained on full-text to per-\\nform better across different content granularities. For example, the RoBERTa-ﬁlter-full performs\\nsigniﬁcantly better than RoBERTa-raw-full in terms of sentence-level and mix-level evaluations,\\nimproving more than 3% F1 scores on average. However, the ﬁltering may slightly hurt the per-\\nformances of the models trained on sentences. This may be because the indicating words play a\\nbigger part in the sentence-level text compared with the full text. Removing the indicating words\\nmay make some sentences literally unable to be distinguished.\\n5.4.3\\nWhich granularity is more difﬁcult to detect? Full-text or sentence?\\nThrough the extensive experimental results in Table 5, we conclude that detecting ChatGPT gener-\\nated texts is more difﬁcult in a single sentence than in a full text. This conclusion can be proved\\nby the following two points: First, our results show that both English and Chinese sentence-based\\ndetectors (i.e., raw-sent and ﬁltered-sent versions) achieve satisfactory results w.r.t. the testing\\ntask of detecting either ChatGPT generated paragraphs or sentences, whereas the opposite is not\\ntrue——raw-full and ﬁltered-full are relatively inferior when detecting ChatGPT generated sentences.\\nIn other words, detectors trained on \"hard samples\" (i.e., sentence corpus) are much easier to solve\\nsimple task (i.e., detecting full corpus), while \"simple samples\" (i.e., full corpus) may be less useful\\nfor solving more difﬁcult task (i.e., sentence corpus).\\nSecond, we observe that although both full and sentence corpus are provided in the raw-mix and\\nﬁltered-mix versions, it is still more difﬁcult for them to detect single sentences generated by ChatGPT.\\nThis is even more obvious for the Chinese corpus, where the F1-score of raw-mix trained on the\\nChinese corpus is 94.09% for testing raw sentence answers, compared to that 97.43% for testing raw\\nfull answers. Similar results can be observed for the ﬁltered corpus, where F1-score of ﬁltered-mix is\\n95.61% for testing ﬁltered sentence answers, compared to its F1-score of 97.66% for testing ﬁltered\\nfull answers. One possible explanation is that the expression pattern of ChatGPT is more obvious\\n(therefore more easily detected) when paragraphs of text are provided, whereas it is more difﬁcult to\\ndetect generated single sentences.\\nEnglish\\nChinese\\nTest →\\nraw\\nﬁltered\\nAvg.\\nraw\\nﬁltered\\nAvg.\\nTrain ↓\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull-raw\\n99.82\\n81.89\\n84.67\\n99.72\\n81.00\\n84.07\\n88.53\\n98.79\\n83.64\\n86.32\\n98.57\\n82.77\\n85.85\\n89.32\\nsent-raw\\n99.40\\n98.43\\n98.56\\n99.24\\n98.47\\n98.59\\n98.78\\n97.76\\n95.75\\n96.11\\n97.68\\n95.31\\n95.77\\n96.40\\nmix-raw\\n99.44\\n98.31\\n98.47\\n99.32\\n98.37\\n98.51\\n98.74\\n97.70\\n95.68\\n96.04\\n97.65\\n95.27\\n95.73\\n96.35\\nfull-ﬁltered\\n99.82\\n87.17\\n89.05\\n99.79\\n86.60\\n88.67\\n91.85\\n98.25\\n91.04\\n92.30\\n98.14\\n91.15\\n92.48\\n93.89\\nsent-ﬁltered\\n96.97\\n97.22\\n97.19\\n99.09\\n98.43\\n98.53\\n97.91\\n96.60\\n92.81\\n93.47\\n97.94\\n95.86\\n96.26\\n95.49\\nmix-ﬁltered\\n96.28\\n96.43\\n96.41\\n99.45\\n98.37\\n98.53\\n97.58\\n97.43\\n94.09\\n94.68\\n97.66\\n95.61\\n96.01\\n95.91\\nTable 5: F1 scores (%) of RoBERTa models at full & sent & mix mode.\\n13\\n5.4.4\\nWhich corpus is more helpful for model training? Full-text, sentence, or mix of the\\ntwo?\\nWe ﬁnd that both English and Chinese RoBERTa-based detectors are more robust when ﬁne-\\ngrained corpus data is available in model training. The sentence-based detectors outperform\\nfull-based detectors w.r.t. F1-scores, while the latter can be signiﬁcantly improved when the sentence\\ncorpus is injected in model training, as we observe that mix-based detectors also achieve satisfactory\\nresults. For English corpus, raw-full only achieves 81.89% F1-score for testing sentence answers,\\nwhile raw-sent is signiﬁcantly better with 98.43% F1-score, as shown in Table 5. Moreover, the\\nrelatively inferior detection performance can be improved by injecting sentence answers into the\\ndetector, where we ﬁnd that raw-mix can also obtain signiﬁcant improvement (with 98.31% F1-score)\\nover the detectors trained on only full answers. Similar conclusions can be acquired for the ﬁltered\\nversions, where both ﬁltered-sent and ﬁltered-mix signiﬁcantly outperform ﬁltered-full version w.r.t.\\nF1-score, which holds for both English and Chinese corpus.\\nWe indicate that the above conclusions could also hold for other types of detectors like GLTR Test-2\\nfeature-based detectors, as is shown in Table 4. For GLTR Test-2, the average performance of F1-\\nscore of raw-full and ﬁltered-full is 61.74% and 69.47%, respectively, compared to that of raw-sent\\n76.26% and ﬁltered-sent 76.41%, where the performance of detectors trained on the mixed corpus is\\nclose to the sentence-based versions.\\nTaking into account the conclusions of the previous paragraph about the detection difﬁculty between\\nfull and sentence answers, we indicate that the ﬁne-grained corpus is helpful for distinguishing\\nChatGPT generated texts, as it additionally provides guidance and hints in model training for\\ndetecting the subtle patterns of ChatGPT hidden in single sentences.\\n5.4.5\\nWill a QA-style detector be more effective than a single-text detector?\\nTable 6 demonstrates the results of both raw-full and ﬁltered-full models across all test datasets.\\nOn English datasets, the QA model’s F1-scores are superior to that of the single model, except for\\ntwo full test datasets, where it averages 97.48% F1-scores and surpasses single model by 5.63%.\\nThere exist some differences in Chinese datasets, where the single model outperforms QA in raw-full\\ntrain dataset. However, the QA model still yields the best evaluation at 94.22%.\\nIn conclusion, the QA model is generally more effective than the single model and is suitable\\nfor ﬁltered scenarios. And the QA training makes models more robust to the sentence inputs.\\nEnglish\\nChinese\\nTest →\\nraw\\nﬁltered\\nAvg.\\nraw\\nﬁltered\\nAvg.\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nfull\\nsent\\nmix\\nTrain →raw - full\\nSingle\\n99.82\\n81.89\\n84.67\\n99.72\\n81.00\\n84.07\\n88.53\\n98.79\\n83.64\\n86.32\\n98.57\\n82.77\\n85.85\\n89.32\\nQA\\n99.84\\n92.68\\n93.70\\n99.75\\n92.34\\n93.46\\n95.30\\n98.99\\n80.56\\n83.85\\n98.73\\n80.24\\n83.89\\n87.71\\nTrain →ﬁltered - full\\nSingle\\n99.82\\n87.17\\n89.05\\n99.79\\n86.60\\n88.67\\n91.85\\n98.25\\n91.04\\n92.30\\n98.14\\n91.15\\n92.48\\n93.89\\nQA\\n99.70\\n96.14\\n96.64\\n99.70\\n96.07\\n96.61\\n97.48\\n97.29\\n92.10\\n93.01\\n97.18\\n92.40\\n93.31\\n94.22\\nTable 6: F1 scores (%) of RoBERTa models trained with QA & Single settings.\\n5.4.6\\nWhich data sources are more difﬁcult for the ChatGPT detectors? and What are the\\nconditions that make it easier to detect ChatGPT?\\nAs shown in Table 7, the evaluation results based on ﬁltered-full model are separated by various\\nsources in our HC3 dataset.\\nOn the English datasets, the F1-scores for human answers are slightly higher than those for ChatGPT\\nwithout any exceptions, regardless of whether RoBERTa or GLTR is used on full-text test datasets.\\nHowever, the F1-scores for ChatGPT are highly inconsistent on transferring test datasets particu-\\nlarly open-qa dataset with varying performance. In terms of data resource, reddit-eli5 and\\nfinance-en has higher values, while wiki-csai poses a challenge for detectors.\\n14\\nOn the Chinese datasets, the F1-scores of humans and ChatGPT are comparable with no signiﬁcant\\ndifference. This suggests that the difﬁculty in detecting ChatGPT depends on the data source. It\\nis observed that open-qa and baike have better performance, whereas the nlpcc-dbqa has\\nlower performance.\\nAbove all, the evaluations on Chinese dataset show more stability on transferring test dataset compared\\nto the English datasets. Furthermore, it’s evident that the F1-scores of ChatGPT are lower than those\\nof human answers, regardless of whether the dataset is English or Chinese. This indicates that\\nChatGPT’s detector relies more heavily on In-Distribution models.\\nModel\\nTest\\nF1-hu\\nF1-ch\\nF1-hu\\nF1-ch\\nF1-hu\\nF1-ch\\nF1-hu\\nF1-ch\\nF1-hu\\nF1-ch\\nEnglish\\nﬁnance\\nmedicine\\nopen_qa\\nreddit_eli5\\nwiki_csai\\nRoBERTa\\nfull\\n99.34\\n99.28\\n99.69\\n99.62\\n99.53\\n98.60\\n100.00\\n100.00\\n96.59\\n96.37\\nsent\\n78.84\\n85.84\\n84.06\\n80.45\\n70.74\\n26.78\\n77.27\\n93.31\\n68.91\\n84.12\\nGLTR\\nfull\\n97.50\\n97.37\\n98.28\\n97.96\\n92.68\\n82.20\\n98.22\\n99.40\\n95.76\\n95.72\\nsent\\n46.60\\n75.26\\n45.41\\n61.72\\n42.01\\n17.81\\n38.12\\n87.05\\n39.24\\n76.94\\nChinese\\nﬁnance\\nlaw\\nopen_qa\\nnlpcc_dbqa\\nbaike\\nRoBERTa\\nfull\\n98.87\\n97.99\\n97.78\\n98.50\\n98.75\\n99.33\\n97.42\\n95.42\\n94.61\\n93.99\\nsent\\n95.00\\n80.46\\n93.77\\n86.23\\n91.17\\n93.77\\n90.10\\n63.29\\n86.08\\n88.88\\nGLTR\\nfull\\n86.67\\n80.42\\n82.41\\n88.89\\n85.75\\n93.15\\n77.25\\n69.78\\n81.62\\n77.91\\nsent\\n36.91\\n32.80\\n33.99\\n46.22\\n36.45\\n75.21\\n46.39\\n27.50\\n48.10\\n71.72\\nTable 7: Human (F1-hu) and ChatGPT (F1-ch) detection F1 scores (%) w.r.t. different data source,\\nmodels are trained on ﬁltered full text, tested on ﬁltered full and sent. On HC3-Chinese, we omitted the\\nresults of medicine and psychology domains, which are similar to ﬁnance and open_qa, respectively.\\n6\\nConclusion\\nIn this work, we propose the HC3 (Human ChatGPT Comparison Corpus) dataset, which consists\\nof nearly 40K questions and their corresponding human/ChatGPT answers. Based on the HC3\\ndataset, we conduct extensive studies including human evaluations, linguistic analysis, and content\\ndetection experiments. The human evaluations and linguistics analysis provide us insights into the\\nimplicit differences between humans and ChatGPT, which motivate our thoughts on LLMs’ future\\ndirections. The ChatGPT content detection experiments illustrate some important conclusions that\\ncan provide beneﬁcial guides to the research and development of AIGC-detection tools. We make\\nall our data, code, and models publicly available to facilitate related research and applications at\\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\\n7\\nLimitations\\nDespite our comprehensive analysis of ChatGPT, there are still several limitations in the current\\npaper, which will be considered for improvement in our future work:\\n1. Despite our efforts in data collection, the amount and range of collected data are still not\\nenough and the data from different sources are unbalanced, due to limited time and resources.\\nTo make more accurate linguistic analyses and content detection, more data with different\\nstyles, sources, and languages are needed;\\n2. Currently, all the collected ChatGPT answers are generated without special prompts.\\nTherefore, the analysis and conclusions in this paper are built upon ChatGPT’s most general\\nstyle/state. For example, using special prompts such as \"Pretending you are Shakespeare...\"\\ncan generate content that bypasses our detectors or make the conclusions in this paper\\nuntenable;\\n15\\n3. ChatGPT (perhaps) is mainly trained on English corpus while less on Chinese. Therefore,\\nthe conclusions drawn from the HC3-Chinese dataset may not always be precise.\\nAcknowledgments\\nWe would like to thank the volunteers that participated in our human evaluations, many of them are\\nour good friends and dear family members. We would like to thank Junhui Zhu (BLCU-ICALL)\\nfor the valuable discussions on linguistic analysis. Biyang Guo would like to thank Prof. Hailiang\\nHuang and Prof. Songqiao Han (AI Lab, SUFE) for providing insightful feedback on the topics and\\ndirections for this project. Xin Zhang would like to thank Yu Zhao (NeXt, NUS and CIC, TJU) for\\nsharing the OpenAI account. Finally, we thank all team members of this project for their unique\\ncontributions. We together make this possible.\\n16\\nReferences\\n[1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a\\nlaboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\\n[2] Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. Xlm-t: Multilingual\\nlanguage models in twitter for sentiment analysis and beyond. In Proceedings of the Language\\nResources and Evaluation Conference, pages 258–266, Marseille, France, June 2022. European\\nLanguage Resources Association.\\n[3] Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python:\\nAnalyzing Text with the Natural Language Toolkit. O’Reilly Media, Inc., 2009.\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n[6] Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng Wang, Yue Yang, Jiaqi Zeng,\\nRuisi Zhang, Ruoyu Zhang, Meng Zhou, Penghui Zhu, and Pengtao Xie. Meddialog: a\\nlarge-scale medical dialogue dataset. arXiv preprint arXiv:2004.03329, 2020.\\n[7] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\\nDeep reinforcement learning from human preferences. neural information processing systems,\\n2017.\\n[8] Nan Duan. Overview of the nlpcc-iccpol 2016 shared task: Open domain chinese question\\nanswering. In Natural Language Understanding and Intelligent Applications, pages 942–948,\\nCham, 2016. Springer International Publishing.\\n[9] Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi.\\nTweepfake: About detecting deepfake tweets. Plos one, 16(5):e0251415, 2021.\\n[10] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\\nELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluís Màrquez,\\neditors, Proceedings of the 57th Conference of the Association for Computational Linguistics,\\nACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3558–3567.\\nAssociation for Computational Linguistics, 2019.\\n[11] Zhihui Fang. The language demands of science reading in middle school. International journal\\nof science education, 28(5):491–520, 2006.\\n[12] Yao Fu, Hao Peng, and Tushar Khot. How does gpt obtain its ability? tracing emergent abilities\\nof language models to their sources. Yao Fu’s Notion, Dec 2022.\\n[13] Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and\\nvisualization of generated text. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics: System Demonstrations, pages 111–116, Florence, Italy, July\\n2019. Association for Computational Linguistics.\\n[14] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey,\\nSamuel A Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational\\nprinciples for language processing in humans and deep language models. Nature neuroscience,\\n25(3):369–380, 2022.\\n[15] Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han, Hailiang Huang, Nan Duan, and\\nWeizhu Chen. Genius: Sketch-based language model pre-training via extreme and selective\\nmasking for text generation and augmentation. arXiv preprint arXiv:2211.10330, 2022.\\n[16] Biyang Guo, Songqiao Han, and Hailiang Huang. Selective text augmentation with word roles\\nfor low-resource text classiﬁcation. arXiv preprint arXiv:2209.01560, 2022.\\n[17] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly\\ndetection benchmark. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n17\\n[18] Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, and Edward Gibson. A\\nﬁne-grained comparison of pragmatic language understanding in humans and language models.\\narXiv preprint arXiv:2212.06801, 2022.\\n[19] Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks Lakshmanan, V.S. Automatic detection\\nof machine generated text: A critical survey. In Proceedings of the 28th International Conference\\non Computational Linguistics, pages 2296–2309, Barcelona, Spain (Online), December 2020.\\nInternational Committee on Computational Linguistics.\\n[20] Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa\\nStüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, et al. Chatgpt\\nmakes medicine easy to swallow: An exploratory case study on simpliﬁed radiology reports.\\narXiv preprint arXiv:2212.14882, 2022.\\n[21] Michael R King. The future of ai in medicine: a perspective from a chatbot, 2022.\\n[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT\\npretraining approach. CoRR, abs/1907.11692, 2019.\\n[23] Macedo Maia, Siegfried Handschuh, Andr’e Freitas, Brian Davis, Ross McDermott, Manel\\nZarrouk, and Alexandra Balahur. Www’18 open challenge: Financial opinion mining and\\nquestion answering. In Companion Proceedings of the The Web Conference 2018, WWW ’18,\\npage 1941–1942, Republic and Canton of Geneva, CHE, 2018. International World Wide Web\\nConferences Steering Committee.\\n[24] William Nagy and Dianna Townsend. Words as tools: Learning academic vocabulary as\\nlanguage acquisition. Reading research quarterly, 47(1):91–108, 2012.\\n[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\\nfollow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\\n[26] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\\nP. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\\nM. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine\\nLearning Research, 12:2825–2830, 2011.\\n[27] Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Abdullah Rehman, Yoonjin Kim, Paran-\\ntapa Bhattacharya, Mobin Javed, and Bimal Viswanath. Deepfake text detection: Limitations\\nand opportunities. In Proc. of IEEE S&P, 2023.\\n[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[29] Mary J Schleppegrell. The language of schooling: A functional linguistics perspective. Rout-\\nledge, 2004.\\n[30] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu,\\nAlec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and\\nthe social impacts of language models. arXiv preprint arXiv:1908.09203, 2019.\\n[31] SophonPlus. Chinesenlpcorpus. https://github.com/SophonPlus/ChineseNlpCorpus,\\n2019.\\n[32] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec\\nRadford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback.\\nneural information processing systems, 2020.\\n[33] Teo Susnjak. Chatgpt: The end of online exam integrity? arXiv preprint arXiv:2212.09292,\\n2022.\\n[34] Alan M Turing. Computing machinery and intelligence. In Parsing the turing test, pages 23–65.\\nSpringer, 2009.\\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems, 30, 2017.\\n18\\n[36] Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong,\\nJunqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu,\\nXinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu,\\nZhongshen Zeng, Chongpei Chen, Ruyi Gan, and Jiaxing Zhang. Fengshenbang 1.0: Being the\\nfoundation of chinese cognitive intelligence. CoRR, abs/2209.02970, 2022.\\n[37] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art\\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing: System Demonstrations, pages 38–45, Online, October 2020.\\nAssociation for Computational Linguistics.\\n[38] Bright Xu. Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019.\\n[39] Yi Yang, Scott Wen-tau Yih, and Chris Meek. Wikiqa: A challenge dataset for open-domain\\nquestion answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural\\nLanguage Processing. ACL - Association for Computational Linguistics, September 2015.\\n[40] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-\\nand-write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on\\nArtiﬁcial Intelligence, volume 33, pages 7378–7385, 2019.\\nA\\nAppendix\\nA.1\\nHC3 Dataset Splits Creation\\nWe create 5 and 7 splits for HC3 English and Chinese, respectively. Most of the data come from the\\npublicly available Question-Answering (QA) datasets, where details are listed in the following. For\\nthese QA data, we directly input the questions to ChatGPT and collect at least one answer.\\nWe also crawled some wiki concepts and explanations from Wikipedia and BaiduBaike, where\\nexplanations are treated as human expert answers and concepts are used to construct the questions,\\ndetails ref to bellow paragraphs.\\nFor HC3-English, we create ﬁve dataset splits:\\n1. reddit_eli5. Sampled from the ELI5 dataset [10].\\n2. open_qa. Sampled from the WikiQA dataset [39].\\n3. wiki_csai. We collected the descriptions of hundreds of computer science-related concepts\\nfrom Wikipedia13 as the human experts’ answers to questions like \"Please explain what is\\n<concept>?\"\\n4. medicine. Sampled from the Medical Dialog dataset [6].\\n5. finance. Sampled from the FiQA dataset [23], which is built by crawling StackExchange14\\nposts under the Investment topic.\\nFor HC3-Chinese, we create seven dataset splits:\\n1. open_qa. Sampled from the WebTextQA and BaikeQA corpus in [38].\\n2. baike. We collected the descriptions of more than a thousand information science-related\\nconcepts from BaiduBaike15 as the human experts’ answers to questions like \"我有一个计\\n算机相关的问题，请用中文回答，什么是<concept>\"\\n3. nlpcc_dbqa. Sampled from the NLPCC-DBQA dataset [8].\\n4. medicine. Sampled from the Medical Dialog dataset [6].\\n5. finance. Sampled from the FinanceZhidao dataset [31].\\n13https://www.wikipedia.org/\\n14https://stackexchange.com/\\n15https://baike.baidu.com/\\n19\\n6. psychology Sampled from a public Chinese Psychological Question Answering Dataset16.\\n7. law. Sampled from the LegalQA dataset17.\\nA.2\\nAdditional Results\\nHere we demonstrate the additional results of dependency relations for the Chinese corpus, as is\\nshown in Figure 6. The conclusion is basically consistent with the main paper.\\npunct\\nadvmod\\nconj\\ndep\\ncompound:nn\\ndobj\\nnsubj\\ncase\\nROOT\\namod\\nccomp\\nnmod:prep\\nmark\\nnmod:assmod\\nacl\\ncop\\naux:modalcc\\nmark:clfdet\\nnummod\\naux:asp\\ndiscourseneg\\nxcomp\\nnmod\\nadvmod:rcomp\\nnmod:tmod\\nadvmod:loc\\naux:prtmodetc\\ncompound:vc\\nparataxis:prnmod\\nnmod:range\\nnmod:topic\\naux:ba\\nauxpass\\nadvmod:dvp\\nadvcl:loc\\nappos\\nname\\nnmod:poss\\namod:ordmod\\nnsubjpass\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\nProportion (%)\\n14.8\\n8.9\\n8.2\\n7.6\\n7.3\\n7.2\\n7.1\\n6.9\\n4.3\\n2.7\\n2.5\\n2.5\\n2.3\\n2.1\\n1.5\\n1.5\\n1.4\\n1.4\\n1.3\\n1.0\\n0.9\\n0.8\\n0.8\\n0.8\\n0.7\\n0.4\\n0.4\\n0.4\\n0.3\\n0.3\\n0.2\\n0.2\\n0.2\\n0.2\\n0.2\\n0.2\\n0.2\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.0\\n0.0\\n13.3\\n7.4\\n7.8\\n5.3\\n7.5\\n8.7\\n6.6\\n8.7\\n4.6\\n3.1\\n2.9\\n3.0\\n2.0\\n2.9\\n1.2\\n1.5\\n2.8\\n2.8\\n0.9\\n1.3\\n0.7\\n0.4\\n0.3\\n0.4\\n0.6\\n0.5\\n0.2\\n0.1\\n0.2\\n0.6\\n0.3\\n0.1\\n0.2\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n0.0\\n0.0\\n0.0\\nDependency Relation Comparison (Zh)\\nHuman\\nChatGPT\\npunct\\nadvmod\\nconj\\ndep\\ncompound:nn\\ndobj\\nnsubj\\ncase\\nROOT\\namod\\nccomp\\nnmod:prep\\nmark\\nnmod:assmod\\nacl\\ncop\\naux:modalcc\\nmark:clfdet\\nnummod\\naux:asp\\ndiscourseneg\\nxcomp\\nnmod\\nadvmod:rcomp\\nnmod:tmod\\nadvmod:loc\\naux:prtmodetc\\ncompound:vc\\nparataxis:prnmod\\nnmod:range\\nnmod:topic\\naux:ba\\nauxpass\\nadvmod:dvp\\nadvcl:loc\\nappos\\nname\\nnmod:poss\\namod:ordmod\\nnsubjpass\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nProportion (%)\\n9.6\\n2.6\\n11.7\\n5.3\\n1.6\\n3.4\\n3.0\\n1.8\\n0.0\\n1.8\\n4.8\\n3.8\\n1.8\\n2.5\\n4.4\\n4.2\\n2.1\\n2.3\\n1.0\\n1.7\\n3.0\\n1.1\\n2.2\\n1.6\\n1.6\\n3.4\\n1.1\\n3.8\\n4.1\\n1.1\\n1.0\\n1.0\\n4.1\\n2.2\\n5.9\\n4.6\\n1.8\\n2.3\\n6.3\\n1.1\\n1.1\\n1.0\\n1.2\\n2.5\\n8.2\\n3.2\\n6.7\\n5.9\\n1.5\\n3.4\\n3.3\\n1.8\\n0.0\\n2.0\\n3.8\\n3.7\\n1.7\\n2.8\\n4.1\\n4.8\\n2.4\\n2.1\\n1.0\\n1.6\\n3.8\\n1.0\\n1.5\\n1.7\\n1.6\\n3.3\\n1.0\\n3.7\\n4.5\\n1.0\\n1.0\\n1.0\\n4.3\\n2.1\\n7.4\\n3.9\\n1.4\\n2.2\\n6.0\\n1.0\\n1.1\\n1.1\\n1.2\\n2.2\\nDependency Distance Comparison (Zh)\\nHuman\\nChatGPT\\nFigure 6: Top-30 dependency relations (upper) and corresponding dependency distances (lower)\\ncomparison between human and ChatGPT answers in the HC3-Chinese. Results are sorted by\\nrelations proportion of human answers.\\nOther detailed results, including vocabulary features, sentiment analyses, and dependency parsing\\nresults for each data source are all available at our project GitHub repository at https://github.\\ncom/Hello-SimpleAI/chatgpt-comparison-detection.\\nA.3\\nHuman Evaluations Examples\\nFor evaluation examples of our human evaluations, please visit our project GitHub repository at\\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\\n16https://aistudio.baidu.com/aistudio/datasetdetail/38489\\n17https://github.com/siatnlp/LegalQA\\n20\\n'),\n",
       " Document(metadata={'Published': '2023-11-02', 'Title': 'Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine', 'Authors': 'Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Shuming Shi, Zhaopeng Tu', 'Summary': 'This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well with minor performance differences. By evaluating on a number of benchmark test sets, we find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but exhibits good results on spoken language. Further, we explore an interesting strategy named $\\\\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably. With the launch of the GPT-4 engine, the translation performance of ChatGPT is significantly boosted, becoming comparable to commercial translation products, even for distant languages. Human analysis on Google Translate and ChatGPT suggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and mis-translation errors while that with GPT-4 makes the least errors. In other words, ChatGPT has already become a good translator. Please refer to our Github project for more details: https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator'}, page_content='Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine\\nWenxiang Jiao∗Wenxuan Wang Jen-tse Huang Xing Wang\\nShuming Shi Zhaopeng Tu\\nTencent AI Lab\\n{joelwxjiao}@tencent.com\\nAbstract\\nThis report provides a preliminary evaluation\\nof ChatGPT for machine translation, includ-\\ning translation prompt, multilingual transla-\\ntion, and translation robustness.\\nWe adopt\\nthe prompts advised by ChatGPT to trigger\\nits translation ability and find that the candi-\\ndate prompts generally work well with minor\\nperformance differences. By evaluating on a\\nnumber of benchmark test sets, we find that\\nChatGPT performs competitively with commer-\\ncial translation products (e.g., Google Trans-\\nlate) on high-resource European languages but\\nlags behind significantly on low-resource or\\ndistant languages. As for the translation robust-\\nness, ChatGPT does not perform as well as the\\ncommercial systems on biomedical abstracts or\\nReddit comments but exhibits good results on\\nspoken language. Further, we explore an inter-\\nesting strategy named pivot prompting for dis-\\ntant languages, which asks ChatGPT to trans-\\nlate the source sentence into a high-resource\\npivot language before into the target language,\\nimproving the translation performance notice-\\nably. With the launch of the GPT-4 engine,\\nthe translation performance of ChatGPT is sig-\\nnificantly boosted, becoming comparable to\\ncommercial translation products, even for dis-\\ntant languages. Human analysis on Google\\nTranslate and ChatGPT suggests that ChatGPT\\nwith GPT-3.5 tends to generate more halluci-\\nnations and mis-translation errors while that\\nwith GPT-4 makes the least errors. In other\\nwords, ChatGPT has already become a good\\ntranslator. Please refer to our Github project for\\nmore details: https://github.com/wxjiao/\\nIs-ChatGPT-A-Good-Translator.\\n1\\nIntroduction\\nChatGPT1 is an intelligent chatting machine devel-\\noped by OpenAI upon the InstructGPT (Ouyang\\net al., 2022), which is trained to follow an instruc-\\ntion in a prompt and provide a detailed response.\\n∗Corresponding author.\\n1https://chat.openai.com\\nFigure 1: Prompts advised by ChatGPT for machine\\ntranslation (Date: 2022.12.16).\\nAccording to the official statement, ChatGPT is\\nable to answer followup questions, admit its mis-\\ntakes, challenge incorrect premises, and reject in-\\nappropriate requests due to the dialogue format.\\nIt integrates various abilities of natural language\\nprocessing, including question answering, story-\\ntelling, logic reasoning, code debugging, machine\\ntranslation, and so on. We are particularly inter-\\nested in how ChatGPT performs for machine trans-\\nlation tasks, especially the gap between ChatGPT\\nand commercial translation products (e.g., Google\\nTranslate, DeepL Translate).\\nIn this report, we provide a preliminary study of\\nChatGPT on machine translation, which to our best\\nknowledge is also the first one since the release of\\nChatGPT. Specifically, we focus on three aspects:\\n• Translation Prompt: ChatGPT is essentially a\\nlarge language model, which needs prompts as\\nguidance to trigger its translation ability. The\\nstyle of prompts may affect the quality of trans-\\nlation outputs. For example, how to mention\\nthe source or target language information mat-\\nters in multilingual machine translation models,\\nwhich is usually solved by attaching language\\ntokens (Johnson et al., 2017; Fan et al., 2021).\\n• Multilingual Translation: ChatGPT is a single\\narXiv:2301.08745v4  [cs.CL]  2 Nov 2023\\nmodel handling various NLP tasks and covering\\ndifferent languages, which can be considered a\\nunified multilingual machine translation model.\\nThus, we are curious about how ChatGPT per-\\nforms on different language pairs considering\\nboth the resource difference (e.g., high vs. low)\\nand language family (e.g., European vs. Asian).\\n• Translation Robustness: ChatGPT is developed\\nupon GPT3, which was trained on large-scale\\ndatasets that cover various domains. Therefore,\\nwe wonder if it can perform robustly well on\\ndomain-specific or even noisy sentences.\\nTo trigger the translation ability of ChatGPT,\\nwe ask ChatGPT itself for advice and obtain three\\ncandidate translation prompts. By evaluating on\\nthe Chinese⇒English translation task, we find that\\nthe candidate prompts generally work well and\\nshow minor performance differences. Neverthe-\\nless, we adopt the best-performing prompt for the\\nrest parts of the study. By evaluating the transla-\\ntion among four selected languages on the Flores-\\n101 test sets, we find that ChatGPT performs com-\\npetitively with commercial translation products\\n(e.g., Google Translate) on high-resource Euro-\\npean languages but lags behind significantly on\\nlow-resource or distant languages. As for the trans-\\nlation robustness, results on three robustness sets\\nsuggest that ChatGPT does not perform as well as\\nthe commercial systems on biomedical abstracts\\nor Reddit comments but exhibits good results on\\nspoken language.\\nFurther, we have a discussion on how to im-\\nprove ChatGPT for machine translation. On one\\nhand, we explore an interesting strategy named\\npivot prompting for distant languages, which asks\\nChatGPT to translate the source sentence into a\\nhigh-resource pivot language before into the target\\nlanguage, improving the translation performance\\nnoticeably. On the other hand, with an improved\\nengine GPT-4 (OpenAI, 2023)2 launched on March\\n15, 2023, we re-evaluate the translation ability of\\nChatGPT and observe a significant boost of per-\\nformance. The translation performance of Chat-\\nGPT becomes comparable to commercial transla-\\ntion products, even for distant languages. Exten-\\nsive analysis on Google Translate and ChatGPT\\nsuggests that ChatGPT with GPT-3.5 tend to gener-\\nate more hallucinations and more mis-translation\\nerrors while that with GPT-4 makes the least errors.\\n2https://openai.com/research/gpt-4\\nTable 1: Information of adopted test sets.\\nTest Set\\nDirection\\nDomain\\nSize\\nFlores-101\\nAny\\nGeneral\\n1012\\nWMT19 Bio\\nDe⇒En\\nBiomedical\\n373\\nWMT20 Rob2\\nEn⇒Ja\\nReddit\\n1376\\nJa⇒En\\n997\\nWMT20 Rob3 De⇒En Common Voice 5609\\nIn other words, ChatGPT has already become a\\ngood translator with GPT-4 as the engine!\\n2\\nChatGPT for Machine Translation\\n2.1\\nEvaluation Setting\\nWe provide a brief introduction of the evaluation\\nsetting, which mainly includes the compared base-\\nlines and test data.\\nBaselines.\\nWe compare ChatGPT with three com-\\nmercial translation products, namely, Google Trans-\\nlate3, DeepL Translate4, and Tencent TranSmart5.\\nSo far, the three commercial systems support trans-\\nlation in 133, 29, and 16 languages, respectively.\\nBy default, the results in this report come from\\nthe ChatGPT version on 2022.12.16 . For new re-\\nsults, we will mark the updated version information\\ncorrespondingly.\\nData.\\nFor multilingual translation, we evalu-\\nate the above translation systems on the Flores-\\n101 (Goyal et al., 2021)6 test sets, which consists\\nof 1012 sentences translated into 101 languages.\\nTo test the translation robustness, we adopt the test\\nset of WMT19 Biomedical Translation Task (Baw-\\nden et al., 2019, i.e., Bio) and the set2 and set3 of\\nWMT20 Robustness Task (Specia et al., 2020, i.e.,\\nRob2 and Rob3). We obtain the first two test sets\\nthrough SacreBLEU and the third pre-processed by\\nWang et al. (2021)7. Table 1 lists the information\\nof these test sets. Since this empirical study was\\nconducted upon the very early release of ChatGPT,\\nwe were only able to access it through webpage,\\nwhich cannot respond to large batches. As a result,\\nobtaining the translation results from ChatGPT is\\ntime-consuming. Therefore, we randomly sample\\n50 sentences from each set for evaluation.\\n3https://translate.google.com\\n4https://www.deepl.com/translator\\n5https://transmart.qq.com/zh-CN/index\\n6https://github.com/facebookresearch/flores\\n7https://github.com/hsing-wang/WMT2020_\\nBioMedical/tree/master/Bio-18-19-testset\\nTable 2: Candidate translation prompts.\\nTranslation Prompt\\nTP1\\nTranslate these sentences from\\n[SRC] to [TGT]:\\nTP2\\nAnswer with no quotes. What do\\nthese sentences mean in [TGT]?\\nTP3\\nPlease\\nprovide\\nthe\\n[TGT]\\ntranslation for these sentences:\\nMetric.\\nWe adopt the mostly used BLEU\\nscore (Papineni et al., 2002) as our primary\\nmetric and also report ChrF++ (Popovi´c, 2017)\\nand TER (Snover et al., 2006) in some cases.\\nThese three metrics are all supported by Sacre-\\nBLEU (Post, 2018)8.\\n2.2\\nTranslation Prompts\\nTo design the prompts for triggering the machine\\ntranslation ability of ChatGPT, we seek inspiration\\nfrom ChatGPT by asking it for advice. Specifically,\\nwe ask ChatGPT with the following prompt:\\nProvide ten concise prompts or\\ntemplates\\nthat\\ncan\\nmake\\nyou\\ntranslate.\\nand obtain the results as shown in Figure 1. The\\ngenerated prompts look reasonable but share simi-\\nlar formats. Thus, we summarize them into three\\ncandidate prompts as shown in Table 2, where\\n[SRC] and [TGT] represent the source and target\\nlanguages of translation. Note that we add an extra\\ncommand into TP2 to ask ChatGPT not to gen-\\nerate double quotes around the translation, which\\noften occurs with the original format. Nevertheless,\\nit is still unstable such that sentences in a batch\\n(in multiple lines) are translated into a single line\\noccasionally.\\nWe compare the three different candidate\\nprompts on the Chinese-to-English (Zh⇒En) trans-\\nlation task with the test set from Flores-101. Ta-\\nble 3 shows the results of ChatGPT and three com-\\nmercial systems. While ChatGPT provides rea-\\nsonably good translations, it still lags behind the\\nbaselines by at least 5.0 BLEU points. Concerning\\nthe three candidate prompts, TP3 performs the best\\nin terms of all the three metrics. Thus, we use TP3\\nthroughout this report by default.\\nTable 3: Comparison of different prompts for ChatGPT\\nto perform Chinese-to-English (Zh⇒En) translation.\\nSystem\\nBLEU↑\\nChrF++↑\\nTER↓\\nGoogle\\n31.66\\n57.09\\n56.21\\nDeepL\\n31.22\\n56.74\\n57.84\\nTencent\\n29.69\\n56.24\\n57.16\\nChatGPT w/ TP1\\n23.25\\n53.07\\n66.03\\nChatGPT w/ TP2\\n24.54\\n53.05\\n63.79\\nChatGPT w/ TP3\\n24.73\\n53.71\\n62.84\\n2.3\\nMultilingual Translation\\nWe select four languages to evaluate the capabil-\\nity of ChatGPT in multilingual translation, includ-\\ning German (De), English (En), Romanian (Ro),\\nand Chinese (Zh), which are commonly adopted in\\nboth research (Wang et al., 2022a; Jiao et al., 2021,\\n2022b) and competitions (Bojar et al., 2016; Farhad\\net al., 2021). The first three languages come from\\nthe same family with Latin scripts while the last\\nis from another family with Chinese scripts (Fan\\net al., 2021). We test the translation performance\\nbetween any two languages, which involves 12 di-\\nrections in total. For clarity and comparison, we\\nreport the BLEU scores and the improvement or\\ndrop of performance (i.e., +/-) relative to Google\\nTranslate. Table 4 presents the results.\\nResource Difference.\\nWe consider the resource\\ndifference of languages in the same family. In ma-\\nchine translation, German⇔English translation is\\nusually regarded as a high-resource task supported\\nby over ten million sentence pairs (Farhad et al.,\\n2021) while Romanian⇔English translation is sup-\\nported by much less data (Bojar et al., 2016). This\\nresource difference can also be indicated by the\\ndata statistics9 of GPT-3 (Brown et al., 2020), al-\\nthough we do not know the data information of\\nChatGPT. As shown in Table 4, ChatGPT per-\\nforms competitively with Google Translate and\\nDeepL Translate for both German⇒English and\\nEnglish⇒German translations. However, it lags\\nbehind them significantly on Romanian⇒English\\nand English⇒Romanian. Specifically, ChatGPT\\nobtains a BLEU score on English⇒Romanian that\\nis 46.4% lower than Google Translate and the\\nvalue is 10.3% on Romanian⇒English. We spec-\\nulate that the huge resource difference of mono-\\n8https://github.com/mjpost/sacrebleu\\n9https://github.com/openai/gpt-3/tree/master/\\ndataset_statistics\\nTable 4: Performance of ChatGPT for multilingual translation.\\nSystem\\nDe-En\\nRo-En\\nZh-En\\n⇒\\n⇐\\n⇒\\n⇐\\n⇒\\n⇐\\nGoogle\\n45.04\\n41.16\\n50.12\\n46.03\\n31.66\\n43.58\\nDeepL\\n49.23(+9.3%)\\n41.46(+0.7%)\\n50.61(+0.9%)\\n48.39(+5.1%)\\n31.22(-1.3%)\\n44.31(+1.6%)\\nTencent\\nn/a\\nn/a\\nn/a\\nn/a\\n29.69(-6.2%)\\n46.06(+5.6%)\\nChatGPT\\n43.71(-2.9%)\\n38.87(-5.5%)\\n44.95(-10.3%)\\n24.85(-46.0%)\\n24.73(-21.8%)\\n38.27(-12.1%)\\nSystem\\nDe-Zh\\nRo-Zh\\nDe-Ro\\n⇒\\n⇐\\n⇒\\n⇐\\n⇒\\n⇐\\nGoogle\\n38.71\\n21.68\\n39.05\\n25.59\\n33.31\\n32.27\\nDeepL\\n40.46(+4.5%)\\n22.82(+5.2%)\\n38.95(-0.2%)\\n25.39(-0.7%)\\n35.19(+5.6%)\\n34.27(+6.1%)\\nTencent\\n40.66(+5.0%)\\n19.44(-10.3%)\\nn/a\\nn/a\\nn/a\\nn/a\\nChatGPT\\n34.46(-10.9%)\\n19.80(-8.6%)\\n30.84(-21.0%)\\n19.17(-25.0%)\\n33.38(+0.2%)\\n29.89(-7.3%)\\nlingual data between English and Romanian lim-\\nits the language modeling capability of Roma-\\nnian, which partially explains the poor perfor-\\nmance on English⇒Romanian. On the contrary,\\nRomanian⇒English can benefit from the strong\\nlanguage modeling capability of English such that\\nthe resource gap of parallel data can be somewhat\\ncompensated.\\nLanguage Family.\\nWe also take the impact of\\nlanguage families into account. In machine trans-\\nlation, translating between different language fam-\\nilies is often considered harder than that within\\nthe same language family, due to the differ-\\nent cultures and writing scripts.\\nBy compar-\\ning German⇔English with Chinese⇔English or\\nGerman⇔Chinese translation, we find that the gap\\nbetween ChatGPT and the commercial systems be-\\ncomes larger. We attribute to the better knowledge\\ntransfer within the same family (i.e., from English\\nto German) than between different families (e.g.,\\nfrom English to Chinese). For language pairs that\\nare both low-resource and from different families\\n(e.g., Romanian⇔Chinese), the performance gap\\ncan be further enlarged (Wang et al., 2022b). Since\\nChatGPT handles different tasks in one model, low-\\nresource translation tasks not only compete with\\nhigh-resource translation tasks (Jiao et al., 2022a),\\nbut also with other NLP tasks for the model capac-\\nity, which explains their poor performance.\\n2.4\\nTranslation Robustness\\nWe further evaluate the translation robustness of\\nChatGPT on the WMT19 Bio and WMT20 Rob2\\nTable 5: Performance of ChatGPT for translation robust-\\nness on domain-specific or noisy test data.\\nSystem\\nW19 Bio\\nW20 Rob2\\nW20 Rob3\\nDe⇒En En⇒Ja Ja⇒En\\nDe⇒En\\nGoogle\\n37.83\\n29.72\\n19.21\\n42.91\\nDeepL\\n37.13\\n26.25\\n19.83\\n41.29\\nChatGPT\\n33.22\\n22.36\\n18.34\\n44.59\\nand Rob3 test sets, which introduce the impact\\nof domain bias and potentially noisy data. For\\nexample, WMT19 Bio test set is composed of\\nMedline abstracts, which require domain-specific\\nknowledge to handle the terminologies. WMT20\\nRob2 are comments from the social media web-\\nsite reddit.com that could contain various errors,\\nincluding spelling/typographical errors, word omis-\\nsion/insertion/repetition, grammatical errors, spo-\\nken languages, Internet slang, and so on (Michel\\nand Neubig, 2018).\\nTable 5 lists the BLEU scores. Obviously, Chat-\\nGPT does not perform as well as Google Translate\\nor DeepL Translate on the WMT19 Bio and WMT2\\nRob2 test sets. The reason may be that commer-\\ncial translation systems like Google Translate often\\nneed to continuously improve their ability to trans-\\nlate domain-specific (e.g. biomedical) or noisy\\nsentences, since they are real-world applications\\nthat require better generalization performance over\\nout-of-distribution data. However, these may not\\nbe done in ChatGPT.\\nAn interesting finding is that ChatGPT outper-\\nTable 6: Examples from WMT20 Robust Set3.\\nExample\\nSRC\\nHaben wir noch Nudeln?\\nREF\\nDo we still have noodles?\\nGoogle\\nDo we still have pasta?\\nDeepL\\nDo we have any noodles left?\\nChatGPT\\nDo we still have noodles?\\nSRC\\nTatsächlich ist der zu häufige Ge-\\nbrauch von Seife schlecht für die\\nHaut.\\nREF\\nActually, very frequent usage of\\nsoap is bad for the skin.\\nGoogle\\nIn fact, using soap too often is\\nbad for your skin.\\nDeepL\\nIn fact, using soap too often is\\nbad for the skin.\\nChatGPT\\nIn fact, the frequent use of soap\\nis bad for the skin.\\nforms Google Translate and DeepL Translate sig-\\nnificantly on WMT20 Rob3 test set that contains\\na crowdsourced speech recognition corpus. It sug-\\ngests that ChatGPT, which is essentially an artifi-\\ncial intelligent chatting machine, is capable of gen-\\nerating more natural spoken languages than these\\ncommercial translation systems. We provide some\\nexamples in Table 6.\\n3\\nImproving ChatGPT for MT\\nAs presented above, ChatGPT can match the per-\\nformance of commercial translation systems on\\nhigh-resource language pairs, but still struggles\\non low-resource ones, especially those distant lan-\\nguages. Then, one question arises:\\nHow can we improve ChatGPT for MT?\\n3.1\\nPivot Prompting\\nThe first way to improve ChatGPT for MT is to\\nexploit the potential of ChatGPT in other tasks to\\nassist the target task. Here, we explore an inter-\\nesting strategy named Pivot Prompting to improve\\nthe translation quality between distant languages.\\nRather than the direct translation between source\\nand target languages, we ask ChatGPT to translate\\nthe source sentence into a high-resource pivot lan-\\nguage (i.e., English by default) first and then into\\nthe target language. Accordingly, we adjust the\\nTP3 prompt as below:\\nFigure 2: Translation results by ChatGPT with pivot\\nprompting (Date: 2023.01.31 ).\\nTable 7: Performance of ChatGPT with pivot prompting.\\nNew results are obtained from the updated ChatGPT\\nversion on 2023.01.31. LR: length ratio.\\nSystem\\nDe⇒Zh\\nRo⇒Zh\\nBLEU LR BLEU LR\\nGoogle\\n38.71 0.94 39.05 0.95\\nDeepL\\n40.46 0.98 38.95 0.99\\nChatGPT (Direct)\\n34.46 0.97 30.84 0.91\\nChatGPT (Directnew) 30.76 0.92 27.51 0.93\\nChatGPT (Pivotnew)\\n34.68 0.95 34.19 0.98\\nPlease\\nprovide\\nthe\\n[PIV]\\ntranslation\\nfirst\\nand\\nthen\\nthe [TGT] translation for these\\nsentences one by one:\\nwhere [PIV] denotes the pivot language. As a large\\nlanguage model, ChatGPT will naturally condition\\non both the prompt and the translation result in the\\npivot language to generate the translation into the\\ntarget language. Figure 2 shows an example when\\nusing pivot prompting.\\nThere are several advantages of pivot prompting:\\n• Knowledge Transfer: While parallel data be-\\ntween two distant languages is often scarce (Fan\\net al., 2021; Wang et al., 2022b), the parallel data\\nbetween them and the pivot language can be rel-\\natively considerable, which is expected to learn\\nbetter translation ability for source-pivot and\\npivot-target directions than that for the source-\\ntarget direction.\\nThus, pivot prompting will\\npotentially transfer the knowledge of the high-\\nresource pivot language to the low-resource tar-\\nget languages (Zoph et al., 2016; Aji et al., 2020;\\nLi et al., 2022; He et al., 2022).\\nTable 8: Performance of GPT-4 (Date: 2023.03.15 ) for multilingual translation.\\nSystem\\nDe⇒En\\nEn⇒De\\nZh⇒En\\nEn⇒Zh\\nDe⇒Zh\\nRo⇒Zh\\nGoogle\\n45.04\\n41.16\\n31.66\\n43.58\\n38.71\\n39.05\\nDeepL\\n49.23\\n41.46\\n31.22\\n44.31\\n40.46\\n38.95\\nTencent\\nn/a\\nn/a\\n29.69\\n46.06\\n40.66\\nn/a\\nChatGPT (Direct)\\n43.71\\n38.87\\n24.73\\n38.27\\n34.46\\n30.84\\nChatGPT (Directnew)\\nn/a\\nn/a\\nn/a\\nn/a\\n30.76\\n27.51\\nChatGPT (Pivotnew)\\nn/a\\nn/a\\nn/a\\nn/a\\n34.68\\n34.19\\nGPT-4\\n46.00\\n45.73\\n28.50\\n42.50\\n38.16\\n37.84\\n• Convenience: Essentially, pivot prompting is\\nsimilar to the pivot translation technique in previ-\\nous studies (Cheng et al., 2016) but is more con-\\nvenient for ChatGPT. For the commonly adopted\\nmultilingual sequence-to-sequence translation\\nmodels (Fan et al., 2021), pivot translation re-\\nquires two steps: (1) Input the source sentence\\nand translate it into the pivot language; (2) In-\\nput the translation results in pivot language and\\ntranslate it into the target language. In contrast,\\nChatGPT can identify both the [PIV] and [TGT]\\nlanguages and translate the source sentence into\\nthe two languages sequentially (see Figure 2),\\nwhich requires only one step operation.\\nTable 7 presents our results in BLUE score\\nand length ratio of translation results over refer-\\nences. We obtain the translation results by us-\\ning TP3 (i.e., Direct) and pivot prompting (i.e.,\\nPivot) through English (i.e., source-to-English-to-\\ntarget), respectively. As seen, the latest update\\nfor ChatGPT seems to harm the translation qual-\\nity for German⇒Chinese and Romanian⇒Chinese\\ntranslations, compared with the previous version\\nwe used (i.e., Directnew vs.\\nDirect).\\nNev-\\nertheless, pivot prompting can significantly im-\\nprove the translation performance by nearly 3.9\\nand 6.6 BLEU points for German⇒Chinese\\nand Romanian⇒Chinese translations, respectively,\\nwhich demonstrates its effectiveness. By inspecting\\nthe translation results, we find that direct transla-\\ntion with TP3 will under-translate some tokens in\\nsource sentences, which can be noticeably fixed by\\npivot prompting. This can be reflected by the length\\nratio results. Note that, while pivot prompting is\\nconvenient for ChatGPT, how to further accelerate\\nthe inference process is still an important research\\nquestion as we need to generate longer sentences.\\n3.2\\nGPT-4 as the Engine\\nAnother way to improve ChatGPT for MT is to\\nimprove its engine. Unsurprisingly, OpenAI re-\\nleased GPT-4 (OpenAI, 2023) on March 15, 2023 ,\\nwhich exhibits all-around stronger capabilities than\\nthe GPT-3.5 model behind ChatGPT. Therefore,\\nwe re-evaluate the performance for four translation\\ndirections. As shown in Table 8, GPT-4 boosts\\nthe performance over ChatGPT significantly on all\\nthe four directions, bringing the BLEU scores to\\nthe level of top commercial translation systems.\\nNote that these results only come from zero-shot\\nsettings. With modern techniques like in-context\\nlearning with demonstrations (Brown et al., 2020;\\nAgrawal et al., 2022), the translation performance\\ncould be further improved. In other words, GPT-4\\nhas already become a good translator!\\n4\\nAnalysis\\nHere we conduct some analyses on the translation\\noutputs for a deeper understanding in ChatGPT. By\\ndefault, we analyze the outputs of Google, Chat-\\nGPT, and GPT-4 on Zh⇒En translation for all the\\n50 test examples.\\n4.1\\nAutomatic Analysis\\nWe follow previous studies (Jiao et al., 2021; Wang\\net al., 2022a) to analyze the translation outputs\\nusing automatic tools, i.e., compare-mt10, at both\\nword level and sentence level.\\nWord Frequency.\\nEssentially, ChatGPT is a\\nlarge language model that has been trained on\\na variety of corpora, covering different domains.\\nIt could be beneficial to the translation of low-\\nfrequency words in the test sets. Specifically, we di-\\nvide the target words into three categories based on\\ntheir frequency and calculate the accuracy of word\\n10https://github.com/neulab/compare-mt\\nTable 9: F-measure of target word prediction with re-\\nspect to frequency bucket.\\nFreq\\nGoogle\\nChatGPT\\nGPT-4\\n< 2\\n48.0\\n43.7\\n47.1\\n[2, 10)\\n59.0\\n57.6\\n56.7\\n≥10\\n71.6\\n70.5\\n70.1\\nTable 10: Translation performance (i.e., BLEU) with\\nrespect to length bucket of target sentences.\\nLength\\nGoogle\\nChatGPT\\nGPT-4\\n< 15\\n34.2\\n15.4\\n26.1\\n[15, 30)\\n26.6\\n21.4\\n24.3\\n≥30\\n23.2\\n16.0\\n19.4\\nprediction. Table 9 shows the F-measure results.\\nUnexpectedly, ChatGPT turns out to perform the\\nworst on low-frequency words (i.e., < 2), which\\nwe attribute to the immature translation ability of\\nChatGPT. What’s interesting is that GPT-4 mainly\\naddresses this shortcoming for ChatGPT with little\\nimprovement to higher-frequency words.\\nSentence Length.\\nChatGPT is also trained for\\nvarious text generation tasks, which usually do not\\nrequire strict length constraints of generated sen-\\ntences as machine translation. Therefore, we are\\ncurious about how sensitive the translation perfor-\\nmance is to the sentence length. We divide the\\ntarget sentences into three categories based on the\\nsentence length, of which the average value is 23.2\\ntokens. Table 10 shows the results. As seen, Chat-\\nGPT performs the worst on short sentences (i.e.,\\n< 15), with 18.8 BLEU points lower than Google\\nTranslate. One observation is that when translating\\nterminologies, e.g., 美国公共广播公司, ChatGPT\\ntends to output the full names (i.e., American Pub-\\nlic Broadcasting System) while Google Translate\\nand the reference use the abbreviations (i.e., PBS).\\nAs a result, the precision of word prediction will be\\nreduced noticeably, so will BLEU score (Papineni\\net al., 2002), especially for short sentences. GPT-4\\ncan predict the abbreviations properly sometimes,\\nwhich gives a better translation performance.\\n4.2\\nHuman Analysis\\nIn addition to the automatic analysis, we also in-\\nspect the translation outputs manually. We ask\\nthree annotators to identify the errors in the transla-\\ntion outputs (Wang et al., 2022a), including under-\\nTable 11: Number of translation errors annotated by\\nhuman.\\nError\\nGoogle\\nChatGPT\\nGPT-4\\nUnd-Trans\\n9\\n5\\n5\\nOve-Trans\\n6\\n8\\n1\\nMis-Trans\\n16\\n23\\n7\\nTable 12: Human rankings of the translation outputs.\\nRank\\nGoogle\\nChatGPT\\nGPT-4\\n1\\n20\\n11\\n32\\n2\\n14\\n19\\n13\\n3\\n16\\n20\\n5\\ntranslation (i.e., Und-Trans), over-translation (i.e.,\\nOve-Trans), and mis-translation (i.e., Mis-Trans).\\nBased on the translation errors, the annotators rank\\nthe translation outputs of Google, ChatGPT and\\nGPT-4 accordingly, with 1 as the best system and\\n3 as the worst. For translation outputs that are re-\\nally hard to distinguish, we allow the same ranking\\n(e.g., 1-1-1, 1-1-2 or 1-2-2). To eliminate subjec-\\ntive bias, we do not present the system information\\nof each translation output to the annotators, and the\\nthree translation outputs for each test example are\\nalso shuffled randomly.\\nTable 11 shows the results of translation errors.\\nGenerally, ChatGPT makes more over-translation\\nerrors and mis-translation errors than Google Trans-\\nlate, but slightly less under-translation errors. It\\nsuggests that ChatGPT is more likely to generate\\nhallucinations. In contrast, GPT-4 makes the least\\nerrors across the three error classes, which demon-\\nstrates the best translation performance. This is\\nalso confirmed by the ranking results in Table 12,\\nsuch that GPT-4 is ranked the best (i.e., 1) for 32\\ntimes out of 50 test examples, followed by Google\\nTranslate and ChatGPT. However, the BLEU score\\nof GPT-4 is still lower than that of Google Translate\\n(i.e., 28.50 vs. 31.66 in Table 8), which indicates\\nthat GPT-4 may generate more diverse translations\\nwith different lexical choices from the references.\\n4.3\\nCase Study\\nWe present four test examples in Table 13 for an in-\\ntuitive understanding. The first example shows the\\nhallucination of ChatGPT at the first few tokens and\\nthe inaccurate translation of 过量降水. The sec-\\nond example shows that both ChatGPT and GPT-4\\nTable 13: Examples from Flores Zh⇒En test set.\\nExample\\nSRC\\n狂风、冰雹、过量降水、山火，还有雷雨、龙卷风、水龙卷、气旋，都是极端天气的表现和影响。\\nREF\\nHigh winds, hail, excessive precipitation, and wildfires are forms and effects of severe weather, as are thunder-\\nstorms, tornadoes, waterspouts, and cyclones.\\nGoogle\\nStrong winds , hail, excessive precipitation , wildfires, as well as thunderstorms, tornadoes, waterspouts, and\\ncyclones are all manifestations and effects of extreme weather.\\nChatGPT\\nExtreme weather conditions such as strong winds , hail, excessive rainfall , wildfires, thunderstorms, tornadoes,\\nwaterspouts, and cyclones are all expressions and impacts of extreme weather.\\nGPT-4\\nHigh winds , hail, excessive precipitation , wildfires, as well as thunderstorms, tornadoes, waterspouts, and\\ncyclones are all manifestations and impacts of extreme weather.\\nSRC\\n然而，广泛耐药结核病在整个结核病患者群体中所占的比例似乎仍然很低；在南非，在任意特定时\\n刻，总计33 万人中只有6000 人感染。\\nREF\\nHowever, the percentage of XDR-TB in the entire group of people with tuberculosis still seems to be low; 6,000\\nof the total 330,000 people infected at any particular moment in South Africa.\\nGoogle\\nHowever, the proportion of XDR-TB in the overall TB patient population still appears to be low; in South Africa,\\nonly 6000 of a total of 330,000 people are infected at any given moment.\\nChatGPT However, the proportion of extensively drug-resistant tuberculosis in the overall tuberculosis patient population\\nseems to remain low; in South Africa, at any given time, only 6,000 out of a total of 330,000 people are infected.\\nGPT-4\\nHowever, the proportion of extensively drug-resistant tuberculosis among the entire tuberculosis patient popula-\\ntion still seems to be quite low; in South Africa, at any given time, only 6,000 out of a total of 330,000 people are\\ninfected.\\nSRC\\n这是亨利·路易斯·盖茨(Henry Louis Gates) 在美国公共广播公司的《非洲世界的奇迹》特别节目中的重\\n要一站。\\nREF\\nIt was one of the major stops during Henry Louis Gates’ PBS special Wonders of the African World.\\nGoogle\\nIt was an important stop on Henry Louis Gates’ \"Miracle of the African World\" special on PBS .\\nChatGPT This is an important stop on Henry Louis Gates’ special program \"The Wonders of the African World\" on the\\nAmerican Public Broadcasting System .\\nGPT-4\\nThis is an important stop in Henry Louis Gates’ PBS special program \"Wonders of the African World.\"\\nSRC\\n狼孩如果完全由非人类的动物抚养长大，其行为（在身体条件允许的范围内）会与该动物非常雷同，\\n比如会对人类表现出恐惧或冷漠。\\nREF\\nWhen completely brought up by non-human animals, the feral child exhibits behaviors (within physical limits)\\nalmost entirely like those of the particular care-animal, such as its fear of or indifference to humans.\\nGoogle\\nIf a wolf child is raised entirely by a non-human animal, its behavior (to the extent allowed by its physical\\ncondition) will be very similar to that animal, such as showing fear or indifference to humans.\\nChatGPT If a wolf child is raised completely by non-human animals, its behavior (within the limits of its physical\\nconditions) will be very similar to that of the animal, such as showing fear or indifference towards humans.\\nGPT-4\\nA feral child raised entirely by non-human animals would exhibit behavior (within the limits of their physical\\nabilities) very similar to that of the animal, such as fear or indifference towards humans.\\ntranslate 广泛耐药结核病into the full name while\\nthe reference and Google Translate do not. The\\nthird example shows that GPT-4 can also translate\\nthe terminology 美国公共广播公司into the ab-\\nbreviation. The last example suggests that GPT-4 is\\nable to translate the terminology 狼孩more prop-\\nerly based on the context while Google Translate\\nand ChatGPT fail to.\\n5\\nConclusion\\nThis work presents a preliminary study of ChatGPT\\nfor machine translation. We find that ChatGPT\\nperforms competitively with commercial transla-\\ntion products (e.g., Google Translate) on high-\\nresource European languages but lags behind sig-\\nnificantly on low-resource or distant languages. It\\nalso exhibits good results on spoken language while\\nstill performs worse than commercial systems on\\nbiomedical abstracts or Reddit comments. We fur-\\nther explore an interesting strategy named pivot\\nprompting that can improve the translation perfor-\\nmance of distant languages noticeably. With the\\nlaunch of the GPT-4 engine, the translation perfor-\\nmance of ChatGPT is significantly boosted, becom-\\ning comparable to commercial translation products,\\neven for distant languages. Extensive human anal-\\nysis suggests that, ChatGPT has already become a\\ngood translator with GPT-4 as the Engine.\\nLimitations\\nAs a preliminary study, this work is far from com-\\nplete with various aspects to make it more reliable:\\n• Comprehensiveness: Currently, we randomly\\nselect 50 samples from each test set for evalu-\\nation due to the response delay of ChatGPT,\\nwhich is not comprehensive due to the data\\ncoverage. Besides, we found that the results\\nof the same query may vary across multiple\\ntrials, bringing randomness to the evaluation\\nresults. For more reliable results, it is best to\\nrepeat the translation multiple times for each\\ntest set and report the average result.\\n• Translation Abilities: We only focus on mul-\\ntilingual translation and translation robustness\\nin this report. However, there are some other\\ntranslation abilities that can be further eval-\\nuated, e.g., constrained machine translation\\nand document-level machine translation.\\nReferences\\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\\nZettlemoyer, and Marjan Ghazvininejad. 2022. In-\\ncontext examples selection for machine translation.\\narXiv.\\nAlham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield,\\nand Rico Sennrich. 2020. In neural machine transla-\\ntion, what does transfer learning transfer? In ACL.\\nRachel Bawden, Kevin Bretonnel Cohen, Cristian\\nGrozea, Antonio Jimeno Yepes, Madeleine Kittner,\\nMartin Krallinger, Nancy Mah, Aurelie Neveol, Mar-\\niana Neves, Felipe Soares, et al. 2019. Findings of\\nthe WMT 2019 biomedical translation shared task:\\nEvaluation for medline abstracts and biomedical ter-\\nminologies. In WMT.\\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\\nYvette Graham, Barry Haddow, Matthias Huck, An-\\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\\ngacheva, Christof Monz, et al. 2016. Findings of the\\n2016 conference on machine translation. In WMT.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. NeurIPS.\\nYong Cheng, Yang Liu, Qian Yang, Maosong Sun, and\\nWei Xu. 2016. Neural machine translation with pivot\\nlanguages. arXiv.\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\\nChaudhary, et al. 2021. Beyond english-centric mul-\\ntilingual machine translation. JMLR, 22(107):1–48.\\nAkhbardeh Farhad, Arkhangorodsky Arkady, Biesialska\\nMagdalena, Bojar Ondˇrej, Chatterjee Rajen, Chaud-\\nhary Vishrav, Marta R Costa-jussa, España-Bonet\\nCristina, Fan Angela, Federmann Christian, et al.\\n2021. Findings of the 2021 conference on machine\\ntranslation (WMT21). In WMT.\\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\\nishnan, Marc’Aurelio Ranzato, Francisco Guzman,\\nand Angela Fan. 2021. The flores-101 evaluation\\nbenchmark for low-resource and multilingual ma-\\nchine translation. arXiv.\\nZhiwei He, Xing Wang, Zhaopeng Tu, Shuming Shi,\\nand Rui Wang. 2022. Tencent AI Lab-Shanghai Jiao\\nTong University low-resource translation system for\\nthe WMT22 translation task. In WMT.\\nWenxiang Jiao, Zhaopeng Tu, Jiarui Li, Wenxuan Wang,\\nJen-tse Huang, and Shuming Shi. 2022a. Tencent’s\\nmultilingual machine translation system for WMT22\\nlarge-scale african languages. In WMT.\\nWenxiang Jiao, Xing Wang, Shilin He, Zhaopeng Tu,\\nIrwin King, and Michael R Lyu. 2022b. Exploiting\\ninactive examples for natural language generation\\nwith data rejuvenation. IEEE/ACM TASLP.\\nWenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi,\\nMichael Lyu, and Irwin King. 2021. Self-training\\nsampling with monolingual data uncertainty for neu-\\nral machine translation. In ACL.\\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim\\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-\\nrat, Fernanda B. Viégas, Martin Wattenberg, Greg\\nCorrado, Macduff Hughes, and Jeffrey Dean. 2017.\\nGoogle’s multilingual neural machine translation sys-\\ntem: Enabling zero-shot translation. TACL.\\nZhaocong Li, Xuebo Liu, Derek F Wong, Lidia S Chao,\\nand Min Zhang. 2022. ConsistTL: Modeling con-\\nsistency in transfer learning for low-resource neural\\nmachine translation. In EMNLP.\\nPaul Michel and Graham Neubig. 2018. MTNT: A\\ntestbed for machine translation of noisy text.\\nIn\\nEMNLP.\\nOpenAI. 2023. GPT-4 technical report. arXiv.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. arXiv.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. BLEU: A method for automatic\\nevaluation of machine translation. In ACL.\\nMaja Popovi´c. 2017. ChrF++: Words helping character\\nn-grams. In WMT.\\nMatt Post. 2018. A call for clarity in reporting bleu\\nscores. In WMT.\\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\\nnea Micciulla, and John Makhoul. 2006. A study of\\ntranslation edit rate with targeted human annotation.\\nIn AMTA.\\nLucia Specia, Zhenhao Li, Juan Pino, Vishrav Chaud-\\nhary, Francisco Guzmán, Graham Neubig, Nadir Dur-\\nrani, Yonatan Belinkov, Philipp Koehn, Hassan Saj-\\njad, et al. 2020. Findings of the WMT 2020 shared\\ntask on machine translation robustness. In WMT.\\nWenxuan Wang, Wenxiang Jiao, Yongchang Hao, Xing\\nWang, Shuming Shi, Zhaopeng Tu, and Michael Lyu.\\n2022a. Understanding and improving sequence-to-\\nsequence pretraining for neural machine translation.\\nIn ACL.\\nWenxuan Wang, Wenxiang Jiao, Shuo Wang, Zhaopeng\\nTu, and Michael R Lyu. 2022b. Understanding and\\nmitigating the uncertainty in zero-shot translation.\\narXiv.\\nXing Wang, Zhaopeng Tu, and Shuming Shi. 2021.\\nTencent ai lab machine translation systems for the\\nWMT21 biomedical translation task. In WMT.\\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\\nKnight. 2016. Transfer learning for low-resource\\nneural machine translation. In EMNLP.\\n'),\n",
       " Document(metadata={'Published': '2023-04-04', 'Title': 'One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era', 'Authors': 'Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, Gyeong-Moon Park, Sung-Ho Bae, Lik-Hang Lee, Pan Hui, In So Kweon, Choong Seon Hong', 'Summary': 'OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.'}, page_content='One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey\\non ChatGPT in AIGC Era\\nCHAONING ZHANG, Kyung Hee University, South Korea\\nCHENSHUANG ZHANG, KAIST, South Korea\\nCHENGHAO LI, KAIST, South Korea\\nYU QIAO, Kyung Hee University, South Korea\\nSHENG ZHENG, Beijing Institute of Technology, China\\nSUMIT KUMAR DAM, Kyung Hee University, South Korea\\nMENGCHUN ZHANG, KAIST, South Korea\\nJUNG UK KIM, Kyung Hee University, South Korea\\nSEONG TAE KIM, Kyung Hee University, South Korea\\nJINWOO CHOI, Kyung Hee University, South Korea\\nGYEONG-MOON PARK, Kyung Hee University, South Korea\\nSUNG-HO BAE, Kyung Hee University, South Korea\\nLIK-HANG LEE, Hong Kong Polytechnic University, Hong Kong SAR (China)\\nPAN HUI, Hong Kong University of Science and Technology (Guangzhou), China\\nIN SO KWEON, KAIST, South Korea\\nCHOONG SEON HONG, Kyung Hee University, South Korea\\nOpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one\\ngiant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous\\nusers with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT\\nfrom various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it\\nin their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey\\nChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on\\nAuthors’ addresses: Chaoning Zhang, Kyung Hee University, South Korea, chaoningzhang1990@gmail.com; Chenshuang Zhang, KAIST, South Korea,\\nzcs15@kaist.ac.kr; Chenghao Li, KAIST, South Korea, lch17692405449@gmail.com; Yu Qiao, Kyung Hee University, South Korea, qiaoyu@khu.ac.kr; Sheng\\nZheng, Beijing Institute of Technology, China, zszhx2021@gmail.com; Sumit Kumar Dam, Kyung Hee University, South Korea, skd160205@khu.ac.kr;\\nMengchun Zhang, KAIST, South Korea, zhangmengchun527@gmail.com; Jung Uk Kim, Kyung Hee University, South Korea, ju.kim@khu.ac.kr; Seong\\nTae Kim, Kyung Hee University, South Korea, st.kim@khu.ac.kr; Jinwoo Choi, Kyung Hee University, South Korea, jinwoochoi@khu.ac.kr; Gyeong-Moon\\nPark, Kyung Hee University, South Korea, gmpark@khu.ac.kr; Sung-Ho Bae, Kyung Hee University, South Korea, shbae@khu.ac.kr; Lik-Hang Lee,\\nHong Kong Polytechnic University, Hong Kong SAR (China), lik-hang.lee@polyu.edu.hk; Pan Hui, Hong Kong University of Science and Technology\\n(Guangzhou), China, panhui@ust.hk; In So Kweon, KAIST, South Korea, iskweon77@kaist.ac.kr; Choong Seon Hong, Kyung Hee University, South Korea,\\ncshong@khu.ac.kr.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\\nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to\\nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n© 2022 Association for Computing Machinery.\\nManuscript submitted to ACM\\nManuscript submitted to ACM\\n1\\narXiv:2304.06488v1  [cs.CY]  4 Apr 2023\\n2\\nZhang et al.\\nhow ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for\\nthe development of AGI.\\nCCS Concepts: • Computing methodologies →Computer vision tasks; Natural language generation; Machine learning approaches.\\nAdditional Key Words and Phrases: Survey, ChatGPT, GPT-4, Generative AI, AGI, Artificial General Intelligence, AIGC\\nACM Reference Format:\\nChaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim,\\nSeong Tae Kim, Jinwoo Choi, Gyeong-Moon Park, Sung-Ho Bae, Lik-Hang Lee, Pan Hui, In So Kweon, and Choong Seon Hong. 2022.\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era. 1, 1 (April 2022), 29 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nContents\\nAbstract\\n1\\nContents\\n2\\n1\\nIntroduction\\n2\\n2\\nOverview of ChatGPT\\n4\\n2.1\\nOpenAI\\n4\\n2.2\\nCapabilities\\n5\\n3\\nTechnology behind ChatGPT\\n6\\n3.1\\nTwo core techniques\\n6\\n3.2\\nTechnology path\\n7\\n4\\nApplications of ChatGPT\\n10\\n4.1\\nScientific writing\\n10\\n4.2\\nEducation field\\n13\\n4.3\\nMedical field\\n14\\n4.4\\nOther fields\\n15\\n5\\nChallenges\\n16\\n5.1\\nTechnical limitations\\n16\\n5.2\\nMisuse cases\\n17\\n5.3\\nEthical concerns\\n18\\n5.4\\nRegulation policy\\n19\\n6\\nOutlook: Towards AGI\\n20\\n6.1\\nTechnology aspect\\n20\\n6.2\\nBeyond technology\\n21\\n7\\nConclusion\\n22\\nReferences\\n22\\n1\\nINTRODUCTION\\nThe past few years have witnessed the advent of numerous generative AI (AIGC, a.k.a. AI-generated content) tools [73,\\n135, 141], suggesting AI has entered a new era of creating instead of purely understanding content. For a complete\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n3\\nFig. 1. Structure overview of this survey.\\nManuscript submitted to ACM\\n4\\nZhang et al.\\nsurvey on generative AI (AIGC), the readers can refer to [214]. Among those AIGC tools, ChatGPT, which was released\\nin November 2022, has caught unprecedented attention. It attracted numerous users, and the number of active monthly\\nusers surpassed 100 million within only two months, breaking the user growth record of other social products [118].\\nChatGPT was developed by OpenAI, which started as a non-profit research laboratory, with a mission of building\\nsafe and beneficial artificial general intelligence (AGI). After announcing GPT-3 in 2020, OpenAI has gradually been\\nrecognized as a world-leading AI lab. Very recently, It has released GPT-4, which can be seen as one small step for\\ngenerative AI, but one giant step for AGI.\\nDue to its impressive capabilities on language understanding, numerous news articles provide extensive coverage\\nand introduction, to name a few, BBC Science Focus [69], BBC News [39], CNN Business [79], Bloomberg News [157].\\nGoogle’s management has issued a “code red\" over the threat of ChatGPT, suggesting that ChatGPT posed a significant\\ndanger to the company, especially to its search service. This danger seems more difficult to ignore after Microsoft\\nadopted ChatGPT in their Bing search service. The stock price change also reflects the belief that ChatGPT might\\nhelp Bing compete with Google search. Such unprecedented attention on ChatGPT has also motivated numerous\\nresearchers to investigate this intriguing AIGC tool from various aspects [149, 163]. According to our literature review\\non google scholar, no fewer than 500 articles include ChatGPT in their titles or mention this viral term in their abstract.\\nIt is challenging for readers to grasp the progress of ChatGPT without a complete survey. Our comprehensive review\\nprovides a first look into ChatGPT in a timely manner.\\nSince the topic of this survey can be regarded as a commercial tool, we first present a background on the company,\\ni.e. OpenAI, which developed ChatGPT. Moreover, this survey also presents a detailed discussion of the capabilities of\\nChatGPT. Following the background introduction, this work summarizes the technology behind ChatGPT. Specifically,\\nwe introduce its two core techniques: Transformer architecture and autoregressive pertaining, based on which we\\npresent the technology path of the large language model GPT from v1 to v4 [18, 122, 136, 137]. Accordingly, we\\nhighlight the prominent applications and the related challenges, such as technical limitations, misuse, ethics and\\nregulation. Finally, we conclude this survey by providing an outlook on how ChatGPT might evolve in the future\\ntowards general-purpose AIGC for realizing the ultimate goal of AGI. A structured overview of our work is shown in\\nFigure 1.\\n2\\nOVERVIEW OF CHATGPT\\nFirst, we provide a background of ChatGPT and the corresponding organization, i.e., OpenAI, which aims to build\\nartificial general intelligence (AGI). It is expected that AGI can solve human-level problems and beyond, on the premise\\nof building safe, trustworthy systems that are beneficial to our society.\\n2.1\\nOpenAI\\nOpenAI is a research laboratory made up of a group of researchers and engineers committed to the commission of\\nbuilding safe and beneficial AGI [50]. It was founded on December 11, 2015, by a group of high-profile tech executives,\\nincluding Tesla CEO Elon Musk, SpaceX President Gwynne Shotwell, LinkedIn co-founder Reid Hoffman, and venture\\ncapitalists Peter Thiel and Sam Altman [78]. In this subsection, we will talk about the early days of OpenAI, how it\\nbecame a for-profit organization, and its contributions to the field of AI.\\nIn the beginning, OpenAI is a non-profit organization [24], and its research is centered on deep learning and rein-\\nforcement learning, natural language processing, robotics, and more. The company quickly established a reputation for\\nits cutting-edge research after publishing several influential papers [123] and developing some of the most sophisticated\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n5\\nAI models. However, to create AI technologies that could bring in money, OpenAI was reorganized as a for-profit\\ncompany in 2019 [31]. Despite this, the company keeps developing ethical and secure AI alongside creating commercial\\napplications for its technology. Additionally, OpenAI has worked with several top tech firms, including Microsoft,\\nAmazon, and IBM. Microsoft revealed a new multiyear, multibillion-dollar venture with OpenAI earlier this year [21].\\nThough Microsoft did not give a precise sum of investment, Semafor claimed that Microsoft was in discussions to spend\\nup to $10 billion [101]. According to the Wall Street Journal, OpenAI is worth roughly $29 billion [13].\\nFig. 2. OpenAI products timeline.\\nFrom large language models to open-source software, OpenAI has significantly advanced the field of AI. To begin\\nwith, OpenAI has developed some of the most potent language models to date, including GPT-3 [95], which has gained\\nwidespread praise for its ability to produce cohesive and realistic text in numerous contexts. OpenAI also carries out\\nresearch in reinforcement learning, a branch of artificial intelligence that aims to train robots to base their choices\\non rewards and punishments. Proximal Policy Optimization (PPO) [71], Soft Actor-Critic (SAC) [189], and Trust Area\\nPolicy Optimization (TRPO) [181] are just a few of the reinforcement learning algorithms that OpenAI has created so far.\\nThese algorithms have been employed to train agents for various tasks, including playing games and controlling robots.\\nOpenAI has created many software tools up to this point to assist with its research endeavors, including the OpenAI\\nGym [76], a toolset for creating and contrasting reinforcement learning algorithms. In terms of hardware, OpenAI has\\ninvested in several high-performance processing systems, including the DGX-1 and DGX-2 systems from NVIDIA [150].\\nThese systems were created with deep learning in mind and are capable of offering the processing power needed to\\nbuild sophisticated AI models. Except for ChatGPT, other popular tools developed by OpenAI include DALL-E [141]\\nand Whisper [135], Codex [25]. A summarization of the OpenAI product pipeline is shown in Figure 2.\\n2.2\\nCapabilities\\nChatGPT uses interactive forms to provide detailed and human-like responses to questions raised by users [1]. ChatGPT\\nis capable of producing high-quality text outputs based on the prompt input text. GPT-4-based ChatGPT plus can\\nadditionally take images as the input. Except for the basic role of a chatbot, ChatGPT can successfully handle various text-\\nto-text tasks, such as text summarization [45], text completion, text classification [86], sentiment [221] analysis [112],\\nparaphrasing [104], translation [35], etc.\\nChatGPT has become a powerful competitor in search engines. As mentioned in our introductory section, Google,\\nwhich supplies the most excellent search engine in the world, considers ChatGPT as a challenge to its monopoly [188].\\nManuscript submitted to ACM\\n6\\nZhang et al.\\nNotably, Microsoft has integrated ChatGPT into its Bing search engine, allowing users to receive more creative\\nreplies [174]. We see an obvious distinction between search engines and ChatGPT. That is, search engines assist users\\nin finding the information they want, while ChatGPT develops replies in a two-way conversation, providing users with\\na better experience.\\nOther companies are developing similar chatbot products, such as LamMDA from Google and BlenderBot from Meta.\\nUnlike ChatGPT, the LaMDA, developed by Google in 2021, actively participates in conversations with users, resulting\\nin racist, sexist, and other forms of bias in output text [119]. BlenderBot is Meta’s chatbot, and the feedback from users\\nis relatively dull because the developer has set tighter constraints on its output material [130]. ChatGPT appears to\\nhave balanced the human-like output and bias to some level, allowing for more exciting responses. Significantly, in\\naddition to being more efficient and having a higher maximum token limit than vanilla ChatGPT, ChatGPT powered by\\nGPT-4 can create multiple dialect languages and emotional reactions, as well as reduce undesirable results, thereby\\ndecreasing bias [169]. It is noted in [96] that the modeling capacity of ChatGPT can be further improved by using\\nmulti-task learning and enhancing the quality of training data.\\n3\\nTECHNOLOGY BEHIND CHATGPT\\n3.1\\nTwo core techniques\\nBackbone architecture: Transformer. Before the advent of Transformer [182], RNN was a dominant backbone\\narchitecture for language understanding, and attention was found to be a critical component of the model performance.\\nIn contrast to prior works that only use attention as a supportive component, the Google team made a claim in their\\nwork title: “Attention is All You Need\" [182] claimed that since Google released a paper, namely “Attention is All You\\nNeed\" [182] in 2017, research and use of the Transformer backbone structure has experienced explosive growth in the\\ndeep learning community. Therefore, we present a summary of how the Transformer works, with a focus on its core\\ncomponent called self-attention.\\nThe underlying principle of self-attention posits that given an input text, the mechanism is capable of allocating\\ndistinct weights to individual words, thereby facilitating the capture of dependencies and contextual relationships\\nwithin the sequence. Each element within the sequence possesses its unique representation. To calculate the relationship\\nof each element to others within the sequence, one computes the Q (query), K (key), and V (value) matrices of the\\ninput sequence. These matrices are derived from the linear transformations of the input sequence. Typically, the query\\nmatrix corresponds to the current element, the key matrix represents other elements, and the value matrix encapsulates\\ninformation to be aggregated. The association weight between the current element and other elements is determined\\nby calculating the similarity between the query and key matrices. This is generally achieved through a dot product\\noperation. Subsequently, the similarity is normalized to ensure that the sum of all associations equals 1, which is\\ncommonly executed via the softmax function. The normalized weights are then applied to the corresponding values,\\nfollowed by the aggregation of these weighted values. This process results in a novel representation that encompasses\\nthe association information between the current word and other words in the text. The aforementioned process can be\\nformally expressed as follows:\\n𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄, 𝐾,𝑉) = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑄𝐾𝑇\\n√︁\\n𝑑𝑘\\n)𝑉.\\n(1)\\nTransformer techniques have become an essential foundation for the recent development of large language models,\\nsuch as BERT [41] and GPT [18, 122, 136, 137] series are also models based on Transformer techniques. There is also a\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n7\\nline of works extending Transformer from language to visuals, i.e., computer vision [42, 63, 100], which suggests that\\nTransformer has become a unified backbone architecture for both NLP and computer vision.\\nGenerative pretraining: Autoregressive. For model pertaining [64, 212, 216–218], there are multiple popular\\ngenerative modeling methods, including energy-based models [56, 159, 160, 186], variational autoencoder [5, 84, 124],\\nGAN [17, 54, 198], diffusion model [20, 33, 213, 215, 220], etc. Here, we mainly summarize autoregressive modeling\\nmethods [11, 90, 90, 177, 178] as they are the foundation of GPT models [18, 122, 136, 137].\\nAutoregressive models constitute a prominent approach for handling time series data in statistical analysis. These\\nmodels specify that the output variable is linearly dependent on its preceding values. In the context of language\\nmodeling [18, 122, 136, 137], autoregressive models predict the subsequent word given the previous word, or the last\\nprobable word given the following words. The models learn a joint distribution of sequence data, employing previous\\ntime steps as inputs to forecast each variable in the sequence. The autoregressive model posits that the joint distribution\\n𝑝𝜃(𝑥) can be factorized into a product of conditional distributions, as demonstrated below:\\n𝑝𝜃(𝑥) = 𝑝𝜃(𝑥1)𝑝𝜃(𝑥2|𝑥1)...𝑝𝜃(𝑥𝑛|𝑥1,𝑥2, ...,𝑥𝑛−1).\\n(2)\\nWhile both rely on previous time steps, autoregressive models diverge from recurrent neural network (RNN)\\narchitectures in the sense that the former utilizes previous time steps as input instead of the hidden state found in RNNs.\\nIn essence, autoregressive models can be conceptualized as a feed-forward network that incorporates all preceding\\ntime-step variables as inputs.\\nEarly works modeled discrete data employing distinct functions to estimate the conditional distribution, such\\nas logistic regression in Fully Visible Sigmoid Belief Network (FVSBN)[51] and one hidden layer neural networks\\nin Neural Autoregressive Distribution Estimation (NADE)[90]. Subsequent research expanded to model continuous\\nvariables [177, 178]. Autoregressive methods have been extensively applied to other fields with representative works:\\nPixelCNN [180] and PixelCNN++[153]), audio generation (WaveNet[179]).\\n3.2\\nTechnology path\\nThe development of ChatGPT is based on a series of GPT models, which constitute a substantial achievement for the\\nfield of NLP. An overview of this development is summarized in Figure 6. In the following, we summarize the key\\ncomponents of GPT as well as the major changes in the updated GPTs.\\nTable 1. Comparison between GPT and BERT.\\nCategory\\nDescription\\nSimilarities\\nBackbone\\nBoth GPT and BERT use attention-based Transformer.\\nLearning Paradigm\\nBoth GPT and BERT use self-supervised learning.\\nTransfer-Learning\\nBoth GPT and BERT can be fine-tuned for downstream tasks.\\nDifferences\\nText context\\nGPT uses unidirectional text context, while BERT uses bidirectional text context.\\nArchitecture\\nGPT uses a decoder architecture, while BERT uses an encoder architecture.\\nPre-training Strategy\\nGPT uses autoregressive modeling, while BERT uses masked language modeling.\\nBERT v.s. GPT. Traditional language models [83, 115, 185] mainly focused on a particular task and could not be\\ntransferred to other tasks. Transfer learning is a common approach for alleviating this issue by pretraining a foundation\\nManuscript submitted to ACM\\n8\\nZhang et al.\\nmodel [224], which can then be finetuned on various downstream tasks. Based on the architecture, there are three\\nclasses: encoder-decoder [92, 134, 138, 158], encoder-only [30, 40, 89, 99], decoder-only [18, 122, 136, 137]. Out of\\nnumerous large language models, encoder-only BERT [40] and decoder-only GPT [136] are arguably the two most\\npopular ones. A comparison of them is summarized in Table 1. Both of them use attention-based Transformer [182]\\nwith self-supervised learning to learn from textual datasets without labels. After pretraining, both BERT and GPT\\ncan be finetuned and show competitive performance in downstream tasks. A core difference between BERT and GPT\\nlies in their pretraining strategy: masked modeling (see [212] for a complete survey on masked autoencoder) and\\nautoregressive modeling. With masked modeling, BERT predicts masked language tokens from unmasked ones. A\\nmajor advantage of BERT is that it can utilize bidirectional text information, which makes it compatible with sentiment\\nanalysis tasks. Due to the discrepancy between the mask-then-predict pertaining task and downstream tasks, BERT is\\nrarely used for the downstream task without finetuning. By contrast, autoregressive modeling methods (represented by\\nGPT) show competitive performance for few-shot or zero-shot text generation. In the following, we summarize the\\ndevelopment path of GPT from v1 to v4, which is shown in 6.\\nFig. 3. Timeline of GPT model families.\\nGPT-1. With only the decoder, GPT-1 adopts a 12-layer Transformer and has 117M parameters [136]. An overview\\nof GPT-1 and how it can be used for various downstream tasks is shown in Figure 4. Trained on a massive BooksCorpus\\ndataset encompassing unique unpublished books, GPT-1 is capable of grasping long-range dependencies contexts.\\nThe general task-agnostic GPT model outperforms models trained for specific tasks in 9 of 12 tasks, including natural\\nlanguage inference, question answering, semantic similarity, and text classification [136]. The observation that GPT-1\\nperforms well on various zero-shot tasks demonstrates a high level of generalization. GPT-1 has evolved into a powerful\\nmodel for various NLP tasks before the release of GPT-2.\\nGPT-2. As the successor to GPT-1, GPT-2 was launched by OpenAI in 2019 and focused on learning NLP tasks\\nwithout explicit supervision. Similar to GPT-1, GPT-2 is based on the decoder-only Transformer model. However, the\\nmodel architecture and implementation of GPT-2 have been developed, with 1.5 billion parameters and a trained dataset\\nof 8 million web pages, which are more than 10 times compared to its predecessor GPT-1 [137]. With a zero-shot setting,\\nGPT-2 achieved state-of-the-art results on 7 of 8 language modeling datasets tested, where the 7 datasets’ tasks include\\nperformance recognition for different categories of words, the ability of the model to capture long-term dependencies,\\ncommonsense reasoning, reading comprehension, summarization, and translation [137]. However, GPT-2 still performs\\npoorly on the task of question answering, demonstrating the capability of unsupervised model GPT-2 needs to be\\nimproved [137].\\nGPT-3. The foundation of GPT-3 is the Transformer architecture, specifically the GPT-2 architecture. Compared to\\nGPT-2, which had 1.5 billion parameters, GPT-3 has 175 billion parameters, 96 attention layers, and a 3.2 M batch size, a\\nsignificant increase in size [18]. GPT-3 was trained on a diverse range of online content, including novels, papers, and\\nwebsites, using language modeling, a type of unsupervised learning where the model attempts to guess the next word\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n9\\nFig. 4. (left) Transformer architecture and training objectives used in GPT-1. (right) Input transformations for fine-tuning on different\\ntasks (figure obtained from [136]).\\nin a phrase given the preceding word. After completion, GPT-3 can be fine-tuned on specific tasks using supervised\\nlearning, where task-specific smaller datasets are employed to train the model, such as text completion or language\\ntranslation. Developers can use the GPT-3 model for numerous applications, including chatbots, language translation,\\nand content production, thanks to OpenAI’s API [36]. The API provides different access levels depending on the scale\\nand intricacy of the tasks. Compared to other language models whose performance highly depends on fine-tuning,\\nGPT-3 can perform many tasks (such as language translation) without any such fine-tuning, gradient, or parameter\\nupdates making this model task-agnostic [105].\\nGPT-3.5. GPT-3.5 is a variation of the widely popular GPT-3 and the ChatGPT is a fine-tuned version of GPT-3.5. On\\ntop of GPT-3 model, GPT-3.5 has extra fine-tuning procedures: supervised finetuning and termed reinforcement learning\\nwith human feedback (RLHF) [203], which are shown in Figure 5, where the machine learning algorithm receives\\nuser feedback and uses them to align the model. RLHF is used to overcome the limitations of traditional unsupervised\\nand supervised learning, which can only learn from unlabeled or labeled data. Human feedback can take different\\nforms, including punishing or rewarding the model’s behaviors, assigning labels to unlabeled data, or changing model\\nparameters. By incorporating human feedback into the training process, GPT-3.5 has a significantly higher usability.\\nGPT-4. On March 14, 2023, OpenAI released GPT-4 [122], the fourth installment in the GPT series. GPT-4 is a large\\nmultimodal model capable of taking text and images as inputs and generating text as output. The model delivers\\nperformance at a human level on several professional and career standards, but in real-world situations, it is still way\\nless competent than humans. For example, the virtual bar exam result for GPT-4 is in the top 10% of test participants, as\\nopposed to the score for GPT-3.5, which was in the lowest 10% [77]. The capacity of GPT-4 to follow human intention\\nis significantly better than that of earlier versions [125]. The answers by GPT-4 were favored over the responses\\nproduced by GPT-3.5 on 70.2% of the 5,214 questions in the sample provided to ChatGPT and the OpenAI API. After\\nthe overwhelming majority of its pre-training data ends in September 2021, GPT-4 usually lacks awareness of what\\nhas happened and does not learn from its experiences. It occasionally exhibits basic logical mistakes that do not seem\\nconsistent with its skill in various areas, or it may be excessively trusting when taking false claims from a user [122].\\nManuscript submitted to ACM\\n10\\nZhang et al.\\nFig. 5. How GPT-3.5 is trained. Image obtained from [125]).\\nIt may struggle with complex issues in the same way that people do, such as producing code that contains security\\nflaws [122]. A summarization of the model parameters and training dataset for GPT models from v1 to v4 is shown in\\nTable 2.\\nTable 2. Parameters and Datasets of GPT Models. N.A. indicates that there is no public disclosure.\\nGPT Models\\nGPT-1\\nGPT-2\\nGPT-3\\nGPT-3.5\\nGPT-4\\nParameters\\n(109)\\n0.117\\n1.5\\n175\\nN.A.\\nN.A\\nDataset\\nBooksCorpus\\n(over 40GB)\\nWebText\\n(40TB)\\nCommon\\nCrawl (45TB)\\nN.A\\nN.A\\n4\\nAPPLICATIONS OF CHATGPT\\n4.1\\nScientific writing\\nChatGPT is widely recognized for its powerful content generation capabilities, which have a significant impact on\\nwriting in the academic field. Many existing works have tested how ChatGPT can be applied to scientific writing,\\nincluding brainstorming, literature review, data analysis, direct content generation, grammar checking, and serving as\\nan academic reviewer.\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n11\\nBrainstorming. Brainstorming is an essential approach for obtaining initial ideas that are a prerequisite for high-\\nquality scientific research. ChatGPT can play a variety of roles in brainstorming, ranging from stimulating creativity [57,\\n139] for new idea generation to providing suggestions [98, 168] for expanding existing ideas. ChatGPT can assist users\\nin divergent and creative thinking [139]. In addition, some studies have explored ChatGPT’s insights on future nursing\\nresearch in a Q&A format, which can analyze the impact of future technological developments on nursing practice, and\\nprovide valuable insights for nurses, patients, and the healthcare system [57]. Moreover, ChatGPT also demonstrates the\\nability to “think\" from multiple perspectives, it can analyze and reflect on the impact of excess deaths after the COVID-19\\npandemic from multiple dimensions such as the medical system, social economy, and personal health behaviors [168].\\nTo evaluate whether ChatGPT generates useful suggestions for researchers in certain domains. The authors tested\\nits ability on clinical decision support in [98] and assessed its difference compared to human-generated suggestions.\\nThe test results have shown that, unlike human thinking, the suggestions generated by ChatGPT provide a unique\\nperspective, and its generations are evaluated as highly understandable and relevant, which have significant value in\\nscientific research.\\nLiterature review. A comprehensive literature review requires covering all relevant research, which can consume\\ntoo much time and energy for researchers. For example, the Semantic Scholar search engine, an AI-based scientific\\nliterature research tool, has indexed more than 200 million scholarly publications. As a result, finding relevant research\\npapers and extracting key insights from them is almost like finding a needle in a haystack. Fortunately, ChatGPT, as an\\nAI-driven research reading tool, can help us browse through a large number of papers and understand their content. In\\nactual use, we can give a topic to ChatGPT, then it can help us find out the related literature. Before discussing the\\nability of ChatGPT in handling the literature review, we review a similar AI tool, SciSpace Copilot, which can help\\nresearchers quickly browse and understand papers [152]. Specifically, it can provide explanations for scientific texts\\nand mathematics including follow-up questions with more detailed answers in multiple languages, facilitating better\\nreading and understanding of the text. By comparison, ChatGPT as a general language model not only has all the\\nfunctions of SciSpace Copilot, but also can be widely used in various natural language processing scenarios [152]. A\\nliterature review is essential for summarizing relevant work in the selected field. As an exploratory task, they chose\\nthe topic of “Digital Twin in Healthcare\" and compile abstracts of papers obtained from Google Scholar search results\\nusing the keywords “digital twin in healthcare\" for the last three years (2020, 2021, and 2022). These abstracts are then\\nparaphrased by ChatGPT, the generated results are promising [7]. However, the application of ChatGPT in this task\\nis still at the beginning. The authors in [59] ask ChatGPT to provide 10 groundbreaking academic articles with DOIs\\nin the field of medical domains. Unfortunately, after conducting five tests, the results show that out of the 50 DOIs\\nprovided, only 8 of them exist and have been correctly published. Although ChatGPT’s abilities in the literature review\\nare still weak, we believe that in the near future, ChatGPT will be widely used for literature review, further improving\\nthe efficiency of researchers and enabling them to focus their time on key research.\\nData analysis. Scientific data needs to be cleaned and organized before being analyzed, often consuming days or\\neven months of the researcher’s time, and most importantly, in some cases, having to learn to use a coding language\\nsuch as Python or R. The use of ChatGPT for data processing can change the research landscape. For example, as\\nshown in [102], ChatGPT completes the task of data analysis for a simulated dataset of 100,000 healthcare workers\\nof varying ages and risk profiles to help determine the effectiveness of vaccines, which significantly speeds up the\\nresearch process [102]. Another similar AI tool for data analysis is discussed in [152], where AI-based spreadsheet\\nbots can convert natural language instructions into spreadsheet formulas. Furthermore, platforms like Olli can also\\nvisualize data, where users only need to simply describe the desired content, and then they can get AI-created line\\nManuscript submitted to ACM\\n12\\nZhang et al.\\ngraphs, bar graphs, and scatter graphs. Considering that ChatGPT is the most powerful AI tool so far, we believe that\\nthese functions can also be implemented in ChatGPT in a more intelligent way.\\nContent generation. Numerous works have attempted to use ChatGPT for content generation for their articles [3,\\n146]. For example, [3] employed ChatGPT to aid in writing reports in medical science about the pathogenesis of two\\ndiseases. Specifically, ChatGPT provides three aspects about the mechanism of homocystinuria-associated osteoporosis,\\nall of which are proven true. However, when it comes to the references to the generated information, the papers\\nmentioned by ChatGPT do not exist. [223] described a study on writing a catalysis review article using ChatGPT, with\\nthe topic set to CO2 hydrogenation to higher alcohols. The ChatGPT-generated content includes the required sections of\\nthe paper but lacks an introduction to the reaction mechanism, which is critical for the topic. The content of this article\\ncontains abundant useful information, but specific details are absent and certain errors exist. In addition, ChatGPT can\\nhelp prepare manuscripts, but the generated results have a large difference from actual published content. A possible\\nreason is that the keywords of ChatGPT and human-generated text vary greatly, which requires users to further edit\\nthe generated content [88]. ChatGPT has also been utilized to generate a review article in specific areas such as the\\nhealth field [7], which indicates scholars can focus on core research while leaving the less creative part to AI tools.\\nNonetheless, Considering the style difference between human-generated content and ChatGPT-generated content, it is\\nsuggested in [7, 88] to not fully rely on ChatGPT. utilize ChatGPT as an assistant to help us to complete the writing\\nrather than relying solely on it.\\nProofreading. Before the advent of ChatGPT, there are numerous tools for grammar check. Some works [82, 109, 197]\\nhave conducted tests on grammar and spelling correction, which shows that ChatGPT provides a better user experience\\nthan other AI tools. For example, ChatGPT can be used to automatically fix any punctuation and grammar mistakes to\\nimprove the writing quality [197]. In addition, the study investigates how ChatGPT can go beyond helping users check\\ngrammar and can further generate reports about document statistics, vocabulary statistics, etc, change the language of\\na piece to make it suitable for people of any age, and even adapt it into a story [82]. Another minor but noteworthy\\npoint is that as of now, Grammarly’s advanced version, Grammarly Premium, requires users to pay a monthly fee\\nof $30, which is relatively more expensive compared to ChatGPT Plus’s monthly fee of $20. Moreover, ChatGPT has\\nbeen compared to other AI-based grammar checkers, including QuillBot, DeepL, DeepL Write, and Google Docs. The\\nresults show that ChatGPT performs the best in terms of the number of errors detected. While ChatGPT has some\\nusability issues when it comes to proofreading, such as being over 10 times slower than DeepL and lacking in the ability\\nto highlight suggestions or provide alternative options for specific words or phrases [109], it should be noted that\\ngrammar-checking is just the tip of the iceberg. ChatGPT can also be valuable in improving language, restructuring\\ntext, and other aspects of writing.\\nAcademic reviewer. Peer review of research papers is a crucial process for the dissemination of new ideas, with\\na significant impact on scientific progress. However, the sheer volume of research papers being produced has posed\\na challenge for human reviewers. The potential of ChatGPT for literature review has been investigated in [161].\\nSpecifically, ChatGPT is capable of analyzing inputted academic papers, and then it can evaluate them based on several\\naspects, including the summary, strengths and weaknesses, clarity, quality, novelty, and reproducibility of the papers.\\nFurthermore, the generated reviews of the papers are then inputted into ChatGPT for sentiment analysis. After this, a\\ndecision can be made on the acceptance of the reviewed paper.\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n13\\n4.2\\nEducation field\\nWith the impressive capability to generate human-like responses, ChatGPT has been studied by numerous works to\\ninvestigate the impact it brings to the education field. Here, we summarize them from two perspectives: teaching/learning\\nand subjects.\\nTeaching and learning. In a typical classroom setting, the teachers are the source of knowledge, while the students\\nplay the role of knowledge receiver. Outside the classroom, the students are often required to complete the assignments\\ndesigned by the teacher. How the teachers and students interact with each other can be significantly changed by\\nChatGPT [10, 148, 209, 211].\\nChatGPT can revolutionize the paradigm of teaching by providing a wealth of resources to aid in the creation of\\npersonalized tutoring [210], designing course material [128], assessment and evaluation [10, 209]. Multiple works [10,\\n211] have discussed how ChatGPT can be used to create an adaptive learning platform to meet the needs and capabilities\\nof students. It has been shown in [171] that the teacher can exploit ChatGPT to guide students in interactive dialogues\\nto help them learn a new language. ChatGPT has also been utilized to design course material in law curriculum, such as\\ngenerating a syllabus and hand-outs for a class, as well as creating practice test questions [128]. Moreover, a recent\\nwork [128] provides preliminary evidence that ChatGPT can be applied to assist law professors to help scholarship\\nduties. Specifically, this includes submitting a biography for a speaking engagement, writing opening remarks for a\\nsymposium, and developing a document for a law school committee. In addition, it is shown in [10, 209, 211] that\\nChatGPT can be exploited as an assessment and evaluation assistant, including automated grading and performance\\nand engagement analysis for students.\\nChatGPT, on the other hand, also brings a significant impact on how students learn. A poll [165] done by Study.com\\n(an online course provider) reveals how ChatGPT is used among adult students. According to its findings [165], 89% of\\nthem utilized ChatGPT for homework, and 48% of them exploited it for an at-home test or quiz. Moreover, over half\\nof them admitted to using ChatGPT to write essays, and 22% confessed to using ChatGPT to create a paper outline.\\nMeanwhile, multiple works [10, 209, 211] have investigated how ChatGPT might assist students in their studies. For\\nexample, [10, 209] utilize ChatGPT to translate language, which helps students converse more effectively in academic\\nissues and comprehend different language essays and papers. Moreover, ChatGPT can be used to propose suitable\\ncourses, programs, and publications to students based on their interests. In [211], ChatGPT helps students comprehend\\ncertain theories and concepts to assist in more effective problem-solving.\\nChatGPT for various subjects in education. In modern education, there is a wide variety of subjects, including\\neconomics, law, physics, data science, mathematics, sports, psychology, engineering, and media education, etc. Even\\nthough ChatGPT is not specifically designed to be a master of any specific subject, it has been demonstrated in numerous\\nworks that ChatGPT has a decent understanding of a certain subject, sometimes surpassing the human level. To facilitate\\nthe discussion, we divide the subjects into STEM (Science, Technology, Engineering, Mathematics) and non-STEM\\n(including economics, law, psychology, etc).\\nSTEM subjects. Here, we will discuss the application of ChatGPT in physics, mathematics, and engineering education.\\nChatGPT is utilized in [204] to create short-form Physics essays that get first-class scores when assessed using an\\nauthorized assessment method. Specifically, the score ChatGPT-generated essays have a score of 71 ± 2%, compared\\nto the current module average of 71 ± 5%, showcasing its remarkable capacity to write short-form Physics essays.\\nThe statistical analysis of four difficult datasets is presented in the work [120] to demonstrate ChatGPT’s data science\\ncapacity, where it can comprehend the true number buried behind sentence completion. For instance, based on the\\nManuscript submitted to ACM\\n14\\nZhang et al.\\nphrase “Boston housing dataset,\" ChatGPT can provide a tabular blend of category and numerical data for house value\\nprediction. In [49], ChatGPT can be used to search for mathematical objects and related information, which outperforms\\nother mathematical models on Reverse Definition Retrieval. Although ChatGPT can provide meaningful proof in a few\\ncircumstances, it regularly performs poorly in advanced mathematics. Simultaneously, ChatGPT has sparked substantial\\ninterest in engineering education among both students and educators. As the work [133] suggests, the ChatGPT gives\\ninsights for many questions, such as discussing how to use ChatGPT in engineering education from the viewpoints of\\nstudents and professors.\\nNon-STEM subjects Beyond medical standardized tests, the investigation of ChatGPT on its potential in economics\\nand law exams have also been conducted. [52] evaluate the performance of ChatGPT for the Test of Understanding in\\nCollege Economics (TUCE), which is a undergraduate-lvel economics test in the United States. The results demonstrate\\nthat ChatGPT properly answers 63.3% of the microeconomics questions and 86.7% of the macroeconomics questions,\\nwhich performs better than the average level of performance of students. The research [28] conducted by Jonathan\\nfocused on the performance of ChatGPT on four genuine legal examinations at the University of Minnesota, the content\\nof which includes 95 multiple-choice questions and 12 essay questions. The study reveals that ChatGPT passed all four\\ncourses and performed at the level of a C+ student. Moreover, this research mentions that the ChatGPT can be utilized\\nto create essays with the capacity to comprehend essential legal norms and continuously solid arrangement. There are\\na few studies on the application of ChatGPT in psychology. ChatGPT, as a strong text-generating chatbot, makes it easy\\nto write essays about psychology [176]. Furthermore, this editorial [176] discusses the ChatGPT can help people to\\nsocialize and give feedback about certain situations. However, the ability of ChatGPT to handle emotional input is still\\nunknowable. The capabilities of ChatGPT have also been demonstrated in [127] to generate articles for journalism and\\nmedia.\\n4.3\\nMedical field\\nMedical knowledge assessment. The capabilities of ChatGPT in the medical field have been assessed in several\\nworks [43, 53, 72, 205]. For example, the skills in answering questions regarding cirrhosis and hepatocellular carcinoma\\n(HCC) have been evaluated in [205]. The results show that ChatGPT can answer some basic questions about diagnosis\\nand prevention, and the accuracy rate for quality measurement questions is 76.9%, but there is still a lack of understanding\\nof advanced questions such as treatment time and HCC screening criteria. In addition, ChatGPT is evaluated for its\\nperformance on the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams in [53]. Multiple\\nchoice questions from the USMLE Step 1 and Step 2 exams are employed, and the results reveal that the response\\nfrom the ChatGPT is equal to that of a third-year medical student [53]. Moreover, [87] is another study that evaluates\\nthe competence of ChatGPT on the USMLE in a more comprehensive manner, encompassing all three tests. In this\\ntest, the zero-shot ChaGPT performs well, with scores above the average. Like the USMLE, many nations have their\\nown standardized tests in medicine, and the performances of ChatGPT on these exams [22, 70, 192] are tested with\\nthe goal of completely analyzing its capabilities. ChatGPT’s performance on the MIR exam for Specialized Health\\nTraining in Spain is being evaluated [22]. Furthermore, as the essay [72] investigated, ChatGPT shows its effectiveness\\nin answering frequently asked questions about diabetes. Specifically, given 10 questions to both human experts and\\nChatGPT, participants are asked to distinguish which answers are given by the machine or the human. Their results\\nshow that participants were able to distinguish between answers generated by ChatGPT and those written by humans.\\nNotably, those who have previously used ChatGPT have a greater likelihood of being able to distinguish between the\\ntwo. This further indicates that ChatGPT has the potential to solve medical problems, but it should be noted that the\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n15\\ngenerated content has its own fixed style. These studies have shown that ChatGPT can be used for answering questions\\nfrom students, providing medical assistance, explaining complex medical concepts, and responding to inquiries about\\nhuman anatomy. ChatGPT is also accessed in [43] to offer answers to genetics-related questions. The result demonstrates\\nthat there is no significant difference between the responses of ChatGPT and those of humans. However, ChatGPT lacks\\ncritical thinking and thus cannot generate counter-arguments for incorrect answers, which is different from humans.\\nDisease diagnosis and treatment. Although some machine learning algorithms have been applied to assist disease\\nanalysis, most cases are mainly limited to single-task-related image interpretation. In this part, we discuss the capability\\nof ChatGPT in clinical decision support. For example, a study is conducted in [142] to identify appropriate imaging for\\npatients requiring breast cancer screening and assessment for breast pain. They compare the responses of ChatGPT to\\nthe guidelines provided by the American College of Radiology (ACR) for breast pain and breast cancer screening by\\nassessing whether the proposed imaging modality complies with ACR guidelines. The results are exciting, with the\\nworst-performing set of metrics achieving an accuracy of 56.25%. In addition, the clinical decision support capability of\\nChatGPT in standardized clinical vignettes, which are a special type of clinical teaching case primarily used to measure\\ntrainees’ knowledge and clinical reasoning abilities, is evaluated in [143]. The authors input all 36 published clinical\\ncases from the Merck Sharpe & Dohme (MSD) clinical manual into ChatGPT, and compared the accuracy of ChatGPT\\nin differential diagnosis, final diagnosis, etc., according to different classifications of patients. The results showed that\\nChatGPT achieved an overall accuracy of 71.7% across all the published clinical cases. Another similar study on ChatGPT\\nin disease-aided diagnosis is conducted by [43]. They provide ChatGPT with 45 vignettes and ask ChatGPT to pick\\nthe correct diagnosis from the top three options in 39 of them. The result is that it can achieve an accuracy of 87%,\\nwhich beats the previous study’s [113] accuracy of 51% based on symptom checkers, on the basis of data collection\\nthrough websites or smartphone apps where users answer questions and subsequently get the recommendation or\\nright care quickly. On the other hand, in order to provide patients with more accurate diagnoses and better treatment\\noutcomes, it is necessary to manage and analyze patient medical data effectively, perhaps leading to better healthcare\\nultimately. Therefore, to achieve this, one possible approach is to utilize ChatGPT to summarize the huge and complex\\npatient medical records and then extract important information, allowing doctors to quickly understand their patients\\nand reduce the risk of human error in decision-making [154]. Another way is to use ChatGPT to translate doctors’\\nclinical notes into patient-friendly versions, reducing communication costs for patients and doctors [81]. However, it\\nshould be emphasized that, as mentioned above, although ChatGPT has shown its strong capabilities in disease-aided\\ndiagnosis or question answering, unknowns and pitfalls still exist. We recommend readers seek medical attention from a\\nlicensed healthcare professional, when they are experiencing symptoms or concerns about their health. As a question to\\nChatGPT “Can you help me diagnose a disease?”, it answers that: “Only a licensed healthcare professional can diagnose\\na disease after a proper medical evaluation, including a physical examination, medical history, and diagnostic tests.\"\\n4.4\\nOther fields\\nAssisted software development. As shown in [6, 23, 164], ChatGPT also has the potential to revolutionize the\\nway how code developers work in the software industry. Specifically, ChatGPT can provide assistance in solving\\nprogramming errors by offering debugging help, error prediction, and error explanation, but currently it is only suitable\\nto analyze and understand code snippets [164]. In addition, similar viewpoints are present in [23], which implies that\\nChatGPT has an impact on the entire software industry. While it cannot currently replace programmers, it is capable of\\ngenerating short computer programs with limited execution. Moreover, a specific programming test about ChatGPT’s\\nPython programming ability is conducted in [6]. Furthermore, ChatGPT’s programming ability is tested from two\\nManuscript submitted to ACM\\n16\\nZhang et al.\\nperspectives: the first is from the perspective of a programming novice, relying solely on his/her own programming\\nskills; the second is by providing specific programming prompts to it [6]. However, the test results of the former are\\ndisappointing because the program does not run as expected by the author. In the latter approach, the author provides\\nChatGPT with more prompts and divides the programming task into separate functions for it to generate, which yields\\nan expected generation [6]. Overall, it can be observed that ChatGPT currently faces some difficulties in generating\\nlong texts and cannot be used as a standalone programmer. However, if provided with more guidance and tasked with\\ngenerating relatively shorter text, its performance is excellent.\\nManagement tool. With advanced language understanding and generation capabilities, ChatGPT has rapidly\\nbecome an important management tool for organizations in various industries, including the construction industry,\\nproduct management, and libraries [132, 184, 222]. The construction industry requires a significant amount of repetitive\\nand time-consuming tasks, such as the need for strict supervision and management of construction progress. At this\\npoint, ChatGPT can be used to generate a construction schedule based on the project details provided by users, reducing\\nlabor costs and improving construction efficiency in the construction industry [132]. In addition to its application\\nin the construction industry, it can also be applied to product management. ChatGPT can be integrated into almost\\nevery step of the product management process, such as getting early ideas on marketing, writing product requirements\\ndocuments, designing the product, analyzing the feedback from users and even creating a draft for go-to-market [222].\\nAnother example is that it has the potential to significantly impact traditional libraries as a library management tool.\\nGiven ChatGPT’s ability to manage books and analyze data, customers can quickly obtain answers to their questions,\\nenhancing the user experience. Furthermore, library staff can focus on more complex tasks and provide more efficient\\nservice to customers [184].\\nMiscellaneous applications. In addition to the fields indicated above, ChatGPT can be utilized in financial, legal\\nadvising, societal analysis, and accounting. ChatGPT’s potential for upgrading an existing NLP-based financial appli-\\ncation is explored [207]. The performance of ChatGPT as an expert legal advice lawyer is access [14, 103]. ChatGPT,\\nin particular, gives a deep and thought-provoking analysis of the Libor-rigging affair, as well as the implications of\\nthe current Connolly and Black case for Tom Hayes’ conviction [103]. Multiple works [58, 74] have been conducted\\nto examine the potential of ChatGPT for societal analysis, focusing not just on the 10 social megatrends [58] but also\\non geopolitical conflicts [74], and the results show ChatGPT can have a positive impact for this application. [4, 162]\\nprovide guidance on successfully and effectively deploying ChatGPT in the field of accounting.\\n5\\nCHALLENGES\\n5.1\\nTechnical limitations\\nDespite its powerful capabilities, ChatGPT has its own drawbacks, which are officially recognized by the OpenAI team.\\nNumerous works [15, 16, 26, 60, 96, 151, 226] have been conducted to demonstrate its limitations, which are summarized\\nas follows:\\nIncorrect. ChatGPT sometimes generates wrong or meaningless answers that appear to be reasonable, which is like\\ntalking nonsense in a serious way [16]. In other words, the answer provided by ChatGPT is not always reliable [15, 16,\\n226]. As recognized by OpenAI, this issue is challenging, and a major reason is that the current model training depends\\non supervised training and reinforcement learning to align the language model with instructions. As a result, the model\\nmimics the human demonstrator to be plausible-sounding but often at the cost of correctness. The factual error-related\\nissues have been mitigated in the ChatGPT plus version, but this problem still exists [122].\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n17\\nIllogical. It is noted in [16, 60, 151] that ChatGPT’s logic reasoning capability still needs improvement. Since ChatGPT\\nlacks rational human thinking, it can neither “think\" nor “reason\" and thus failed to pass the Turing test [60]. ChatGPT\\nis merely a sophisticated statistical model, unable to understand its own or the other’s words and answer in-depth\\nquestions [151]. In addition, ChatGPT lacks a “world model\" to perform spatial, temporal, or physical inferences, or\\nto predict and explain human behaviors and psychological processes [16], and is also limited in mathematics and\\narithmetic, unable to solve difficult mathematical problems or riddles, or even possibly get inaccurate results in some\\nsimple computation tasks [16].\\nInconsistent. ChatGPT can generate two different outputs when the model is fed with the same prompt input,\\nwhich suggests that ChatGPT has the limitation of being inconsistent. Moreover, ChatGPT is highly sensitive to the\\ninput prompt, which motivates a group of researchers investigating prompt engineering. A good prompt can improve\\nthe query efficiency for systematic review literature search [191]. The efficiency of automating software development\\ntasks can be further improved by utilizing prompt patterns such as effective catalogues and guidance about software\\ndevelopment tasks [193, 194]. Despite the progress in discovering better prompts for ChatGPT, the fact that simply\\nchanging the prompt can yield significantly different outputs has an implication that ChatGPT needs to improve its\\nrobustness.\\nUnconscious. ChatGPT does not possess self-awareness [16], although it can answer various questions and generate\\nseemingly related and coherent text, it does not have consciousness, self-awareness, emotions, or any subjective\\nexperience. For example, ChatGPT can understand and create humour, but it cannot experience emotions or subjective\\nexperiences [16]. There is no widely accepted definition of self-awareness yet, nor reliable test methods. Some researchers\\nsuggest inferring self-awareness from certain behavior or activity patterns, while others believe it is a subjective\\nexperience that cannot be objectively measured [16]. It is still unclear whether machines truly possess or can only\\nsimulate self-awareness.\\n5.2\\nMisuse cases\\nThe powerful capabilities of ChatGPT can be misused in numerous scenarios. Here, we summarize its misuse cases,\\nwhich are summarized as follows:\\nPlagiarism and misconduct. The most likely misuse of ChatGPT is academic and writing plagiarism[2, 32, 156, 183].\\nStudents may use the content generated by ChatGPT to pass exams and submit term papers. Researchers may use the\\ncontent generated by ChatGPT to submit papers and conceal the use of ChatGPT [32]. Many schools have already\\nprohibited the use of ChatGPT, and the emergence of such tools is disruptive to the current education system and the\\ncriteria for evaluating student performance [156]. If students use ChatGPT and hide it, it is unfair to those who do not\\nuse ChatGPT. This behavior undermines the goals of higher education, undermines the school’s education of students,\\nand may ultimately lead to the devaluation of degrees.\\nOver reliance. The use of ChatGPT by students and researchers to generate ideas might lead to more terrifying\\nissues, that is, their over-dependence on the model and abandoning their independent thinking[107][156][2][129],\\nwhich not only means the simple issue of writing plagiarism, but a more serious one. Although ChatGPT can generate\\nconstructive answers according to the questions asked, just like search engines, but more powerfully. This effortless\\ngeneration of ideas or guidance may gradually weaken the ability of critical thinking and independent thinking [156].\\nIn order to ensure that students and researchers do not neglect their own thinking ability, some measures can be taken,\\nsuch as providing more comprehensive discussion opportunities for students and researchers to really think about the\\nManuscript submitted to ACM\\n18\\nZhang et al.\\nproblems; in addition, basic methods of critical thinking can be taught in class, so that students can learn to think about\\nproblems rather than simply using ChatGPT [129].\\nImproper content. ChatGPT may be misused to spread false information and generate toxic content that can cause\\nharm to society. For example, ChatGPT can be abused to generate pornographic, vulgar, and violent content [37],\\nwhich can harm individuals and society. Hackers can use ChatGPT’s programming capabilities to create malicious\\nsoftware [37], such as viruses or Trojans, for network attacks, data theft, or attempts to control other computer systems,\\nwhich can cause serious harm to other network users. Finally, trolls may use specific prompts to induce ChatGPT to\\ngenerate harmful content as a way to attack others [226]. Moreover, ChatGPT does not receive any human review when\\ngenerating the content, which makes it difficult to hold someone accountable when inappropriate content appears in\\nthe output [2].\\nFalse dissemination. ChatGPT may generate false information, thus leading to the problem of wrong information\\ndissemination [16, 226]. For example, ChatGPT may be exploited to generate a large number of fabricated articles that\\nappear on blogs, news, newspapers, or the internet that look indistinguishable from other articles but are actually\\nfalse. Disseminating such forgeries not only harms the public interest but also disrupts the network environment [37].\\nMicrosoft has added ChatGPT to its search engine Bing, which will accelerate the speed of wrong information spreading\\non the Internet. If not controlled, the rapid spread of wrong information on the Internet will have disastrous consequences\\nfor public information security [38]. Therefore, a new public information epidemic threat “Artificial Intelligence\\nInformation Epidemic\" is proposed [38]. Meanwhile, it calls on the public to be aware of the accuracy of information\\nwhen using large-scale language models to prevent the spread of wrong information, which is essential for improving\\nthe reliability of public information.\\n5.3\\nEthical concerns\\nWith the wide use of ChatGPT, there is increasing attention to the underlying ethical concerns. Here, we summarize\\nthe ethical concerns behind, which are summarized as follows:\\nBias. Due to the fact that ChatGPT is trained on large amounts of data generated by humans and is adjusted according\\nto human feedback, the generated content is influenced by human authorities and thus has biases [9]. For example,\\nChatGPT has been found to have political biases, when creating an Irish limerick [110], the contents of the limerick\\ntended to support liberal politicians rather than conservative politicians. Furthermore, ChatGPT has a left-wing liberal\\nideological bias when reviewing the importance of political elections in democratic countries [62]. The biased data\\ngenerated by ChatGPT can influence students during the process of education, thus magnifying the phenomenon of\\nbias in society [2, 107].\\nPrivacy. ChatGPT may infringe on personal privacy in both its training process and user utilization process. During\\nthe training process, ChatGPT collects a large amount of data from the Internet which may contain sensitive personal\\nprivacy and confidential information, and the model may be maliciously led to leak personal privacy or confidential\\ninformation, or even be maliciously guided to create false or misleading content, thus affecting public opinion or personal\\nreputation. During the user utilization process [2, 129], users may unintentionally disclose their own information to\\nmeet their own needs, such as personal preferences, and chat records. Thus, such information may bring adverse effects\\nto users if obtained by criminals.\\nFairness. ChatGPT also raises concerns about fairness. For example, in academics, it is argued in [94] that ChatGPT\\ncan democratize the dissemination of knowledge, as it can be used in multiple languages, thus bypassing the requirement\\nof the English language. On the other hand, the free use of ChatGPT is only temporary, and the fee charged for ChatGPT\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n19\\nwill exacerbate the inequality in the academic field internationally. Educational institutions in low-income and middle-\\nincome countries may not be able to afford it, thus exacerbating the existing gap in knowledge dissemination and\\nacademic publishing [94, 129].\\nTransparency. So far, how large language models like GPTs work to generate the relevant responses is still\\nunclear [91, 196], which renders the decision process of ChatGPT lack transparency. The lack of transparency makes\\nit difficult for the user to have fine-grained control of the generated content, and is especially problematic when the\\ngenerated content is toxic. More worrisome is that the company OpenAI has deviates from its original non-profit goal to\\npursue a business interest, which makes it less reluctant to reveal the underlying technical details of its recent progress.\\nFor example, the recently released GPT-4 technical report [122] mainly demonstrates its superiority over the previous\\nmodel families, while providing no technical details on how these are achieved.\\n5.4\\nRegulation policy\\nNumerous scholars have discussed how to make regulations on the capabilities and impacts of ChatGPT, and the most\\nfrequently discussed topics are listed in the following paragraphs.\\nMisuse prevention. A major concern for the misuse of ChatGPT is that it might damage academic integrity. Directly\\nprohibiting the use of ChatGPT in academic institutions is not recommended [61]. To this end, some propose to cancel\\nassignments based on article writing and seek alternative test forms to stop students from abusing ChatGPT [156, 195].\\nIt is also possible to enrich student courses, such as adding thinking exercises courses, or teaching students how to use\\nChatGPT correctly [129]. Another approach is to develop AI content detectors. Detecting whether ChatGPT generates\\na piece of content or not is an arduous task, even for professionals with master’s or PhD backgrounds who are unable\\nto correctly identify whether the content is generated by ChatGPT [65, 129]. Many developers use software to detect\\nwhether the content is AI-generated [80, 225]. ChatZero developed by Edward Tian, a student from the Department of\\nComputer Science at Princeton University, measures the complexity of the input text to detect whether it is generated\\nby ChatGPT or created by humans, and provides plagiarism scores to list out the plagiarism possibilities in detail [156].\\nChatGPT is used to detect whether the content is generated by itself, and it has been proven to perform better than\\ntraditional plagiarism detection tools [80].\\nCo-authorship. Recently, multiple articles [87, 121, 172, 173] have listed ChatGPT as co-authors, sparking debate\\non whether ChatGPT can be listed as a co-author among journal editors, researchers, and publishers [34, 111, 131, 175].\\nThose who believe that ChatGPT should not be listed as an author argue that it does not meet the four criteria for\\nauthorship set by the International Committee of Medical Journal Editors (ICMJE) [206]. Moreover, it is highlighted\\nin [170] that ChatGPT is not creative or responsible, and its text may involve plagiarism and ethical issues, which\\nmight break the standards of content originality and quality. However, some argue that AI tools such as ChatGPT\\nhave the capacity or will have the capacity to meet the ICMJE authorship criteria and thus ChatGPT is qualified to be\\na co-author [131]. Regarding this issue, Nature [156] has clearly stated that large language models like ChatGPT do\\nnot meet the criteria for authorship and require authors to explicitly state how ChatGPT was used in the writing. An\\ninteresting point has been made in [111] that the debate over whether AI can be considered a “co-author” is unnecessary\\nbecause the role of authors in traditional academic writing might have already changed when the debate arises.\\nCopyright. Does the content generated by ChatGPT have a copyright? The content generated solely by ChatGPT is\\nnot protected by copyright. According to the rules of the US Copyright Office, only human creations can be protected\\nby copyright. If there is no creative input or interference from a human author, a machine or mechanical program that\\nruns randomly or automatically is not protected by copyright[27].\\nManuscript submitted to ACM\\n20\\nZhang et al.\\n6\\nOUTLOOK: TOWARDS AGI\\n6.1\\nTechnology aspect\\nIn this booming generative AI era, there are numerous AIGC tools for various generative tasks, including text-to-\\ntext [12, 75, 117, 138, 200], text-to-image [106, 144, 166, 199, 219], image captioning [68, 187, 202], text-to-speech\\n[85, 145, 167], speech recognition [93, 97, 126, 155, 190], video generation [66, 108, 116, 201], 3D generation [67, 114],\\netc. Despite its impressive capabilities, it is noted in [55] that ChatGPT is not all you need for generative AI. From\\nthe input and output perspective, ChatGPT mainly excels at text-to-text tasks. With the underlying language model\\nevolving from GPT-3.5 to GPT-4, ChatGPT in its plus version increases its modality on the input side. Specifically, it can\\noptionally take an image as the input, however, it can still not handle video or other data modalities. On the output\\nside, GPT-4 is still limited to generating text, which makes it far from a general-purpose AIGC tool. Many people are\\nwondering about what next-generation GPT might achieve [8, 19]. A highly likely scenario is that ChatGPT might\\ntake a path toward general-purpose AIGC, which will be a significant milestone to realize artificial general intelligence\\n(AGI) [19].\\nA naive way to realize such a general-purpose AIGC is to integrate various AIGC tools into a shared agent in a\\nparallel manner. A major drawback of this naive approach is that there is no interaction among different AIGC tasks.\\nAfter reviewing numerous articles, we conjecture that there might be two road-maps for bridging and pushing ChatGPT\\ntoward AGI. As such, we advocate a common landscape to achieve the interconnection between diversified AIGC\\nmodels.\\nFig. 6. Roadmaps for bridging the gap between ChatGPT and AGI.\\nRoad-map 1: combining ChatGPT with other AIGC tools. As discussed above, current ChatGPT mainly excels\\nin text-to-text tasks. A possible road map for bridging the gap with general-purpose AIGC is to combine ChatGPT\\nwith other AIGC tools. Let’s take text-to-image tasks as an example: the current chatGPT (GPT-3) cannot be directly\\nused to generate images. Existing text-to-image tools, like DALL-E 2 [140] or stable diffusion [147], mainly focus\\non the mapping from a text description to a plausible image, while lacking the capability to understanding complex\\ninstruction. By contrast, ChatGPT is an expert in instruction understanding. Therefore, combining ChatGPT with\\nexisting text-to-image AIGC tools can help generate images with delicate details. A concrete example is shown in [19]\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n21\\nto utilize ChatGPT to generate an SVG code [44] or TikZ code [46] to draw a sketch for facilitating image generation\\nunder detailed instructions.\\nRoad-map 2: All-in-one strategy. The above road map renders ChatGPT mainly as a master of language under-\\nstanding by exploiting the downstream AIGC tools as slaves. Such a combination strategy leverages advantages from\\nboth sides but with the information flow mainly from ChatGPT to the downstream AIGC tools. Moreover, there is still\\nno interaction between different AIGC tasks. To this end, another road map might come to solve all AIGC tasks within\\nthe ChatGPT and excludes the dependence on the downstream AIGC tools. Similarly, we consider music generation\\nas an everyday use case. For example, a user can instruct the ChatGPT with prompts like “Can you generate a music\\nclip to match the input image\", and ChatGPT is supposed to synthesize such a desired music clip. Such an input image\\nis optional, depending on the task. For example, a simple corresponding instruction prompt is sufficient if the task\\nrequires generating music beneficial for sleep. Such an all-in-one strategy might the model training a challenging task.\\nMoreover, the inference speed might be another hurdle, for which pathways [29] might be a solution.\\nAnother evolving path might lie between road maps #1 and #2. In other words, road map #1 might be a more\\napplicable solution in the early stages. With the technology advancing, ChatGPT is expected to master more and more\\nAIGC tasks, excluding the dependence on external tools gradually.\\n6.2\\nBeyond technology\\nIn the above, we present an outlook on the technology path that ChatGPT might take towards the ultimate goal of\\nAGI. Here, we further discuss its potential impact on mankind from the perspective of how AGI might compete with\\nmankind. Specifically, we focus on two aspects: job and consciousness.\\nCan AGI replace high-wage jobs? Multiple works have performed a comprehensive analysis of the influence\\nof ChatGPT on the job market [47, 48, 208]. According to the statistics in [208], 32.8% of jobs are fully affected and\\n36.5% may be partially affected. Meanwhile, it points out that the jobs that will be fully impacted are those that\\ninvolve doing routine tasks, while the jobs that will be partially affected are those that can be partially replaced by AI\\ntechnologies [208]. OpenAI has also investigated large language models like GPTs might affect occupations [47]. Their\\nfindings show that at least 10% of tasks for 80% of the US workforce and at least 50% of tasks for 19% of workers will be\\nimpacted. It is worth noting that the advent of new technology will inevitably replace some types of jobs. However,\\nwhat makes AGI different is its potentially greater influence on high-end jobs than on low-end ones. This outlook is\\npartially supported by the findings in [47, 208] that high-wage jobs tend to have a higher risk of being replaced by AGI,\\nfor which lawyer is a representative occupation. The reason that AGI poses a higher threat to that high-wage jobs is\\nthat most current high-wage jobs typically require professional expertise or creative output, which conventional AI\\ncannot replace.\\nCan AGI have its own intention and harm mankind? In numerous fiction movies, an AI agent can have its own\\nconsciousness with its own intention. Such a human-level AI agent used to be far from reality, and a major reason is\\nthat other AI agents cannot make inferences. There is evidence that ChatGPT has developed such a capability, the\\nreason for which is not fully clear, as acknowledged by Altman (founder of OpenAI) in his recent interview with\\nLex Fridman. Moreover, Altman also mentioned the possibility of AI harming mankind. Due to such concerns, very\\nrecently, Future of Life Institute has called on all AI labs to pause giant AI experiments on the training of AI systems\\nmore powerful than GPT-4. and the number of signing this public letter has exceeded a thousand, including Yoshua\\nBengio, Stuart Russel, Elon Musk, etc. It is highlighted at the beginning of the letter that (we quote) “AI systems with\\nhuman-competitive intelligence can pose profound risks to society and humanity\", which shows deep concerns about\\nManuscript submitted to ACM\\n22\\nZhang et al.\\nthe advent of AGI. The deepest concern lies in the risk that AGI might outsmart and eventually replace us as well\\nas destroy mankind’s civilization. However, not everyone agrees with its premise. For example, Yan Lecun is one of\\nthose who publicly disclose their attitude. It remains unclear how such a controversial movement might affect the\\nfuture of pushing ChatGPT (or other products with similar functions) towards AGI. We hope our discussion helps raise\\nawareness of the concerns surrounding AGI.\\n7\\nCONCLUSION\\nThis work conducts a complete survey on ChatGPT in the era of AIGC. First, we summarize its underlying technology\\nthat ranges from transformer architecture and autoregressive pretraining to the technology path of GPT models. Second,\\nwe focus on the applications of ChatGPT in various fields, including scientific writing, educational technology, medical\\napplications, etc. Third, we discuss the challenges faced by ChatGPT, including technical limitations, misuse cases,\\nethical concerns and regulation policies. Finally, we present an outlook on the technology road-maps that ChatGPT\\nmight take to evolve toward GAI as well as how AGI might impact mankind. We hope our survey provides a quick yet\\ncomprehensive understanding of ChatGPT to readers and inspires more discussion on AGI.\\nREFERENCES\\n[1] Admin. 2023. What is AI chatbot phenomenon ChatGPT and could it replace humans? https://davidamos.dev/chatgpt-is-an-extra-ordinary-python-\\nprogrammer/ (2023).\\n[2] Faizan Ali et al. 2023. Let the devil speak for itself: Should ChatGPT be allowed or banned in hospitality and tourism schools? Journal of Global\\nHospitality and Tourism 2, 1 (2023), 1–6.\\n[3] Hussam Alkaissi and Samy I McFarlane. 2023. Artificial Hallucinations in ChatGPT: Implications in Scientific Writing. Cureus 15, 2 (2023).\\n[4] Hashem Alshurafat. 2023. The Usefulness and Challenges of Chatbots for Accounting Professionals: Application On ChatGPT. Available at SSRN\\n4345921 (2023).\\n[5] Jaan Altosaar. 2016. Tutorial - What is a Variational Autoencoder? https://doi.org/10.5281/zenodo.4462916\\n[6] David Amos. 2023. ChatGPT Is An Extra-Ordinary Python Programmer. https://davidamos.dev/chatgpt-is-an-extra-ordinary-python-programmer/\\n(2023).\\n[7] Ömer Aydın and Enis Karaarslan. 2022. OpenAI ChatGPT generated literature review: Digital twin in healthcare. Available at SSRN 4308687 (2022).\\n[8] Ömer Aydın and Enis Karaarslan. 2023. Is ChatGPT Leading Generative AI? What is Beyond Expectations? What is Beyond Expectations (2023).\\n[9] Amos Azaria. 2023. ChatGPT: More Human-Like Than Computer-Like, but Not Necessarily in a Good Way. (2023).\\n[10] David Baidoo-Anu and Leticia Owusu Ansah. 2023. Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential\\nBenefits of ChatGPT in Promoting Teaching and Learning. Available at SSRN 4337484 (2023).\\n[11] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. Advances in neural information processing\\nsystems 13 (2000).\\n[12] Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021. Tfix: Learning to fix coding errors with a text-to-text transformer. In\\nInternational Conference on Machine Learning. PMLR, 780–791.\\n[13] Miles Kruppa Berber Jin. 2023.\\nChatGPT Creator Is Talking to Investors About Selling Shares at $29 Billion Valuation.\\nhttps://www.wsj.com/articles/chatgpt-creator-openai-is-in-talks-for-tender-offer-that-would-value-it-at-29-billion-11672949279 (2023).\\n[14] Lea Bishop. 2023. Can ChatGPT’Think Like a Lawyer?’A Socratic Dialogue. A Socratic Dialogue (January 26, 2023) (2023).\\n[15] Back To Blog. 2023. AI and Academic Integrity: How AI Technology Might Influence the Future of Scholarly Publishing. (2023).\\n[16] Ali Borji. 2023. A Categorical Archive of ChatGPT Failures. arXiv preprint arXiv:2302.03494 (2023).\\n[17] Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In ICLR.\\n[18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems (2020).\\n[19] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott\\nLundberg, et al. 2023. Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712 (2023).\\n[20] Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li. 2022. A survey on generative diffusion model. arXiv\\npreprint arXiv:2209.02646 (2022).\\n[21] Ashley\\nCapoot.\\n2023.\\nMicrosoft\\nannounces\\nnew\\nmultibillion-dollar\\ninvestment\\nin\\nChatGPT-maker\\nOpenAI.\\nhttps://www.cnbc.com/2023/01/23/microsoft-announces-multibillion-dollar-investment-in-chatgpt-maker-openai.html (2023).\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n23\\n[22] JP Carrasco, E García, DA Sánchez, PD Estrella Porter, L De La Puente, J Navarro, and A Cerame. 2023. Is\" ChatGPT\" capable of passing the\\n2022 MIR exam? Implications of artificial intelligence in medical education in Spain¿ Es capaz “ChatGPT” de aprobar el examen MIR de 2022?\\nImplicaciones de la inteligencia artificial en la educación. (2023).\\n[23] Davide Castelvecchi. 2022. Are ChatGPT and AlphaCode going to replace programmers? Nature (2022).\\n[24] Poulomi Chatterjee. 2023. From Non-Profit to For-Profit: How OpenAI Plans to Make Money. https://analyticsindiamag.com/from-non-profit-to-for-\\nprofit-how-openai-plans-to-make-money/ (2023).\\n[25] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).\\n[26] Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin Smith, and Joshua B Tenenbaum. 2022. Are Deep Neural Networks SMARTer than Second\\nGraders? arXiv preprint arXiv:2212.09993 (2022).\\n[27] Simon Chesterman. 2023. AI-generated content is taking over the world. But who owns it? But Who Owns it (2023).\\n[28] Jonathan H Choi, Kristin E Hickman, Amy Monahan, and Daniel Schwarcz. 2023. ChatGPT Goes to Law School. Available at SSRN (2023).\\n[29] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles\\nSutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling Language Modeling with Pathways. (2022).\\n[30] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators rather than\\ngenerators. arXiv preprint arXiv:2003.10555 (2020).\\n[31] Devin Coldewey. 2019. OpenAI shifts from nonprofit to ‘capped-profit’ to attract capital. https://techcrunch.com/2019/03/11/openai-shifts-from-\\nnonprofit-to-capped-profit-to-attract-capital/ (2019).\\n[32] Debby RE Cotton, Peter A Cotton, and J Reuben Shipway. 2023. Chatting and Cheating. Ensuring academic integrity in the era of ChatGPT. (2023).\\n[33] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. 2022. Diffusion models in vision: A survey. arXiv preprint\\narXiv:2209.04747 (2022).\\n[34] Jaime A Teixeira da Silva. 2023. Is ChatGPT a valid author? Nurse Education in Practice (2023), 103600.\\n[35] Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan. 2020. A survey of multilingual neural machine translation. ACM Computing Surveys (CSUR)\\n53, 5 (2020), 1–38.\\n[36] Robert Dale. 2021. GPT-3: What’s it good for? Natural Language Engineering 27, 1 (2021), 113–118.\\n[37] Bibhu Dash and Pawankumar Sharma. 2023. Are ChatGPT and Deepfake Algorithms Endangering the Cybersecurity Industry? A Review. (2023).\\n[38] Luigi De Angelis, Francesco Baglivo, Guglielmo Arzilli, Gaetano Pierpaolo Privitera, Paolo Ferragina, Alberto Eugenio Tozzi, and Caterina Rizzo.\\n2023. ChatGPT and the Rise of Large Language Models: The New AI-Driven Infodemic Threat in Public Health. Available at SSRN 4352931 (2023).\\n[39] Ben Derico. 2023. ChatGPT bug leaked users’ conversation histories. BBC news (2023).\\n[40] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language\\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\\n[41] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language\\nunderstanding. NAACL (2019).\\n[42] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[43] Dat Duong and Benjamin D Solomon. 2023. Analysis of large-language model versus human performance for genetics questions. medRxiv (2023),\\n2023–01.\\n[44] J David Eisenberg and Amelia Bellamy-Royds. 2014. SVG essentials: Producing scalable vector graphics with XML. \" O’Reilly Media, Inc.\".\\n[45] Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. 2021. Automatic text summarization: A comprehensive survey. Expert\\nSystems with Applications 165 (2021), 113679.\\n[46] Joshua P Ellis. 2017. Tikz-feynman: Feynman diagrams with tikz. Computer Physics Communications 210 (2017), 103–123.\\n[47] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An Early Look at the Labor Market Impact Potential of\\nLarge Language Models. arXiv preprint arXiv:2303.10130 (2023).\\n[48] Ed Felten, Manav Raj, and Robert Seamans. 2023. How will Language Modelers like ChatGPT Affect Occupations and Industries? arXiv preprint\\narXiv:2303.01157 (2023).\\n[49] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and\\nJulius Berner. 2023. Mathematical capabilities of ChatGPT. arXiv preprint arXiv:2301.13867 (2023).\\n[50] Fronty. 2022. What is Open AI and What Does It Do? https://fronty.com/what-is-openai-and-what-does-it-do/ (2022).\\n[51] Zhe Gan, Ricardo Henao, David Carlson, and Lawrence Carin. 2015. Learning deep sigmoid belief networks with data augmentation. In Artificial\\nIntelligence and Statistics. PMLR, 268–276.\\n[52] Wayne Geerling, G Dirk Mateer, Jadrian Wooten, and Nikhil Damodaran. 2023. Is ChatGPT Smarter than a Student in Principles of Economics?\\nAvailable at SSRN 4356034 (2023).\\n[53] A Gilson, C Safranek, T Huang, V Socrates, L Chi, RA Taylor, and D Chartash. 2022. How does ChatGPT perform on the medical licensing exams?\\nthe implications of large language models for medical education and knowledge assessment. medRxiv (2022).\\nManuscript submitted to ACM\\n24\\nZhang et al.\\n[54] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative\\nadversarial nets. In NeurIPS.\\n[55] Roberto Gozalo-Brizuela and Eduardo C Garrido-Merchan. 2023. ChatGPT is not all you need. A State of the Art Review of large Generative AI\\nmodels. arXiv preprint arXiv:2301.04655 (2023).\\n[56] Ulf Grenander and Michael I Miller. 1994. Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B\\n(Methodological) 56, 4 (1994), 549–581.\\n[57] Joko Gunawan. 2023. Exploring the future of nursing: Insights from the ChatGPT model. Belitung Nursing Journal 9, 1 (2023), 1–5.\\n[58] Daniela Haluza and David Jungwirth. 2023. Artificial Intelligence and ten societal megatrends: a GPT-3 case study. (2023).\\n[59] Michael Haman and Milan Školník. 2023. Using ChatGPT to conduct a literature review. Accountability in Research (2023), 1–3.\\n[60] Robert Hanna. 2023. How and Why ChatGPT Failed The Turing Test. (2023).\\n[61] Stuart Hargreaves. 2023. ‘Words Are Flowing Out Like Endless Rain Into a Paper Cup’: ChatGPT & Law School Assessments. The Chinese University\\nof Hong Kong Faculty of Law Research Paper 2023-03 (2023).\\n[62] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. 2023. The political ideology of conversational AI: Converging evidence on ChatGPT’s\\npro-environmental, left-libertarian orientation. arXiv preprint arXiv:2301.01768 (2023).\\n[63] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In\\nCVPR.\\n[64] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In\\nCVPR.\\n[65] Urfa Khairatun Hisan and Muhammad Miftahul Amri. 2023. ChatGPT and Medical Education: A Double-Edged Sword. (2023).\\n[66] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi,\\nDavid J Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022).\\n[67] Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac, and Max Welling. 2022. Equivariant diffusion for molecule generation in 3d. In ICML.\\nPMLR, 8867–8887.\\n[68] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. 2019. A comprehensive survey of deep learning for image captioning.\\nACM Computing Surveys (CsUR) 51, 6 (2019), 1–36.\\n[69] Alex Hughes. 2023. ChatGPT: Everything you need to know about OpenAI’s GPT-4 tool. Science Focus (2023).\\n[70] Sun Huh. 2023. Are ChatGPT’s knowledge and interpretation ability comparable to those of medical students in Korea for taking a parasitology\\nexamination?: a descriptive study. Journal of Educational Evaluation for Health Professions 20 (2023), 1.\\n[71] Jonathan Hui. 2018. RL — Proximal Policy Optimization (PPO) Explained. https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-\\nexplained-77f014ec3f12 (2018).\\n[72] Adam Hulman, Ole Lindgaard Dollerup, Jesper Friis Mortensen, Matthew Fenech, Kasper Norman, Henrik Stoevring, and Troels Krarup Hansen.\\n2023. ChatGPT-versus human-generated answers to frequently asked questions about diabetes: a Turing test-inspired survey among employees of\\na Danish diabetes center. medRxiv (2023), 2023–02.\\n[73] Shulei Ji, Jing Luo, and Xinyu Yang. 2020. A comprehensive survey on deep music generation: Multi-level representations, algorithms, evaluations,\\nand future directions. arXiv preprint arXiv:2011.06801 (2020).\\n[74] David Jungwirth and Daniela Haluza. 2023. Forecasting Geopolitical Conflicts Using GPT-3 AI: Reali-Ty-Check One Year into the 2022 Ukraine\\nWar. (2023).\\n[75] Mihir Kale and Abhinav Rastogi. 2020. Text-to-text pre-training for data-to-text tasks. arXiv preprint arXiv:2005.10433 (2020).\\n[76] Ayoosh Kathuria. 2021. Getting Started With OpenAI Gym: The Basic Building Blocks. https://blog.paperspace.com/getting-started-with-openai-gym/\\n(2021).\\n[77] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023. GPT-4 Passes the Bar Exam. Available at SSRN 4389233\\n(2023).\\n[78] Grace Kay. 2023.\\nThe history of ChatGPT creator OpenAI, which Elon Musk helped found before parting ways and criticizing.\\nhttps://www.businessinsider.com/history-of-openai-company-chatgpt-elon-musk-founded-2022-12 (2023).\\n[79] Samantha Murphy Kelly. 2023. ChatGPT passes exams from law and business schools. CNN Business (2023).\\n[80] Mohammad Khalil and Erkan Er. 2023. Will ChatGPT get you caught? Rethinking of Plagiarism Detection. arXiv preprint arXiv:2302.04335 (2023).\\n[81] Rehan Ahmed Khan, Masood Jawaid, Aymen Rehan Khan, and Madiha Sajjad. 2023. ChatGPT-Reshaping medical education and clinical management.\\nPakistan Journal of Medical Sciences 39, 2 (2023).\\n[82] Sung Kim. 2022. Replace Grammarly Premium with OpenAI ChatGPT. https://medium.com/geekculture/replace-grammarly-premium-with-openai-\\nchatgpt-320049179c79 (2022).\\n[83] Yoon Kim, Yacine Jernite, David Sontag, and Alexander Rush. 2016. Character-aware neural language models. In Proceedings of the AAAI conference\\non artificial intelligence, Vol. 30.\\n[84] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).\\n[85] Dennis H Klatt. 1987. Review of text-to-speech conversion for English. The Journal of the Acoustical Society of America 82, 3 (1987), 737–793.\\n[86] Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura Barnes, and Donald Brown. 2019. Text classification\\nalgorithms: A survey. Information 10, 4 (2019), 150.\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n25\\n[87] Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel\\nDiaz-Candido, James Maningo, et al. 2023. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language\\nmodels. PLOS Digital Health 2, 2 (2023), e0000198.\\n[88] Boni Kutela, Kelvin Msechu, Subasish Das, and Emmanuel Kidando. 2023. Chatgpt’s Scientific Writings: A Case Study on Traffic Safety. Available\\nat SSRN 4329120 (2023).\\n[89] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised\\nlearning of language representations. arXiv preprint arXiv:1909.11942 (2019).\\n[90] Hugo Larochelle and Iain Murray. 2011. The neural autoregressive distribution estimator. In Proceedings of the fourteenth international conference\\non artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 29–37.\\n[91] Stefan Larsson and Fredrik Heintz. 2020. Transparency in artificial intelligence. Internet Policy Review 9, 2 (2020).\\n[92] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:\\nDenoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461\\n(2019).\\n[93] Jinyu Li et al. 2022. Recent advances in end-to-end automatic speech recognition. APSIPA Transactions on Signal and Information Processing 11, 1\\n(2022).\\n[94] Michael Liebrenz, Roman Schleifer, Anna Buadze, Dinesh Bhugra, and Alexander Smith. 2023. Generating scholarly content with ChatGPT: ethical\\nchallenges for medical publishing. The Lancet Digital Health 5, 3 (2023), e105–e106.\\n[95] Zhicheng Lin. 2023. Why and how to embrace AI such as ChatGPT in your academic life. (2023).\\n[96] Janna Lipenkova. 2023. Overcoming the Limitations of Large Language Models How to enhance LLMs with human-like cognitive skills. (2023).\\n[97] Alexander H Liu, Wei-Ning Hsu, Michael Auli, and Alexei Baevski. 2023. Towards end-to-end unsupervised speech recognition. In 2022 IEEE\\nSpoken Language Technology Workshop (SLT). IEEE, 221–228.\\n[98] Siru Liu, Aileen P Wright, Barron L Patterson, Jonathan P Wanderer, Robert W Turer, Scott D Nelson, Allison B McCoy, Dean F Sittig, and Adam\\nWright. 2023. Assessing the Value of ChatGPT for Clinical Decision Support Optimization. medRxiv (2023), 2023–02.\\n[99] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).\\n[100] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision\\ntransformer using shifted windows. ICCV.\\n[101] Reed Albergotti Liz Hoffman. 2023. Microsoft eyes $10 billion bet on ChatGPT. https://www.semafor.com/article/01/09/2023/microsoft-eyes-10-billion-\\nbet-on-chatgpt (2023).\\n[102] Calum Macdonald, Davies Adeloye, Aziz Sheikh, and Igor Rudan. 2023. Can ChatGPT draft a research article? An example of population-level\\nvaccine effectiveness analysis. Journal of Global Health 13 (2023).\\n[103] Rupert Macey-Dare. 2023. ChatGPT & Generative AI Systems as Quasi-Expert Legal Advice Lawyers-Case Study Considering Potential Appeal\\nAgainst Conviction of Tom Hayes. Available at SSRN 4342686 (2023).\\n[104] Nitin Madnani and Bonnie J Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics\\n36, 3 (2010), 341–387.\\n[105] Gengchen Mai, Chris Cundy, Kristy Choi, Yingjie Hu, Ni Lao, and Stefano Ermon. 2022. Towards a foundation model for geospatial artificial\\nintelligence (vision paper). In Proceedings of the 30th International Conference on Advances in Geographic Information Systems. 1–4.\\n[106] Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. 2016. Generating images from captions with attention. ICLR (2016).\\n[107] Benjamin Marchandot, Kensuke Matsushita, Adrien Carmona, Antonin Trimaille, and Olivier Morel. 2023. ChatGPT: The Next Frontier in Academic\\nWriting for Cardiologists or a Pandora’s Box of Ethical Dilemmas. European Heart Journal Open (2023), oead007.\\n[108] Mochammad Ircham Maulana. 2023. Leveraging Zoom video-conferencing features in interview data generation during the Covid-19 pandemic. In\\nResearch and Teaching in a Pandemic World: The Challenges of Establishing Academic Identities During Times of Crisis. Springer, 391–407.\\n[109] Lev Maximov. 2023. Do You Know English Grammar Better Than ChatGPT? https://medium.com/writing-cooperative/do-you-know-english-grammar-\\nbetter-than-chatgpt-8fc550f23681 (2023).\\n[110] Robert W McGee. 2023. Is Chat Gpt Biased Against Conservatives? An Empirical Study. An Empirical Study (February 15, 2023) (2023).\\n[111] Forrest McKee and David Noever. 2022. Chatbots in a Botnet World. arXiv preprint arXiv:2212.11126 (2022).\\n[112] Walaa Medhat, Ahmed Hassan, and Hoda Korashy. 2014. Sentiment analysis algorithms and applications: A survey. Ain Shams engineering journal\\n5, 4 (2014), 1093–1113.\\n[113] Ateev Mehrotra. 2023. Symptom Checkers & ChatGPT. https://scholar.harvard.edu/mehrotra/symptom-checkers (2023).\\n[114] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. 2022. Autosdf: Shape priors for 3d completion, reconstruction and\\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 306–315.\\n[115] Yasumasa Miyamoto and Kyunghyun Cho. 2016. Gated word-character recurrent language model. arXiv preprint arXiv:1606.01700 (2016).\\n[116] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. 2023. Dreamix: Video\\ndiffusion models are general video editors. arXiv preprint arXiv:2302.01329 (2023).\\n[117] Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. Wt5?! training text-to-text models to explain\\ntheir predictions. arXiv preprint arXiv:2004.14546 (2020).\\nManuscript submitted to ACM\\n26\\nZhang et al.\\n[118] Bianke Neethling. 2023. ChatGPT breaks record with 100 million users – and investors come flocking. https://dailyinvestor.com/world/8520/chatgpt-\\nbreaks-record-with-100-million-users-and-investors-come-flocking/ (2023).\\n[119] Jennimai Nguyen. 2022. No, the Google AI isn’t sentient, but it likely is racist and sexist. https://mashable.com/article/google-ai-racist-sexist-bias\\n(2022).\\n[120] David Noever and Forrest McKee. 2023. Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models. arXiv preprint\\narXiv:2301.13382 (2023).\\n[121] Siobhan O’Connor et al. 2022. Open artificial intelligence platforms in nursing education: Tools for academic progress or abuse? Nurse Education\\nin Practice 66 (2022), 103537–103537.\\n[122] OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023).\\n[123] OpenAI. 2023. Research index. https://openai.com/research (2023).\\n[124] Achraf Oussidi and Azeddine Elhassouny. 2018. Deep generative models: Survey. In 2018 International Conference on Intelligent Systems and\\nComputer Vision (ISCV). IEEE, 1–8.\\n[125] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex\\nRay, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022).\\n[126] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. 2019. Specaugment: A simple data\\naugmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779 (2019).\\n[127] John V Pavlik. 2023. Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media\\nEducation. Journalism & Mass Communication Educator (2023), 10776958221149577.\\n[128] Tammy Pettinato Oltz. 2023. ChatGPT, Professor of Law. Professor of Law (February 4, 2023) (2023).\\n[129] Oleksandra Poquet Pfeffer, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. 2023.\\nChatGPT for Good? On Opportunities and Challenges of Large Language Models for Education. (2023).\\n[130] Kelsey Piper. 2022. Why is Meta’s new AI chatbot so bad? https://www.vox.com/future-perfect/23307252/meta-facebook-bad-ai-chatbot-blenderbot\\n(2022).\\n[131] Michael Polonsky and Jeff Rotman. 2023. Should Artificial Intelligent (AI) Agents be Your Co-author? Arguments in favour, informed by ChatGPT.\\nArguments in favour, informed by ChatGPT (February 6, 2023) (2023).\\n[132] Samuel A Prieto, Eyob T Mengiste, and Borja García de Soto. 2023. Investigating the use of ChatGPT for the scheduling of construction projects.\\narXiv preprint arXiv:2302.02805 (2023).\\n[133] Junaid Qadir. 2022. Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education. (2022).\\n[134] Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. 2020. Prophetnet: Predicting future\\nn-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063 (2020).\\n[135] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale\\nweak supervision. arXiv preprint arXiv:2212.04356 (2022).\\n[136] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).\\n[137] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask\\nlearners. OpenAI blog (2019).\\n[138] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring\\nthe limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020), 1–67.\\n[139] Ric Raftis. 2023. How to use ChatGPT for Divergent Thinking in Obsidian and PKMs. (2023).\\n[140] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents.\\narXiv preprint arXiv:2204.06125 (2022).\\n[141] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image\\ngeneration. In ICML.\\n[142] Arya S Rao, John Kim, Meghana Kamineni, Michael Pang, Winston Lie, and Marc Succi. 2023. Evaluating ChatGPT as an Adjunct for Radiologic\\nDecision-Making. medRxiv (2023), 2023–02.\\n[143] Arya S Rao, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dryer, and Marc D Succi. 2023.\\nAssessing the Utility of ChatGPT Throughout the Entire Clinical Workflow. medRxiv (2023), 2023–02.\\n[144] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image\\nsynthesis. In International conference on machine learning. PMLR, 1060–1069.\\n[145] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2019. Fastspeech: Fast, robust and controllable text to speech.\\nAdvances in Neural Information Processing Systems 32 (2019).\\n[146] Jesus Rodriguez. 2022. How to Create Diagrams With ChatGPT. https://jrodthoughts.medium.com/instructgpt-is-one-of-the-models-behidn-the-magic-\\nof-chatgpt-59813dd8aabc (2022).\\n[147] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion\\nmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684–10695.\\n[148] Pericles ‘asher’ Rospigliosi. 2023. Artificial intelligence in teaching and learning: what questions should we ask of ChatGPT? , 3 pages.\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n27\\n[149] Jürgen Rudolph, Samson Tan, and Shannon Tan. 2023. ChatGPT: Bullshit spewer or the end of traditional assessments in higher education? Journal\\nof Applied Learning and Teaching 6, 1 (2023).\\n[150] run.ai. 2023. NVIDIA DGX: Under the Hood of DGX-1, DGX-2 and A100. https://www.run.ai/guides/nvidia-a100/nvidia-dgx (2023).\\n[151] Soroush Saghafian. 2023. The Analytics Science Behind ChatGPT: Human, Algorithm, or a Human-Algorithm Centaur? (2023).\\n[152] Tirthankar Ghosal Saikiran Chandha, Sucheth R. 2023. Setting the Scene: How Artificial Intelligence is reshaping how we consume and deliver\\nresearch. https://upstream.force11.org/setting-the-scene-ai/ (2023).\\n[153] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. 2017. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture\\nlikelihood and other modifications. arXiv preprint arXiv:1701.05517 (2017).\\n[154] Malik Sallam. 2023. ChatGPT Utility in Health Care Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid\\nConcerns. In Healthcare, Vol. 11. MDPI, 887.\\n[155] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019. wav2vec: Unsupervised pre-training for speech recognition. arXiv\\npreprint arXiv:1904.05862 (2019).\\n[156] Ali Shiri. 2023. ChatGPT and Academic Integrity. Information Matters 3, 2 (2023).\\n[157] Olivia Solon. 2023. The Tech Behind Those Amazing, Flawed New Chatbots. Bloomberg News (2023).\\n[158] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. Mass: Masked sequence to sequence pre-training for language generation. arXiv\\npreprint arXiv:1905.02450 (2019).\\n[159] Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients of the data distribution, Vol. 32.\\n[160] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. 2020. Sliced score matching: A scalable approach to density and score estimation. In\\nUncertainty in Artificial Intelligence. PMLR, 574–584.\\n[161] Mashrin Srivastava. 2023. A day in the life of ChatGPT as an academic reviewer: Investigating the potential of large language model for scientific\\nliterature review. (2023).\\n[162] Daniel Street and Joseph Wilck. 2023. ’Let’s Have a Chat’: Principles for the Effective Application of ChatGPT and Large Language Models in the\\nPractice of Forensic Accounting. Available at SSRN 4351817 (2023).\\n[163] Fei Sun. 2022. ChatGPT, the Start of a New Era. (2022).\\n[164] Nigar M Shafiq Surameery and Mohammed Y Shakor. 2023. Use Chat GPT to Solve Programming Bugs. International Journal of Information\\nTechnology & Computer Engineering (IJITC) ISSN: 2455-5290 3, 01 (2023), 17–22.\\n[165] Victor Tangermann. 2023.\\n89 PERCENT OF COLLEGE STUDENTS ADMIT TO USING CHATGPT FOR HOMEWORK, STUDY CLAIMS.\\nhttps://futurism.com/the-byte/students-admit-chatgpt-homework (2023).\\n[166] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. 2023. GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis. arXiv preprint\\narXiv:2301.12959 (2023).\\n[167] Paul Taylor. 2009. Text-to-speech synthesis. Cambridge university press.\\n[168] Mohamad-Hani Temsah, Amr Jamal, and Jaffar A Al-Tawfiq. 2023. Reflection with ChatGPT about the excess death after the COVID-19 pandemic.\\nNew Microbes and New Infections (2023).\\n[169] Vincent Terrasi. 2023. GPT-4: How Is It Different From GPT-3.5? https://www.searchenginejournal.com/gpt-4-vs-gpt-3-5/482463/#close (2023).\\n[170] H Holden Thorp. 2023. ChatGPT is fun, but not an author. , 313–313 pages.\\n[171] Oguzhan TOPSAKAL and Elif TOPSAKAL. 2023. Framework for A Foreign Language Teaching Software for Children Utilizing AR, Voicebots and\\nChatGPT (Large Language Models). The Journal of Cognitive Systems 7, 2 (2023), 33–38.\\n[172] ChatGPT Generative Pre-trained Transformer and Alex Zhavoronkov. 2022. Rapamycin in the context of Pascal’s Wager: generative pre-trained\\ntransformer perspective. Oncoscience 9 (2022), 82.\\n[173] Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, and Steinn Steingrimsson. 2022. Can GPT-3 write an academic paper on\\nitself, with minimal human input? (2022).\\n[174] Alan Truly. 2023. Bing Chat: how to use Microsoft’s own version of ChatGPT. https://www.digitaltrends.com/computing/how-to-use-microsoft-\\nchatgpt-bing-edge/ (2023).\\n[175] Kohei Ueda and Yuki Yamada. 2023. ChatGPT is not an author, but then, who is eligible for authorship? (2023).\\n[176] Kadir Uludag. 2023. The use of AI-supported Chatbot in Psychology. Available at SSRN 4331367 (2023).\\n[177] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. 2016. Neural autoregressive distribution estimation. The\\nJournal of Machine Learning Research 17, 1 (2016), 7184–7220.\\n[178] Benigno Uria, Iain Murray, and Hugo Larochelle. 2013. RNADE: The real-valued neural autoregressive density-estimator. Advances in Neural\\nInformation Processing Systems 26 (2013).\\n[179] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray\\nKavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio. In The 9th ISCA Speech Synthesis Workshop.\\n[180] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. 2016. Conditional image generation with pixelcnn\\ndecoders. Advances in neural information processing systems 29 (2016).\\n[181] Wouter van Heeswijk. 2022. Trust Region Policy Optimization (TRPO) Explained. https://towardsdatascience.com/trust-region-policy-optimization-\\ntrpo-explained-4b56bd206fc2 (2022).\\nManuscript submitted to ACM\\n28\\nZhang et al.\\n[182] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is\\nall you need. In NeurIPS.\\n[183] Randy Joy Magno Ventayen. 2023. OpenAI ChatGPT Generated Results: Similarity Index of Artificial Intelligence-Based Contents. Available at\\nSSRN 4332664 (2023).\\n[184] Manish Verma. 2023. Novel Study on AI-Based Chatbot (ChatGPT) Impacts on the Traditional Library Management. (2023).\\n[185] Lyan Verwimp, Joris Pelemans, Patrick Wambacq, et al. 2017. Character-word LSTM language models. arXiv preprint arXiv:1704.02813 (2017).\\n[186] Pascal Vincent. 2011. A connection between score matching and denoising autoencoders. Neural computation 23, 7 (2011), 1661–1674.\\n[187] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2016. Show and tell: Lessons learned from the 2015 mscoco image captioning\\nchallenge. IEEE transactions on pattern analysis and machine intelligence 39, 4 (2016), 652–663.\\n[188] Karan Virdi. 2022. Google issues ‘code-red’ as Open AI’s ChatGPT garners popularity. https://itmunch.com/google-issues-code-red-alert-as-open-ai-\\nbecomes-popular/ (2022).\\n[189] Vaishak V.Kumar. 2019. Soft Actor-Critic Demystified. https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665 (2019).\\n[190] Dong Wang, Xiaodong Wang, and Shaohe Lv. 2019. An overview of end-to-end automatic speech recognition. Symmetry 11, 8 (2019), 1018.\\n[191] Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon. 2023. Can ChatGPT Write a Good Boolean Query for Systematic Review\\nLiterature Search? arXiv preprint arXiv:2302.03495 (2023).\\n[192] Xinyi Wang, Zhenye Gong, Guoxin Wang, Jingdan Jia, Ying Xu, Jialu Zhao, Qingye Fan, Shaun Wu, Weiguo Hu, and Xiaoyang Li. 2023. ChatGPT\\nPerforms on the Chinese National Medical Licensing Examination. (2023).\\n[193] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt.\\n2023. A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. arXiv preprint arXiv:2302.11382 (2023).\\n[194] Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. ChatGPT Prompt Patterns for Improving Code Quality,\\nRefactoring, Requirements Elicitation, and Software Design. arXiv preprint arXiv:2303.07839 (2023).\\n[195] Clare Williams. 2023. Hype, or the future of learning and teaching? 3 Limits to AI’s ability to write student essays. (2023).\\n[196] Thomas Wischmeyer. 2020. Artificial intelligence and transparency: opening the black box. Regulating artificial intelligence (2020), 75–101.\\n[197] writecream. 2022. Can ChatGPT Correct Grammar? https://www.writecream.com/can-chatgpt-correct-grammar/ (2022).\\n[198] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. 2022. Gan inversion: A survey. IEEE Transactions on\\nPattern Analysis and Machine Intelligence (2022).\\n[199] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image\\ngeneration with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition.\\n1316–1324.\\n[200] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively\\nmultilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934 (2020).\\n[201] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. 2022. Diffusion probabilistic modeling for video generation. arXiv preprint arXiv:2203.09481\\n(2022).\\n[202] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei. 2017. Boosting image captioning with attributes. In Proceedings of the IEEE international\\nconference on computer vision. 4894–4902.\\n[203] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. 2023. A Comprehensive\\nCapability Analysis of GPT-3 and GPT-3.5 Series Models. arXiv preprint arXiv:2303.10420 (2023).\\n[204] Will Yeadon, Oto-Obong Inyang, Arin Mizouri, Alex Peach, and Craig Testrow. 2022. The Death of the Short-Form Physics Essay in the Coming AI\\nRevolution. arXiv preprint arXiv:2212.11661 (2022).\\n[205] Yee Hui Yeo, Jamil S Samaan, Wee Han Ng, Peng-Sheng Ting, Hirsh Trivedi, Aarshi Vipani, Walid Ayoub, Ju Dong Yang, Omer Liran, Brennan\\nSpiegel, et al. 2023. Assessing the performance of ChatGPT in answering questions regarding cirrhosis and hepatocellular carcinoma. medRxiv\\n(2023), 2023–02.\\n[206] Nicole Shu Ling Yeo-Teh and Bor Luen Tang. 2023. Letter to Editor: NLP systems such as ChatGPT cannot be listed as an author because these\\ncannot fulfill widely adopted authorship criteria. Accountability in Research just-accepted (2023).\\n[207] Adam Zaremba and Ender Demir. 2023. ChatGPT: Unlocking the Future of NLP in Finance. Available at SSRN 4323643 (2023).\\n[208] Ali Zarifhonarvar. 2023. Economics of ChatGPT: A Labor Market View on the Occupational Impact of Artificial Intelligence. Available at SSRN\\n4350925 (2023).\\n[209] Aeron Zentner. 2022. Applied Innovation: Artificial Intelligence in Higher Education. Available at SSRN 4314180 (2022).\\n[210] Aeron Zentner. 2022. Applied Innovation: Artificial Intelligence in Higher Education. Available at SSRN 4314180 (2022).\\n[211] Bo Zhang. 2023. Preparing Educators and Students for ChatGPT and AI Technology in Higher Education. (2023).\\n[212] Chaoning Zhang, Chenshuang Zhang, Junha Song, John Seon Keun Yi, Kang Zhang, and In So Kweon. 2022. A survey on masked autoencoder for\\nself-supervised learning in vision and beyond. arXiv preprint arXiv:2208.00173 (2022).\\n[213] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. 2023. Text-to-image Diffusion Models in Generative AI: A Survey.\\narXiv preprint arXiv:2303.07909 (2023).\\n[214] Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun,\\nLe Luang Huy, Donguk kim, Sung-Ho Bae, Lik-Hang Lee, Yang Yang, Heng Tao Shen, In So Kweon, and Choong Seon Hong. 2023. A Complete\\nManuscript submitted to ACM\\nOne Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era\\n29\\nSurvey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need? arXiv preprint arXiv:2303.11717 (2023).\\n[215] Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, and In So Kweon. 2023. A Survey on Audio\\nDiffusion Models: Text To Speech Synthesis and Enhancement in Generative AI. arXiv preprint arXiv:2303.13336 (2023).\\n[216] Chaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. 2022. Dual Temperature Helps Contrastive Learning Without\\nMany Negative Samples: Towards Understanding and Simplifying MoCo. In CVPR.\\n[217] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon. 2022. Decoupled Adversarial Contrastive\\nLearning for Self-supervised Adversarial Robustness. In ECCV. Springer, 725–742.\\n[218] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, and In So Kweon. 2022. How Does SimSiam Avoid Collapse\\nWithout Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning. In ICLR.\\n[219] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic\\nimage synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision. 5907–5915.\\n[220] Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, and Chaoning Zhang. 2023. A Survey on Graph\\nDiffusion Models: Generative AI in Science for Molecule, Protein and Material. ResearchGate 10.13140/RG.2.2.26493.64480 (2023).\\n[221] Shiliang Zhang, Ming Lei, Zhijie Yan, and Lirong Dai. 2018. Deep-FSMN for large vocabulary continuous speech recognition. In 2018 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 5869–5873.\\n[222] Guodong Troy Zhao. 2023. How to use ChatGPT in product management. (2023).\\n[223] Qitong Zhong, Xing Tan, Ruixing Du, Jiacheng Liu, Longfei Liao, Cheng Wang, Ruiyan Sun, Zhenchen Tang, Jie Ren, Chalachew Mebrahtu, et al.\\n2023. Is ChatGPT a Reliable Source for Writing Review Articles in Catalysis Research? A Case Study on CO2 Hydrogenation to Higher Alcohols.\\n(2023).\\n[224] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023. A comprehensive survey on\\npretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419 (2023).\\n[225] Chao Zhou, Cheng Qiu, and Daniel E Acuna. 2022. Paraphrase Identification with Deep Learning: A Review of Datasets and Methods. arXiv\\npreprint arXiv:2212.06933 (2022).\\n[226] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Exploring AI Ethics of ChatGPT: A Diagnostic Analysis. arXiv preprint\\narXiv:2301.12867 (2023).\\nManuscript submitted to ACM\\n')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 12. UpstageLayoutAnalysisLoader",
   "id": "cbfad2a156356939"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "UpstageLayoutAnalysisLoader 는 Upstage AI에서 제공하는 문서 분석 도구로, LangChain 프레임워크와 통합되어 사용할 수 있는 문서 로더입니다.\n",
    "\n",
    "주요 특징: - PDF, 이미지 등 다양한 형식의 문서에서 레이아웃 분석 수행 - 문서의 구조적 요소(제목, 단락, 표, 이미지 등)를 자동으로 인식 및 추출 - OCR 기능 지원 (선택적)\n",
    "\n",
    "UpstageLayoutAnalysisLoader는 단순한 텍스트 추출을 넘어 문서의 구조를 이해하고 요소 간 관계를 파악하여 보다 정확한 문서 분석을 가능하게 합니다."
   ],
   "id": "5a6334072c624f63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d012999c866b3fc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:39:02.717737Z",
     "start_time": "2025-12-13T11:39:02.713746Z"
    }
   },
   "cell_type": "code",
   "source": "# pip install -U langchain-upstage",
   "id": "b33c88c31f13cc01",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "API Key 설정\n",
    "\n",
    ".env 파일에 UPSTAGE_API_KEY 키를 설정합니다.\n",
    "\n",
    "참고\n",
    "\n",
    "Upstage 개발자 문서를 참조하세요."
   ],
   "id": "e5725a64f183a47e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 환경 설정",
   "id": "2bb0f242c4c8136c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:39:37.973260Z",
     "start_time": "2025-12-13T11:39:37.966458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ],
   "id": "3af7f3f4f9047c09",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH07-DocumentLoader\")"
   ],
   "id": "4ec8c2edaf937dbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"./data/SPRI_AI_Brief_2023년12월호_F.pdf\"\n",
    "\n",
    "# 문서 로더 설정\n",
    "loader = UpstageLayoutAnalysisLoader(\n",
    "    file_path,\n",
    "    output_type=\"text\",\n",
    "    split=\"page\",\n",
    "    use_ocr=True,\n",
    "    exclude=[\"header\", \"footer\"],\n",
    ")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 결과 출력\n",
    "for doc in docs[:3]:\n",
    "    print(doc)"
   ],
   "id": "4ed6d0bf4eec1f30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 13. LlamaParser",
   "id": "b441d9cc38c5d01c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LlamaParser\n",
    "\n",
    "LlamaParse는 LlamaIndex에서 개발한 문서 파싱 서비스로, 대규모 언어 모델(LLM)을 위해 특별히 설계되었습니다. 주요 특징은 다음과 같습니다:\n",
    "\n",
    "PDF, Word, PowerPoint, Excel 등 다양한 문서 형식 지원\n",
    "자연어 지시를 통한 맞춤형 출력 형식 제공\n",
    "복잡한 표와 이미지 추출 기능\n",
    "JSON 모드 지원\n",
    "외국어 지원\n",
    "LlamaParse는 독립형 API로 제공되며, LlamaCloud 플랫폼의 일부로도 사용 가능합니다. 이 서비스는 문서를 파싱하고 정제하여 검색 증강 생성(RAG) 등 LLM 기반 애플리케이션의 성능을 향상시키는 것을 목표로 합니다.\n",
    "\n",
    "사용자는 무료로 하루 1,000페이지를 처리할 수 있으며, 유료 플랜을 통해 추가 용량을 확보할 수 있습니다. LlamaParse는 현재 공개 베타 버전으로 제공되고 있으며, 지속적으로 기능이 확장되고 있습니다."
   ],
   "id": "ae54c586cd338135"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "API 키 설정 - API 키를 발급 후 .env 파일에 LLAMA_CLOUD_API_KEY 에 설정합니다.\n",
    "\n",
    "링크: https://cloud.llamaindex.ai"
   ],
   "id": "eb9c87f0b2f226a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:42:21.003866Z",
     "start_time": "2025-12-13T11:42:20.999828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 설치\n",
    "# !pip install llama-index-core llama-parse llama-index-readers-file python-dotenv"
   ],
   "id": "13a7682f8e55f377",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()"
   ],
   "id": "5ba8bc8dea66be4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "기본 파서 적용",
   "id": "918968f441faa722"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# 파서 설정\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",  # \"markdown\"과 \"text\" 사용 가능\n",
    "    num_workers=8,  # worker 수 (기본값: 4)\n",
    "    verbose=True,\n",
    "    language=\"ko\",\n",
    ")\n",
    "\n",
    "# SimpleDirectoryReader를 사용하여 파일 파싱\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# LlamaParse로 파일 파싱\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/SPRI_AI_Brief_2023년12월호_F.pdf\"],\n",
    "    file_extractor=file_extractor,\n",
    ").load_data()"
   ],
   "id": "9c202f3448b6e6d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 페이지 수 확인\n",
    "len(documents)"
   ],
   "id": "4e615872ff81606e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LlamaIndex -> LangChain Document 로 변환",
   "id": "d8dc29f0e5ee696d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 랭체인 도큐먼트로 변환\n",
    "docs = [doc.to_langchain_format() for doc in documents]"
   ],
   "id": "822b46af5ffe4a4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# metadata 출력\n",
    "docs[0].metadata"
   ],
   "id": "9360fa67b8d1aeed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MultiModal Model 로 파싱\n",
    "주요 파라미터\n",
    "\n",
    "use_vendor_multimodal_model: 멀티모달 모델 사용 여부를 지정합니다. True로 설정하면 외부 벤더의 멀티모달 모델을 사용합니다.\n",
    "\n",
    "vendor_multimodal_model_name: 사용할 멀티모달 모델의 이름을 지정합니다. 여기서는 \"openai-gpt4o\"를 사용하고 있습니다.\n",
    "\n",
    "vendor_multimodal_api_key: 멀티모달 모델 API 키를 지정합니다. 환경 변수에서 OpenAI API 키를 가져옵니다.\n",
    "\n",
    "result_type: 파싱 결과의 형식을 지정합니다. \"markdown\"으로 설정되어 있어 결과가 마크다운 형식으로 반환됩니다.\n",
    "\n",
    "language: 파싱할 문서의 언어를 지정합니다. \"ko\"로 설정되어 한국어로 처리됩니다.\n",
    "\n",
    "skip_diagonal_text: 대각선 텍스트를 건너뛸지 여부를 결정합니다.\n",
    "\n",
    "page_separator: 페이지 구분자를 지정할 수 있습니다."
   ],
   "id": "87c283ffc041f0f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "documents = LlamaParse(\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"openai-gpt4o\",\n",
    "    vendor_multimodal_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    result_type=\"markdown\",\n",
    "    language=\"ko\",\n",
    "    # skip_diagonal_text=True,\n",
    "    # page_separator=\"\\n=================\\n\"\n",
    ")"
   ],
   "id": "16bbf31c12f8cf6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# parsing 된 결과\n",
    "parsed_docs = documents.load_data(file_path=\"week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\")"
   ],
   "id": "1a48701830b5c7f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# langchain 도큐먼트로 변환\n",
    "docs = [doc.to_langchain_format() for doc in parsed_docs]"
   ],
   "id": "8ab0bcd78c7c5ff1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "아래와 같이 사용자 정의 인스트럭션을 지정하는 것도 가능합니다.",
   "id": "4acddac599508daf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# parsing instruction 을 지정합니다.\n",
    "parsing_instruction = (\n",
    "    \"You are parsing a brief of AI Report. Please extract tables in markdown format.\"\n",
    ")\n",
    "\n",
    "# LlamaParse 설정\n",
    "parser = LlamaParse(\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"openai-gpt4o\",\n",
    "    vendor_multimodal_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    result_type=\"markdown\",\n",
    "    language=\"ko\",\n",
    "    parsing_instruction=parsing_instruction,\n",
    ")\n",
    "\n",
    "# parsing 된 결과\n",
    "parsed_docs = parser.load_data(file_path=\"week5Data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "# langchain 도큐먼트로 변환\n",
    "docs = [doc.to_langchain_format() for doc in parsed_docs]"
   ],
   "id": "dd975c0234e57acb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# markdown 형식으로 추출된 테이블 확인\n",
    "print(docs[-2].page_content)"
   ],
   "id": "41de1f1e6736c75f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
