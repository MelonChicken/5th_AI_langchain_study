{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 01.CharacterTextSplitter",
   "id": "a08695f85225e989"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## CharacterTextSplitter\n",
    "\n",
    "이 방법은 가장 간단한 방식입니다.\n",
    "\n",
    "기본적으로 \"\\n\\n\" 을 기준으로 문자 단위로 텍스트를 분할하고, 청크의 크기를 문자 수로 측정합니다.\n",
    "\n",
    "- 텍스트 분할 방식: 단일 문자 기준\n",
    "- 청크 크기 측정 방식: 문자 수 기준"
   ],
   "id": "27bdb16ecce8adbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T09:35:45.725062Z",
     "start_time": "2025-12-27T09:35:29.299927Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install -qU langchain-text-splitters",
   "id": "a482777adb2a65b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.21 requires langchain-core<1.0.0,>=0.3.45, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain 0.3.21 requires langchain-text-splitters<1.0.0,>=0.3.7, but you have langchain-text-splitters 1.1.0 which is incompatible.\n",
      "langchain 0.3.21 requires langsmith<0.4,>=0.1.17, but you have langsmith 0.5.1 which is incompatible.\n",
      "langchain-anthropic 0.3.10 requires langchain-core<1.0.0,>=0.3.45, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-azure-ai 0.1.2 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-chroma 0.2.2 requires langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-cohere 0.4.3 requires langchain-core<0.4.0,>=0.3.27, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-community 0.3.20 requires langchain-core<1.0.0,>=0.3.45, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-community 0.3.20 requires langsmith<0.4,>=0.1.125, but you have langsmith 0.5.1 which is incompatible.\n",
      "langchain-elasticsearch 0.3.2 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-experimental 0.3.4 requires langchain-core<0.4.0,>=0.3.28, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-google-genai 2.1.0 requires langchain-core<0.4.0,>=0.3.43, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-huggingface 0.1.2 requires langchain-core<0.4.0,>=0.3.15, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-milvus 0.1.8 requires langchain-core<0.4,>=0.2.38, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-ollama 0.2.3 requires langchain-core<0.4.0,>=0.3.33, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langchain-openai 0.3.9 requires langchain-core<1.0.0,>=0.3.45, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langgraph 0.3.18 requires langchain-core<0.4,>=0.1, but you have langchain-core 1.2.5 which is incompatible.\n",
      "langgraph-prebuilt 0.1.8 requires langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43, but you have langchain-core 1.2.5 which is incompatible.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`./data/appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.\n",
    "\n",
    "읽어들인 내용을 file 변수에 저장합니다."
   ],
   "id": "ee897644f16533fe"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-27T09:32:31.324640Z",
     "start_time": "2025-12-27T09:32:31.319642Z"
    }
   },
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "파일로부터 읽은 파일의 일부 내용을 출력합니다.",
   "id": "3c5f1bd2c7bdf0a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T09:34:11.810711Z",
     "start_time": "2025-12-27T09:34:11.806836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:500])"
   ],
   "id": "bf9125b9896d4fcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "CharacterTextSplitter를 사용하여 텍스트를 청크(chunk)로 분할하는 코드입니다.\n",
    "\n",
    "- separator 매개변수로 분할할 기준을 설정합니다. 기본 값은 \"\\n\\n\" 입니다.\n",
    "- chunk_size 매개변수를 250 으로 설정하여 각 청크의 최대 크기를 250자로 제한합니다.\n",
    "- chunk_overlap 매개변수를 50으로 설정하여 인접한 청크 간에 50자의 중복을 허용합니다.\n",
    "- length_function 매개변수를 len으로 설정하여 텍스트의 길이를 계산하는 함수를 지정합니다.\n",
    "- is_separator_regex 매개변수를 False로 설정하여 separator를 정규식이 아닌 일반 문자열로 처리합니다."
   ],
   "id": "573ebf8d04f100d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T09:37:41.386447Z",
     "start_time": "2025-12-27T09:35:45.812469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    # 텍스트를 분할할 때 사용할 구분자를 지정합니다. 기본값은 \"\\n\\n\"입니다.\n",
    "    # separator=\" \",\n",
    "    # 분할된 텍스트 청크의 최대 크기를 지정합니다.\n",
    "    chunk_size=250,\n",
    "    # 분할된 텍스트 청크 간의 중복되는 문자 수를 지정합니다.\n",
    "    chunk_overlap=50,\n",
    "    # 텍스트의 길이를 계산하는 함수를 지정합니다.\n",
    "    length_function=len,\n",
    "    # 구분자가 정규식인지 여부를 지정합니다.\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ],
   "id": "9005404c9936413c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- text_splitter를 사용하여 file 텍스트를 문서 단위로 분할합니다.\n",
    "- 분할된 문서 리스트 중 첫 번째 문서(texts[0])를 출력합니다."
   ],
   "id": "4b1e8e76d8c054ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:07:34.515729Z",
     "start_time": "2025-12-27T13:07:34.331578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text_splitter를 사용하여 state_of_the_union 텍스트를 문서로 분할합니다.\n",
    "texts = text_splitter.create_documents([file])\n",
    "print(texts[0])  # 분할된 문서 중 첫 번째 문서를 출력합니다."
   ],
   "id": "4b07ac70b5fd9ff3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된\n",
      "\n",
      "결과를 반환하는 검색 방식입니다.\n",
      "\n",
      "\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.'\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:04:31.018646Z",
     "start_time": "2025-12-27T13:04:31.013760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) 실제로 \\n\\n가 몇 번 있는지\n",
    "print(\"count('\\\\n\\\\n') =\", file.count(\"\\n\\n\"))\n",
    "\n",
    "# 2) 윈도우 개행인지\n",
    "print(\"count('\\\\r\\\\n') =\", file.count(\"\\r\\n\"))\n",
    "print(\"count('\\\\r\\\\n\\\\r\\\\n') =\", file.count(\"\\r\\n\\r\\n\"))\n",
    "\n",
    "# 3) '빈 줄'처럼 보이지만 공백이 낀 패턴\n",
    "import re\n",
    "print(\"blank lines with spaces =\", len(re.findall(r\"\\n[ \\t]+\\n\", file)))\n"
   ],
   "id": "445fe91d047c3a4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count('\\n\\n') = 58\n",
      "count('\\r\\n') = 0\n",
      "count('\\r\\n\\r\\n') = 0\n",
      "blank lines with spaces = 0\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "다음은 문서와 함께 메타데이터를 전달하는 예시입니다.\n",
    "\n",
    "메타데이터가 문서와 함께 분할되는 점에 주목해 주세요.\n",
    "\n",
    "create_documents 메서드는 텍스트 데이터와 메타데이터 리스트를 인자로 받습니다."
   ],
   "id": "545628ceff23d1d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T09:41:50.984740Z",
     "start_time": "2025-12-27T09:41:50.980381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metadatas = [\n",
    "    {\"document\": 1},\n",
    "    {\"document\": 2},\n",
    "]  # 문서에 대한 메타데이터 리스트를 정의합니다.\n",
    "documents = text_splitter.create_documents(\n",
    "    [\n",
    "        file,\n",
    "        file,\n",
    "    ],  # 분할할 텍스트 데이터를 리스트로 전달합니다.\n",
    "    metadatas=metadatas,  # 각 문서에 해당하는 메타데이터를 전달합니다.\n",
    ")\n",
    "print(documents[0])  # 분할된 문서 중 첫 번째 문서를 출력합니다."
   ],
   "id": "efce91fdfb126d2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding' metadata={'document': 1}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "split_text() 메서드를 사용하여 텍스트를 분할합니다.\n",
    "\n",
    "text_splitter.split_text(file)[0]은 file 텍스트를 text_splitter를 사용하여 분할한 후, 분할된 텍스트 조각 중 첫 번째 요소를 반환합니다."
   ],
   "id": "e9c3d32a91de07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T09:42:12.894564Z",
     "start_time": "2025-12-27T09:42:12.887145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 분할하고, 분할된 텍스트의 첫 번째 요소를 반환합니다.\n",
    "text_splitter.split_text(file)[0]"
   ],
   "id": "1d9a06cfe4dedf1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 02. 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter)",
   "id": "de837518a46bf64a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "이 텍스트 분할기는 일반적인 텍스트에 권장되는 방식입니다.\n",
    "\n",
    "이 분할기는 문자 목록을 매개변수로 받아 동작합니다.\n",
    "\n",
    "분할기는 청크가 충분히 작아질 때까지 주어진 문자 목록의 순서대로 텍스트를 분할하려고 시도합니다.\n",
    "\n",
    "기본 문자 목록은 [\"\\n\\n\", \"\\n\", \" \", \"\"]입니다.\n",
    "\n",
    "- 단락 -> 문장 -> 단어 순서로 재귀적으로 분할합니다.\n",
    "\n",
    "- 이는 단락(그 다음으로 문장, 단어) 단위가 의미적으로 가장 강하게 연관된 텍스트 조각으로 간주되므로, 가능한 한 함께 유지하려는 효과가 있습니다."
   ],
   "id": "e5b56443d6c418a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 텍스트가 분할되는 방식: 문자 목록([\"\\n\\n\", \"\\n\", \" \", \"\"]) 에 의해 분할됩니다.\n",
    "\n",
    "2. 청크 크기가 측정되는 방식: 문자 수에 의해 측정됩니다."
   ],
   "id": "a1640dfd1a047f50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "appendix-keywords.txt 파일을 열어 내용을 읽어들입니다.\n",
    "\n",
    "읽어들인 내용을 file 변수에 저장합니다."
   ],
   "id": "15b0ec8858cf3f45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:06:11.361350Z",
     "start_time": "2025-12-27T10:06:11.356144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ],
   "id": "f95b773329cc8ac0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "파일로부터 읽은 파일의 일부 내용을 출력합니다.",
   "id": "2758c8d07196e14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:06:24.748707Z",
     "start_time": "2025-12-27T10:06:24.742627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:500])"
   ],
   "id": "ef9775fb9d5a991a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:07:19.638912Z",
     "start_time": "2025-12-27T10:07:19.629492Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_text_splitters import RecursiveCharacterTextSplitter",
   "id": "1cb56904528d77d2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "RecursiveCharacterTextSplitter를 사용하여 텍스트를 작은 청크로 분할하는 예제입니다.\n",
    "\n",
    "- chunk_size를 250 으로 설정하여 각 청크의 크기를 제한합니다.\n",
    "- chunk_overlap을 50 으로 설정하여 인접한 청크 간에 50 개 문자의 중첩을 허용합니다.\n",
    "- length_function으로 len 함수를 사용하여 텍스트의 길이를 계산합니다.\n",
    "- is_separator_regex를 False로 설정하여 구분자로 정규식을 사용하지 않습니다"
   ],
   "id": "1a4ce33a630f0ca0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:07:52.361059Z",
     "start_time": "2025-12-27T10:07:52.356915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 청크 크기를 매우 작게 설정합니다. 예시를 위한 설정입니다.\n",
    "    chunk_size=250,\n",
    "    # 청크 간의 중복되는 문자 수를 설정합니다.\n",
    "    chunk_overlap=50,\n",
    "    # 문자열 길이를 계산하는 함수를 지정합니다.\n",
    "    length_function=len,\n",
    "    # 구분자로 정규식을 사용할지 여부를 설정합니다.\n",
    "    is_separator_regex=False,\n",
    ")"
   ],
   "id": "803127d075bc618",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- text_splitter를 사용하여 file 텍스트를 문서 단위로 분할합니다.\n",
    "- 분할된 문서는 texts 리스트에 저장됩니다.\n",
    "- print(texts[0])과 print(texts[1])을 통해 분할된 문서의 첫 번째와 두 번째 문서를 출력합니다."
   ],
   "id": "d5b23ff12b7dd201"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:08:18.309475Z",
     "start_time": "2025-12-27T10:08:18.304120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 문서로 분할합니다.\n",
    "texts = text_splitter.create_documents([file])\n",
    "print(texts[0])  # 분할된 문서의 첫 번째 문서를 출력합니다.\n",
    "print(\"===\" * 20)\n",
    "print(texts[1])  # 분할된 문서의 두 번째 문서를 출력합니다.\n"
   ],
   "id": "22bfd73f58b970ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding'\n",
      "============================================================\n",
      "page_content='Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token'\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "text_splitter.split_text() 함수를 사용하여 file 텍스트를 분할합니다.",
   "id": "9afcdb11754d2aa2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:08:36.035153Z",
     "start_time": "2025-12-27T10:08:36.028592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 텍스트를 분할하고 분할된 텍스트의 처음 2개 요소를 반환합니다.\n",
    "text_splitter.split_text(file)[:2]"
   ],
   "id": "fb76d3505d440d69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding',\n",
       " 'Embedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CharacterTextSplitter vs. RecursiveCharacterTextSplitter",
   "id": "e3ed9bbdf0db42f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:01:52.732662Z",
     "start_time": "2025-12-27T11:01:52.725688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ],
   "id": "c057d4d66aaabaf9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:01:53.525002Z",
     "start_time": "2025-12-27T11:01:53.518565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 트래킹 용도\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"langchain-study-chapter07\")"
   ],
   "id": "490aaf49e6d77c58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "langchain-study-chapter07\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:07:10.044356Z",
     "start_time": "2025-12-27T11:07:10.012173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import os\n",
    "def show_chunks(name, chunks, length_fn=len, max_show=10):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"num_chunks = {len(chunks)}\")\n",
    "    for i, c in enumerate(chunks[:max_show], 1):\n",
    "        print(f\"[{i:02d}] len={length_fn(c):>4} | {repr(c[:80])}\" + (\"...\" if len(c) > 80 else \"\"))\n",
    "    if len(chunks) > max_show:\n",
    "        print(f\"... (showing {max_show}/{len(chunks)})\")\n",
    "\n",
    "with open(\"./chapter07Data/text_splitter_example.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()\n",
    "\n",
    "chunk_size = 120\n",
    "chunk_overlap = 20\n",
    "\n",
    "# 1) CharacterTextSplitter: separator 하나(\"\\n\\n\")만 사용\n",
    "cts = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "cts_chunks = cts.split_text(file)\n",
    "show_chunks(\"CharacterTextSplitter (separator='\\\\n\\\\n')\", cts_chunks)\n",
    "\n",
    "# 2) RecursiveCharacterTextSplitter: 여러 separator를 단계적으로 사용\n",
    "rcts = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "rcts_chunks = rcts.split_text(file)\n",
    "show_chunks(\"RecursiveCharacterTextSplitter (default separators)\", rcts_chunks)\n",
    "\n",
    "os.makedirs(\"./chapter07Data/chunks_cts\", exist_ok=True)\n",
    "os.makedirs(\"./chapter07Data/chunks_rcts\", exist_ok=True)\n",
    "\n",
    "# CharacterTextSplitter chunks 저장\n",
    "for i, c in enumerate(cts_chunks, 1):\n",
    "    with open(f\"./chapter07Data/chunks_cts/chunk_{i:03d}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(c)\n",
    "\n",
    "# RecursiveCharacterTextSplitter chunks 저장\n",
    "for i, c in enumerate(rcts_chunks, 1):\n",
    "    with open(f\"./chapter07Data/chunks_rcts/chunk_{i:03d}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(c)\n",
    "\n",
    "print(\"Saved all chunks to:\")\n",
    "print(\"./chapter07Data/chunks_cts/\")\n",
    "print(\"./chapter07Data/chunks_rcts/\")\n"
   ],
   "id": "b584c39be8db3df0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 294, which is longer than the specified 120\n",
      "Created a chunk of size 271, which is longer than the specified 120\n",
      "Created a chunk of size 1931, which is longer than the specified 120\n",
      "Created a chunk of size 250, which is longer than the specified 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CharacterTextSplitter (separator='\\n\\n') ===\n",
      "num_chunks = 6\n",
      "[01] len=  59 | 'F1 Weekend Notebook: Strategy, Tyres, and the Small Details'\n",
      "[02] len= 294 | 'In modern Formula 1, the headline is often a single lap: pole position, a daring'...\n",
      "[03] len= 271 | 'Section 1 — Why Tyre Management Looks Boring on TV\\nThe tyre is not just a grip g'...\n",
      "[04] len=1931 | 'Section 2 — Race Radio (One Long Block On Purpose)\\nENGINEER: We are boxing this '...\n",
      "[05] len= 250 | 'Section 3 — The Pit Stop as a System\\nA pit stop is a choreography of constraints'...\n",
      "[06] len= 259 | 'Section 4 — The Lap Time is a Stack of Micro-Choices\\nLift two meters earlier, ch'...\n",
      "\n",
      "=== RecursiveCharacterTextSplitter (default separators) ===\n",
      "num_chunks = 35\n",
      "[01] len=  59 | 'F1 Weekend Notebook: Strategy, Tyres, and the Small Details'\n",
      "[02] len= 109 | 'In modern Formula 1, the headline is often a single lap: pole position, a daring'...\n",
      "[03] len= 104 | 'But most races are decided in the quiet moments—when the driver saves the front '...\n",
      "[04] len=  79 | 'chooses the safer undercut, and when a team avoids one extra second in the box.'\n",
      "[05] len=  50 | 'Section 1 — Why Tyre Management Looks Boring on TV'\n",
      "[06] len= 111 | 'The tyre is not just a grip generator; it is a time budget. If you spend it too '...\n",
      "[07] len= 108 | 'survival. If you protect it too much, you never unlock pace. The best drivers ma'...\n",
      "[08] len=  50 | 'Section 2 — Race Radio (One Long Block On Purpose)'\n",
      "[09] len=  92 | 'ENGINEER: We are boxing this lap. Tyres are falling off the cliff, keep it tidy '...\n",
      "[10] len=  84 | \"DRIVER: Copy. Front left is gone. I can't rotate mid-corner and I'm sliding on e\"...\n",
      "... (showing 10/35)\n",
      "Saved all chunks to:\n",
      "./chapter07Data/chunks_cts/\n",
      "./chapter07Data/chunks_rcts/\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:05:56.004481Z",
     "start_time": "2025-12-27T11:05:55.977201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "with open(\"./chapter07Data/text_splitter_example.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chunk_size = 120\n",
    "chunk_overlap = 20\n",
    "\n",
    "cts = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "rcts = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "cts_chunks = cts.split_text(text)\n",
    "rcts_chunks = rcts.split_text(text)\n",
    "\n",
    "os.makedirs(\"./chapter07Data/chunks_cts\", exist_ok=True)\n",
    "os.makedirs(\"./chapter07Data/chunks_rcts\", exist_ok=True)\n",
    "\n",
    "# CharacterTextSplitter chunks 저장\n",
    "for i, c in enumerate(cts_chunks, 1):\n",
    "    with open(f\"./chapter07Data/chunks_cts/chunk_{i:03d}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(c)\n",
    "\n",
    "# RecursiveCharacterTextSplitter chunks 저장\n",
    "for i, c in enumerate(rcts_chunks, 1):\n",
    "    with open(f\"./chapter07Data/chunks_rcts/chunk_{i:03d}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(c)\n",
    "\n",
    "print(\"Saved all chunks to:\")\n",
    "print(\"./chapter07Data/chunks_cts/\")\n",
    "print(\"./chapter07Data/chunks_rcts/\")\n"
   ],
   "id": "1aa2ec710474d1b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 294, which is longer than the specified 120\n",
      "Created a chunk of size 271, which is longer than the specified 120\n",
      "Created a chunk of size 1931, which is longer than the specified 120\n",
      "Created a chunk of size 250, which is longer than the specified 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all chunks to:\n",
      "./chapter07Data/chunks_cts/\n",
      "./chapter07Data/chunks_rcts/\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 03. 토큰 텍스트 분할(TokenTextSplitter)",
   "id": "fb9fc8607cf13f50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `TokenTextSplitter`\n",
    "\n",
    "언어 모델에는 토큰 제한이 있습니다. 따라서 토큰 제한을 초과하지 않아야 합니다.\n",
    "\n",
    "TokenTextSplitter 는 텍스트를 토큰 수를 기반으로 청크를 생성할 때 유용합니다."
   ],
   "id": "6cf511cdafa9057e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## tiktoken\n",
    "\n",
    "tiktoken 은 OpenAI에서 만든 빠른 BPE Tokenizer 입니다.\n",
    "\n",
    "%pip install --upgrade --quiet langchain-text-splitters tiktoken\n",
    "./data/appendix-keywords.txt 파일을 열어 내용을 읽어들입니다.\n",
    "읽어들인 내용을 file 변수에 저장합니다."
   ],
   "id": "9608ce2c97f9ba26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:14:14.768111Z",
     "start_time": "2025-12-27T11:14:14.761860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "# 파일로부터 읽은 파일의 일부 내용을 출력합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:500])"
   ],
   "id": "f45cebcae61c3a9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`CharacterTextSplitter`를 사용하여 텍스트를 분할합니다.\n",
    "\n",
    "`from_tiktoken_encoder` 메서드를 사용하여 Tiktoken 인코더 기반의 텍스트 분할기를 초기화합니다."
   ],
   "id": "7386f9001fc0e5a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:16:12.030652Z",
     "start_time": "2025-12-27T11:14:58.164670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # 청크 크기를 300으로 설정합니다.\n",
    "    chunk_size=300,\n",
    "    # 청크 간 중복되는 부분이 없도록 설정합니다.\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "# file 텍스트를 청크 단위로 분할합니다.\n",
    "texts = text_splitter.split_text(file)"
   ],
   "id": "b48bc2dafd3a5404",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 305, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n",
      "Created a chunk of size 335, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 336, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 337, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 354, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 381, which is longer than the specified 300\n",
      "Created a chunk of size 365, which is longer than the specified 300\n",
      "Created a chunk of size 377, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "분할된 청크의 개수를 출력합니다.",
   "id": "853915aec52be9a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:16:12.058475Z",
     "start_time": "2025-12-27T11:16:12.054350Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(texts))  # 분할된 청크의 개수를 출력합니다.",
   "id": "5e5f412a4cfa92c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "texts 리스트의 첫 번째 요소를 출력합니다.",
   "id": "e0f017b399c87f84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:16:12.082010Z",
     "start_time": "2025-12-27T11:16:12.076977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# texts 리스트의 첫 번째 요소를 출력합니다.\n",
    "print(texts[0])"
   ],
   "id": "41a3eaff274fd7bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "참고\n",
    "\n",
    "CharacterTextSplitter.from_tiktoken_encoder를 사용하는 경우, 텍스트는 CharacterTextSplitter에 의해서만 분할되고 tiktoken 토크나이저는 분할된 텍스트를 병합하는 데 사용됩니다. (이는 분할된 텍스트가 tiktoken 토크나이저로 측정한 청크 크기보다 클 수 있음을 의미합니다.)\n",
    "\n",
    "\n",
    "RecursiveCharacterTextSplitter.from_tiktoken_encoder를 사용하면 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할됩니다. 또한 tiktoken 분할기를 직접 로드할 수 있으며, 이는 각 분할이 청크 크기보다 작음을 보장합니다."
   ],
   "id": "e5189e55ec74528e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TokenTextSplitter\n",
    "TokenTextSplitter 클래스를 사용하여 텍스트를 토큰 단위로 분할합니다."
   ],
   "id": "37c3bf685931d640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:17:24.546860Z",
     "start_time": "2025-12-27T11:17:24.540558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=200,  # 청크 크기를 10으로 설정합니다.\n",
    "    chunk_overlap=0,  # 청크 간 중복을 0으로 설정합니다.\n",
    ")\n",
    "\n",
    "# state_of_the_union 텍스트를 청크로 분할합니다.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # 분할된 텍스트의 첫 번째 청크를 출력합니다."
   ],
   "id": "9f1abb6863e88a15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"�\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## spaCy\n",
    "spaCy는 Python과 Cython 프로그래밍 언어로 작성된 고급 자연어 처리를 위한 오픈 소스 소프트웨어 라이브러리입니다.\n",
    "\n",
    "NLTK의 또 다른 대안은 spaCy tokenizer를 사용하는 것입니다.\n",
    "\n",
    "텍스트가 분할되는 방식: spaCy tokenizer에 의해 분할됩니다.\n",
    "\n",
    "chunk size가 측정되는 방법: 문자 수로 측정됩니다.\n",
    "\n",
    "spaCy 라이브러리를 최신 버전으로 업그레이드하는 pip 명령어입니다."
   ],
   "id": "ab15070c8d57fdd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:23:38.716896Z",
     "start_time": "2025-12-27T11:23:36.355229Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install --upgrade --quiet spacy",
   "id": "43ab25df9ab730a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "en_core_web_sm 모델을 다운로드합니다.",
   "id": "9036204e6d34a60c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:23:47.697176Z",
     "start_time": "2025-12-27T11:23:38.737934Z"
    }
   },
   "cell_type": "code",
   "source": "!python -m spacy download en_core_web_sm --quiet",
   "id": "f171ed65a92f8bdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "appendix-keywords.txt 파일을 열어 내용을 읽어들입니다.",
   "id": "6d02ebc2127a5831"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:26:02.037109Z",
     "start_time": "2025-12-27T12:26:02.020446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ],
   "id": "8b2bfcfa9803fbd2",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "일부 내용을 출력하여 확인합니다.",
   "id": "989822f2478eb1bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:26:03.321907Z",
     "start_time": "2025-12-27T12:26:03.317430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:350])"
   ],
   "id": "6f7e9ecc3c13e10a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- `SpacyTextSplitter` 클래스를 사용하여 텍스트 분할기를 생성합니다.",
   "id": "ee8ef9404c04f0da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:27:24.242059Z",
     "start_time": "2025-12-27T12:27:23.982085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "# 경고 메시지를 무시합니다.\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# SpacyTextSplitter를 생성합니다.\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=200,  # 청크 크기를 200으로 설정합니다.\n",
    "    chunk_overlap=50,  # 청크 간 중복을 50으로 설정합니다.\n",
    ")"
   ],
   "id": "6ac3041fc44bbe7b",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "text_splitter 객체의 split_text 메서드를 사용하여 file 텍스트를 분할합니다.",
   "id": "5bc79d011fed6234"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:27:26.243585Z",
     "start_time": "2025-12-27T12:27:26.085497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 분할합니다.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # 분할된 텍스트의 첫 번째 요소를 출력합니다."
   ],
   "id": "2daa1c9aacc9ecc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된\n",
      "\n",
      "결과를 반환하는 검색 방식입니다.\n",
      "\n",
      "\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SentenceTransformers",
   "id": "211e178a45205407"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "SentenceTransformersTokenTextSplitter는 sentence-transformer 모델에 특화된 텍스트 분할기입니다.\n",
    "\n",
    "기본 동작은 사용하고자 하는 sentence transformer 모델의 토큰 윈도우에 맞게 텍스트를 청크로 분할하는 것입니다"
   ],
   "id": "88e3b90af205d9ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:25:05.411388Z",
     "start_time": "2025-12-27T11:24:35.069963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# 문장 분할기를 생성하고 청크 간 중복을 0으로 설정합니다.\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=0)"
   ],
   "id": "3627c37c424cd66b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41817581feb7440bb466e1b580bd8d27"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c811ed4dd03842118406134f931a38ec"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85e8cddbd58c452488829f6be4e29aa9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f5229153db24ab4b37bc771c6cb1f56"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a673eecea66047908249d00936bb69be"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d572c4360e084478a7e1f3cdc94d2316"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca3ebbd941b046d7b1733d0d6cb11a61"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e2a359d08954c71ac38e113d0a180a5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a32e044744b401794632f50a577f73f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2aa528aaabd4ae187d31ed030703ed9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "671d82aa22a54fb3a400a3c63defe2a4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "샘플 텍스트를 확인합니다.",
   "id": "f2c8cc478cad6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:25:18.482104Z",
     "start_time": "2025-12-27T11:25:18.476032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:350])"
   ],
   "id": "857394ccd034e2a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "다음은 file 변수에 담긴 텍스트의 토큰의 개수를 세는 코드입니다. 시작과 종료 토큰의 개수를 제외한 후 출력합니다.",
   "id": "a93875bfc711a19f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:26:01.018132Z",
     "start_time": "2025-12-27T11:26:00.994363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_start_and_stop_tokens = 2  # 시작과 종료 토큰의 개수를 2로 설정합니다.\n",
    "\n",
    "# 텍스트의 토큰 개수에서 시작과 종료 토큰의 개수를 뺍니다.\n",
    "text_token_count = splitter.count_tokens(\n",
    "    text=file) - count_start_and_stop_tokens\n",
    "print(text_token_count)  # 계산된 텍스트 토큰 개수를 출력합니다."
   ],
   "id": "905b50216b271114",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7686\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "splitter.split_text() 함수를 사용하여 text_to_split 변수에 저장된 텍스트를 청크(chunk) 단위로 분할합니다.",
   "id": "327d933618944b7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:26:21.085789Z",
     "start_time": "2025-12-27T11:26:21.038385Z"
    }
   },
   "cell_type": "code",
   "source": "text_chunks = splitter.split_text(text=file)  # 텍스트를 청크로 분할합니다.",
   "id": "3e87eeb6bcee2981",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "첫 번째 청크를 출력하여 내용을 확인합니다.",
   "id": "d0039aa75ce95426"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:26:34.702052Z",
     "start_time": "2025-12-27T11:26:34.695446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0번째 청크를 출력합니다.\n",
    "print(text_chunks[1])  # 분할된 텍스트 청크 중 두 번째 청크를 출력합니다."
   ],
   "id": "1875f421c4d0f15a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 [UNK] 합니다. [UNK] : \" 사과 \" 라는 단어를 [ 0. 65, - 0. 23, 0. 17 ] 과 [UNK] 벡터로 표현합니다. 연관키워드 : 자연어 처리, 벡터화, 딥러닝 token 정의 : 토큰은 텍스트를 더 작은 [UNK] 분할하는 [UNK] 의미합니다. 이는 일반적으로 단어, 문장, [UNK] 구절일 수 [UNK]. [UNK] : 문장 \" 나는 학교에 간다 \" 를 \" 나는 \", \" 학교에 \", \" 간다 \" 로 분할합니다. 연관키워드 : 토큰화, 자연어 처리, 구문 분석 tokenizer 정의 : 토크\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## NLTK\n",
    "\n",
    "Natural Language Toolkit (NLTK)은 Python 프로그래밍 언어로 작성된 영어 자연어 처리(NLP)를 위한 라이브러리와 프로그램 모음입니다.\n",
    "\n",
    "단순히 \"\\n\\n\"으로 분할하는 대신, NLTK tokenizers를 기반으로 텍스트를 분할하는 데 NLTK를 사용할 수 있습니다.\n",
    "\n",
    "1. 텍스트 분할 방법: NLTK tokenizer에 의해 분할됩니다.\n",
    "2. chunk 크기 측정 방법: 문자 수에 의해 측정됩니다.\n",
    "3. nltk 라이브러리를 설치하는 pip 명령어입니다.\n",
    "4. NLTK(Natural Language Toolkit)는 자연어 처리를 위한 파이썬 라이브러리입니다.\n",
    "5. 텍스트 데이터의 전처리, 토큰화, 형태소 분석, 품사 태깅 등 다양한 NLP 작업을 수행할 수 있습니다."
   ],
   "id": "707b1198e2352ba0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:33:08.550998Z",
     "start_time": "2025-12-27T11:33:04.815519Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install -qU nltk",
   "id": "23764f22accdafa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "샘플 텍스트를 확인합니다.",
   "id": "74bd530f84bf2613"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:33:40.802092Z",
     "start_time": "2025-12-27T11:33:40.797496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:350])"
   ],
   "id": "7d3a7e09bf011910",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "NLTKTextSplitter 클래스를 사용하여 텍스트 분할기를 생성합니다.\n",
    "\n",
    "chunk_size 매개변수를 1000으로 설정하여 텍스트를 최대 1000자 단위로 분할하도록 지정합니다."
   ],
   "id": "b04e02f9d45149a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:33:54.164592Z",
     "start_time": "2025-12-27T11:33:54.157003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=200,  # 청크 크기를 200으로 설정합니다.\n",
    "    chunk_overlap=0,  # 청크 간 중복을 0으로 설정합니다.\n",
    ")"
   ],
   "id": "3dc88778c5d934ed",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "text_splitter 객체의 split_text 메서드를 사용하여 file 텍스트를 분할합니다.",
   "id": "938f1e31ea13dee6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:52.846873Z",
     "start_time": "2025-12-27T11:34:52.768248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 분할합니다.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # 분할된 텍스트의 첫 번째 요소를 출력합니다."
   ],
   "id": "1a23619f37de03d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## KoNLPy",
   "id": "883bd649bf88b158"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "KoNLPy(Korean NLP in Python)는 한국어 자연어 처리(NLP)를 위한 파이썬 패키지입니다.\n",
    "\n",
    "토큰 분할은 텍스트를 토큰이라고 하는 더 작고 관리하기 쉬운 단위로 분할하는 과정을 포함합니다.\n",
    "\n",
    "이러한 토큰은 종종 단어, 구, 기호 또는 추가 처리 및 분석에 중요한 다른 의미 있는 요소입니다.\n",
    "\n",
    "영어와 같은 언어에서 토큰 분할은 일반적으로 공백과 구두점으로 단어를 분리하는 것을 포함합니다.\n",
    "\n",
    "토큰 분할의 효과는 언어 구조에 대한 토크나이저의 이해에 크게 의존하며, 이는 의미 있는 토큰 생성을 보장합니다.\n",
    "\n",
    "영어를 위해 설계된 토크나이저는 한국어와 같은 다른 언어의 고유한 의미 구조를 이해할 수 있는 능력이 없기 때문에 한국어 처리에 효과적으로 사용될 수 없습니다."
   ],
   "id": "ba6f4ab280b468c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "KoNLPy의 Kkma 분석기를 사용한 한국어 토큰 분할\n",
    "\n",
    "한국어 텍스트의 경우 KoNLPY에는 Kkma(Korean Knowledge Morpheme Analyzer)라는 형태소 분석기가 포함되어 있습니다.\n",
    "\n",
    "Kkma는 한국어 텍스트에 대한 상세한 형태소 분석을 제공합니다.\n",
    "\n",
    "문장을 단어로, 단어를 각각의 형태소로 분해하고 각 토큰에 대한 품사를 식별합니다.\n",
    "\n",
    "텍스트 블록을 개별 문장으로 분할할 수 있어 긴 텍스트 처리에 특히 유용합니다."
   ],
   "id": "ce1a0c1f89f2368f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "사용시 고려사항\n",
    "Kkma는 상세한 분석으로 유명하지만, 이러한 정밀성이 처리 속도에 영향을 미칠 수 있다는 점에 유의해야 합니다. 따라서 Kkma는 신속한 텍스트 처리보다 분석적 깊이가 우선시되는 애플리케이션에 가장 적합합니다.\n",
    "\n",
    "- KoNLPy 라이브러리를 설치하는 pip 명령어입니다.\n",
    "- KoNLPy는 한국어 자연어 처리를 위한 파이썬 패키지로, 형태소 분석, 품사 태깅, 구문 분석 등의 기능을 제공합니다."
   ],
   "id": "2714b872c46c569d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4f85b621bfb92c4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:35:58.417480Z",
     "start_time": "2025-12-27T11:35:54.032099Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install -qU konlpy",
   "id": "190a3528910cfd58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "샘플 텍스트를 확인합니다.",
   "id": "d4f7b86753bedaf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:36:19.340898Z",
     "start_time": "2025-12-27T11:36:19.335744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:350])\n"
   ],
   "id": "cb388fc1de39590f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "KonlpyTextSplitter를 사용하여 한국어 텍스트를 분할하는 예제입니다.",
   "id": "4e23911c8fa6dc8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:36:36.187533Z",
     "start_time": "2025-12-27T11:36:32.940264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "# KonlpyTextSplitter를 사용하여 텍스트 분할기 객체를 생성합니다.\n",
    "text_splitter = KonlpyTextSplitter()"
   ],
   "id": "d484ac31efbcbc88",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "text_splitter를 사용하여 file를 문장 단위로 분할합니다.",
   "id": "7584e2772a969ada"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:36:49.421471Z",
     "start_time": "2025-12-27T11:36:42.103810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts = text_splitter.split_text(file)  # 한국어 문서를 문장 단위로 분할합니다.\n",
    "print(texts[0])  # 분할된 문장 중 첫 번째 문장을 출력합니다."
   ],
   "id": "428df36f16b9ecb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "\n",
      "예시: 사용자가 \" 태양계 행성\" 이라고 검색하면, \" 목성\", \" 화 성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝 Embedding 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다.\n",
      "\n",
      "이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "\n",
      "예시: \" 사과\" 라는 단어를 [0.65, -0.23, 0.17] 과 같은 벡터로 표현합니다.\n",
      "\n",
      "연관 키워드: 자연어 처리, 벡터화, 딥 러닝 Token 정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다.\n",
      "\n",
      "이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "\n",
      "예시: 문장 \" 나는 학교에 간다 \"를 \" 나는\", \" 학교에\", \" 간다\" 로 분할합니다.\n",
      "\n",
      "연관 키워드: 토큰 화, 자연어 처리, 구 문 분석 Tokenizer 정의: 토크 나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다.\n",
      "\n",
      "이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "\n",
      "예시: \"I love programming.\" 이라는 문장을 [ \"I\", \"love\", \"programming\", \".\" ]으로 분할합니다.\n",
      "\n",
      "연관 키워드: 토큰 화, 자연어 처리, 구 문 분석 VectorStore 정의: 벡터 스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템 입니\n",
      "\n",
      "다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "\n",
      "예시: 단어 임 베 딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\n",
      "\n",
      "연관 키워드: 임 베 딩, 데이터베이스, 벡터화 SQL 정의: SQL(Structured Query Language) 은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다.\n",
      "\n",
      "데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "\n",
      "예시: SELECT * FROM users WHERE age > 18; 은 18세 이상의 사용자 정보를 조회합니다.\n",
      "\n",
      "연관 키워드: 데이터베이스, 쿼 리, 데이터 관리 CSV 정의: CSV(Comma-Separated Values) 는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다.\n",
      "\n",
      "표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n",
      "\n",
      "예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\n",
      "\n",
      "연관 키워드: 데이터 형식, 파일 처리, 데이터 교환 JSON 정의: JSON(JavaScript Object Notation) 은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\n",
      "\n",
      "예시: {\" 이름\": \" 홍길동\", \" 나이\": 30, \" 직업\": \" 개발자\"} 는 JSON 형식의 데이터 입니\n",
      "\n",
      "다. 연관 키워드: 데이터 교환, 웹 개발, API Transformer 정의: 트랜스포머는 자연어 처리에서 사용되는 딥 러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다.\n",
      "\n",
      "이는 Attention 메커니즘을 기반으로 합니다.\n",
      "\n",
      "예시: 구 글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\n",
      "\n",
      "연관 키워드: 딥 러닝, 자연어 처리, Attention HuggingFace 정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다.\n",
      "\n",
      "이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\n",
      "\n",
      "예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\n",
      "\n",
      "연관 키워드: 자연어 처리, 딥 러닝, 라이브러리 Digital Transformation 정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다.\n",
      "\n",
      "이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n",
      "\n",
      "예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n",
      "\n",
      "연관 키워드: 혁신, 기술, 비즈니스 모델 Crawling 정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다.\n",
      "\n",
      "이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "\n",
      "예시: 구 글 검색 엔진이 인터넷 상의 웹사이트를 방문 하여 콘텐츠를 수집하고 인 덱싱하는 것이 크롤링입니다.\n",
      "\n",
      "연관 키워드: 데이터 수집, 웹 스크 래핑, 검색 엔진 Word2Vec 정의: Word2Vec 은 단어를 벡터 공간에 매 핑 하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다.\n",
      "\n",
      "이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "\n",
      "예시: Word2Vec 모델에서 \" 왕\" 과 \" 여 왕\" 은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "\n",
      "연관 키워드: 자연어 처리, 임 베 딩, 의미론적 유사성 LLM (Large Language Model) 정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다.\n",
      "\n",
      "이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\n",
      "\n",
      "예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\n",
      "\n",
      "연관 키워드: 자연어 처리, 딥 러닝, 텍스트 생성 FAISS (Facebook AI Similarity Search) 정의: FAISS는 페이스 북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "\n",
      "연관 키워드: 벡터 검색, 머신 러닝, 데이터베이스 최적화 Open Source 정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다.\n",
      "\n",
      "이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\n",
      "\n",
      "예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\n",
      "\n",
      "연관 키워드: 소프트웨어 개발, 커뮤니티, 기술 협업 Structured Data 정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다.\n",
      "\n",
      "이는 데이터베이스, 스프레드 시트 등에서 쉽게 검색하고 분석할 수 있습니다.\n",
      "\n",
      "예시: 관계 형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\n",
      "\n",
      "연관 키워드: 데이터베이스, 데이터 분석, 데이터 모델링 Parser 정의: 파서는 주어진 데이터( 문자열, 파일 등 )를 분석하여 구조화된 형태로 변환하는 도구입니다.\n",
      "\n",
      "이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\n",
      "\n",
      "예시: HTML 문서를 구 문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\n",
      "\n",
      "연관 키워드: 구 문 분석, 컴파일러, 데이터 처리 TF-IDF (Term Frequency-Inverse Document Frequency) 정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다.\n",
      "\n",
      "이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\n",
      "\n",
      "예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\n",
      "\n",
      "연관 키워드: 자연어 처리, 정보 검색, 데이터 마이닝 Deep Learning 정의: 딥 러닝은 인공 신경망을 이용하여 복잡한 문제를 해결하는 머신 러닝의 한 분야입니다.\n",
      "\n",
      "이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\n",
      "\n",
      "예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥 러닝 모델이 활용됩니다.\n",
      "\n",
      "연관 키워드: 인공 신경망, 머신 러닝, 데이터 분석 Schema 정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는 지에 대한 청사진을 제공합니다.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hugging Face tokenizer",
   "id": "61c89a08e5e3b862"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hugging Face는 다양한 토크나이저를 제공합니다.\n",
    "\n",
    "이 코드에서는 Hugging Face의 토크나이저 중 하나인 GPT2TokenizerFast를 사용하여 텍스트의 토큰 길이를 계산합니다.\n",
    "\n",
    "텍스트 분할 방식은 다음과 같습니다:\n",
    "\n",
    "전달된 문자 단위로 분할됩니다.\n",
    "청크 크기 측정 방식은 다음과 같습니다:\n",
    "\n",
    "Hugging Face 토크나이저에 의해 계산된 토큰 수를 기준으로 합니다.\n",
    "\n",
    "- GPT2TokenizerFast 클래스를 사용하여 tokenizer 객체를 생성합니다.\n",
    "\n",
    "- from_pretrained 메서드를 호출하여 사전 학습된 \"gpt2\" 토크나이저 모델을 로드합니다."
   ],
   "id": "cbb6e65bd9d9cc15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:37:18.863652Z",
     "start_time": "2025-12-27T11:37:13.762434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# GPT-2 모델의 토크나이저를 불러옵니다.\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ],
   "id": "ecfa963f51702069",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec4f3de4e3b947a692755bdb00e0c034"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6361f128c3c04fe1bdca46824886ad99"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd13d3eea88f48a58e6b7d32e9ce97c7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be8a86d26f184c3abe23fa28f46f4821"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1db93c87bfe4dcebefb85c2cd0551b2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "샘플 텍스트를 확인합니다.",
   "id": "f79d3d6f94054518"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:37:39.637797Z",
     "start_time": "2025-12-27T11:37:39.633025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:350])"
   ],
   "id": "47e8f595d7dfb621",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "from_huggingface_tokenizer 메서드를 통해 허깅페이스 토크나이저(tokenizer)를 사용하여 텍스트 분할기를 초기화합니다.",
   "id": "543461bee65a3810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:38:27.766469Z",
     "start_time": "2025-12-27T11:38:27.671057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    # 허깅페이스 토크나이저를 사용하여 CharacterTextSplitter 객체를 생성합니다.\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "# state_of_the_union 텍스트를 분할하여 texts 변수에 저장합니다.\n",
    "texts = text_splitter.split_text(file)"
   ],
   "id": "246544c1d0d8232c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 305, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n",
      "Created a chunk of size 335, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 336, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 337, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 354, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 381, which is longer than the specified 300\n",
      "Created a chunk of size 365, which is longer than the specified 300\n",
      "Created a chunk of size 377, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:38:41.709847Z",
     "start_time": "2025-12-27T11:38:41.705429Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[1])  # texts 리스트의 1 번째 요소를 출력합니다.",
   "id": "729032470ab9522b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aaba75bc3780e639"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 04. 시멘틱 청커(SemanticChunker)",
   "id": "b0d7f5dc90f79ac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "SemanticChunker\n",
    "\n",
    "텍스트를 의미론적 유사성에 기반하여 분할합니다.\n",
    "\n",
    "Reference\n",
    "\n",
    "- [Greg Kamradt의 노트북](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)"
   ],
   "id": "ab544188adb678c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "이 방법은 텍스트를 문장 단위로 분할한 후, 3개의 문장씩 그룹화하고, 임베딩 공간에서 유사한 문장들을 병합하는 과정을 거칩니다.\n",
    "\n",
    "의존성 패키지 설치"
   ],
   "id": "60d2c4874a543305"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain_experimental langchain_openai",
   "id": "1d86786c451a3501"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "샘플 텍스트를 로드하고 내용을 출력합니다.",
   "id": "66f877cdf293c876"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:16:50.004196Z",
     "start_time": "2025-12-27T13:16:49.998921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./chapter07Data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:350])\n"
   ],
   "id": "489aaf34064ecb16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SemanticChunker 생성\n",
    "\n",
    "SemanticChunker는 LangChain의 실험적 기능 중 하나로, 텍스트를 의미론적으로 유사한 청크로 분할하는 역할을 합니다.\n",
    "\n",
    "이를 통해 텍스트 데이터를 보다 효과적으로 처리하고 분석할 수 있습니다."
   ],
   "id": "afd312aa9fb5a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:09.910945Z",
     "start_time": "2025-12-27T13:17:09.875652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ],
   "id": "383f22730077a182",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:46.895040Z",
     "start_time": "2025-12-27T13:17:46.886010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 트래킹 용도\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"langchain-study-chapter07\")"
   ],
   "id": "6eb4bf660b5e1375",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "langchain-study-chapter07\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:17:58.567295Z",
     "start_time": "2025-12-27T13:17:54.216978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ],
   "id": "6cca1f9a8d4760f9",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 텍스트 분할\n",
    "\n",
    "text_splitter를 사용하여 file 텍스트를 문서 단위로 분할합니다."
   ],
   "id": "ed99c42bc5edeef1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:18:30.013438Z",
     "start_time": "2025-12-27T13:18:24.336903Z"
    }
   },
   "cell_type": "code",
   "source": "chunks = text_splitter.split_text(file)",
   "id": "ed5c1fb68b524e5",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "분할된 청크를 확인합니다.",
   "id": "a8d82f6f6747a208"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:18:36.649902Z",
     "start_time": "2025-12-27T13:18:36.645717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 분할된 청크 중 첫 번째 청크를 출력합니다.\n",
    "print(chunks[0])"
   ],
   "id": "dcd54fb61d8e3c52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다. 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다. 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n",
      "\n",
      "JSON\n",
      "\n",
      "정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API\n",
      "\n",
      "Transformer\n",
      "\n",
      "정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다. 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다. 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리\n",
      "\n",
      "Digital Transformation\n",
      "\n",
      "정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다. 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다. 연관키워드: 혁신, 기술, 비즈니스 모델\n",
      "\n",
      "Crawling\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다. 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다. 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "create_documents() 함수를 사용하여 청크를 문서로 변환할 수 있습니다.",
   "id": "8b0aa76a7b16900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:18:55.750430Z",
     "start_time": "2025-12-27T13:18:53.954815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text_splitter를 사용하여 분할합니다.\n",
    "docs = text_splitter.create_documents([file])\n",
    "print(docs[0].page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다."
   ],
   "id": "35507cb0b98f05ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다. 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다. 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n",
      "\n",
      "JSON\n",
      "\n",
      "정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API\n",
      "\n",
      "Transformer\n",
      "\n",
      "정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다. 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다. 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리\n",
      "\n",
      "Digital Transformation\n",
      "\n",
      "정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다. 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다. 연관키워드: 혁신, 기술, 비즈니스 모델\n",
      "\n",
      "Crawling\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다. 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다. 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Breakpoints\n",
    "\n",
    "이 chunker는 문장을 \"분리\"할 시점을 결정하여 작동합니다. 이는 두 문장 간의 임베딩 차이를 살펴봄으로써 이루어집니다.\n",
    "\n",
    "그 차이가 특정 임계값을 넘으면 문장이 분리됩니다.\n",
    "\n",
    "참고 영상: https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580"
   ],
   "id": "19c94e8ed5c6189e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Percentile\n",
    "\n",
    "기본적인 분리 방식은 백분위수(Percentile) 를 기반으로 합니다.\n",
    "\n",
    "이 방법에서는 문장 간의 모든 차이를 계산한 다음, 지정한 백분위수를 기준으로 분리합니다."
   ],
   "id": "f5608464d4f2bb3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:19:32.017618Z",
     "start_time": "2025-12-27T13:19:31.151476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준점 유형을 백분위수로 설정합니다.\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=70,\n",
    ")"
   ],
   "id": "c38ea0a7132fbb7b",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "분할된 결과를 확인합니다.",
   "id": "523f0416c04ccc36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:19:45.064876Z",
     "start_time": "2025-12-27T13:19:42.925616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = text_splitter.create_documents([file])\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n",
    "    print(\"===\" * 20)"
   ],
   "id": "1556baa5465773b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "docs의 길이를 출력합니다.",
   "id": "8be4a1b2dcacdc80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:22:24.122912Z",
     "start_time": "2025-12-27T13:22:24.118706Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(docs))  # docs의 길이를 출력합니다.",
   "id": "7a98b6bf48156c16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Standard Deviation\n",
    "\n",
    "이 방법에서는 지정한 breakpoint_threshold_amount 표준편차보다 큰 차이가 있는 경우 분할됩니다.\n",
    "\n",
    "breakpoint_threshold_type 매개변수를 \"standard_deviation\"으로 설정하여 청크 분할 기준을 표준편차 기반으로 지정합니다."
   ],
   "id": "243f40463bbd7008"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:22:50.042567Z",
     "start_time": "2025-12-27T13:22:49.295503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준으로 표준 편차를 사용합니다.\n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=1.25,\n",
    ")"
   ],
   "id": "5ec5765979f6f880",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "분할된 결과를 확인합니다.",
   "id": "85fc885d53ded817"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:23:08.674342Z",
     "start_time": "2025-12-27T13:23:04.688831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text_splitter를 사용하여 분할합니다.\n",
    "docs = text_splitter.create_documents([file])\n",
    "docs = text_splitter.create_documents([file])\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n",
    "    print(\"===\" * 20)\n"
   ],
   "id": "7b8c63cd5b0efcb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n",
      "\n",
      "JSON\n",
      "\n",
      "정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API\n",
      "\n",
      "Transformer\n",
      "\n",
      "정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리\n",
      "\n",
      "Digital Transformation\n",
      "\n",
      "정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "docs의 길이를 출력합니다.\n",
   "id": "28fbf1fdc191ddd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:23:33.142033Z",
     "start_time": "2025-12-27T13:23:33.137673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(len(docs))  # docs의 길이를 출력합니다."
   ],
   "id": "bc6c0138901f01a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interquartile\n",
    "\n",
    "이 방법에서는 사분위수 범위(interquartile range)를 사용하여 청크를 분할합니다.\n",
    "\n",
    "breakpoint_threshold_type 매개변수를 \"interquartile\"로 설정하여 청크 분할 기준을 사분위수 범위로 지정합니다."
   ],
   "id": "b659519b732ec8ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:23:56.292903Z",
     "start_time": "2025-12-27T13:23:54.692363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 의미론적 청크 분할기를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준점 임계값 유형을 사분위수 범위로 설정합니다.\n",
    "    breakpoint_threshold_type=\"interquartile\",\n",
    "    breakpoint_threshold_amount=0.5,\n",
    ")"
   ],
   "id": "18804f0aa0859666",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:24:01.111196Z",
     "start_time": "2025-12-27T13:23:59.337559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text_splitter를 사용하여 분할합니다.\n",
    "docs = text_splitter.create_documents([file])\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n",
    "    print(\"===\" * 20)"
   ],
   "id": "28ce8f05ae1fa485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:24:05.218837Z",
     "start_time": "2025-12-27T13:24:05.214338Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(docs))  # docs의 길이를 출력합니다.",
   "id": "72225d1a4c2721f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등)",
   "id": "4afd6aabd18f1c6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split code\n",
    "\n",
    "CodeTextSplitter를 사용하면 다양한 프로그래밍 언어로 작성된 코드를 분할할 수 있습니다.\n",
    "\n",
    "이를 위해서는 Language enum을 import하고, 해당하는 프로그래밍 언어를 지정해주면 됩니다."
   ],
   "id": "426528404b01f188"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "RecursiveCharacterTextSplitter를 사용하여 텍스트를 분할하는 예제입니다.\n",
    "\n",
    "langchain_text_splitters 모듈에서 Language와 RecursiveCharacterTextSplitter 클래스를 임포트합니다.\n",
    "\n",
    "RecursiveCharacterTextSplitter는 텍스트를 문자 단위로 재귀적으로 분할하는 텍스트 분할기입니다."
   ],
   "id": "d178fcba98ba7d03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:40:52.848809Z",
     "start_time": "2025-12-27T13:40:52.842693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")"
   ],
   "id": "6c1e6457be485ebe",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "지원되는 언어의 전체 목록을 가져옵니다.",
   "id": "a961fe9aca48ac2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:41:02.663711Z",
     "start_time": "2025-12-27T13:41:02.657958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 지원되는 언어의 전체 목록을 가져옵니다.\n",
    "[e.value for e in Language]"
   ],
   "id": "d2ba9585ae190e2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'r',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol',\n",
       " 'c',\n",
       " 'lua',\n",
       " 'perl',\n",
       " 'haskell',\n",
       " 'elixir',\n",
       " 'powershell',\n",
       " 'visualbasic6']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "RecursiveCharacterTextSplitter 클래스의 get_separators_for_language 메서드를 사용하여 특정 언어에 사용되는 구분자(separators)를 확인할 수 있습니다.\n",
    "\n",
    "- 예시에서는 Language.PYTHON 열거형 값을 인자로 전달하여 Python 언어에 사용되는 구분자를 확인합니다."
   ],
   "id": "3c78be9c817e41eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:41:23.379664Z",
     "start_time": "2025-12-27T13:41:23.373348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 주어진 언어에 대해 사용되는 구분자를 확인할 수 있습니다.\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ],
   "id": "f63edecc53a684c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Python\n",
    "\n",
    "RecursiveCharacterTextSplitter 사용한 예제는 다음과 같습니다.\n",
    "\n",
    "RecursiveCharacterTextSplitter를 사용하여 Python 코드를 문서 단위로 분할합니다.\n",
    "\n",
    "language 매개변수에 Language.PYTHON을 지정하여 Python 언어를 사용합니다.\n",
    "\n",
    "chunk_size를 50으로 설정하여 각 문서의 최대 크기를 제한합니다.\n",
    "\n",
    "chunk_overlap을 0으로 설정하여 문서 간의 중복을 허용하지 않습니다."
   ],
   "id": "478ffea7b58e8638"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:41:50.722139Z",
     "start_time": "2025-12-27T13:41:50.717205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")"
   ],
   "id": "47d6068008e180f8",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Document 를 생성합니다. 생성된 Document 는 리스트 형태로 반환됩니다.",
   "id": "102eb2209d53eb21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:42:00.269966Z",
     "start_time": "2025-12-27T13:42:00.263626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ],
   "id": "93c4f69d72b36ace",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='hello_world()')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:42:18.133417Z",
     "start_time": "2025-12-27T13:42:18.126604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for doc in python_docs:\n",
    "    print(doc.page_content, end=\"\\n==================\\n\")"
   ],
   "id": "8b6d503e4627c9fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "==================\n",
      "hello_world()\n",
      "==================\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## JS\n",
    "\n",
    "다음은 JS 텍스트 분할기를 사용한 예시입니다"
   ],
   "id": "6481edde98a4a72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:42:32.961152Z",
     "start_time": "2025-12-27T13:42:32.952107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "JS_CODE = \"\"\"\n",
    "function helloWorld() {\n",
    "  console.log(\"Hello, World!\");\n",
    "}\n",
    "\n",
    "helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "\n",
    "js_docs = js_splitter.create_documents([JS_CODE])\n",
    "js_docs"
   ],
   "id": "c27640364ce33487",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='function helloWorld() {\\n  console.log(\"Hello, World!\");\\n}'),\n",
       " Document(metadata={}, page_content='helloWorld();')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TS\n",
    "\n",
    "다음은 TS 텍스트 분할기를 사용한 예시입니다."
   ],
   "id": "f6ed87ce01bf1f6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:42:46.002265Z",
     "start_time": "2025-12-27T13:42:45.995940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TS_CODE = \"\"\"\n",
    "function helloWorld(): void {\n",
    "  console.log(\"Hello, World!\");\n",
    "}\n",
    "\n",
    "helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "ts_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.TS, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "ts_docs = ts_splitter.create_documents([TS_CODE])\n",
    "ts_docs"
   ],
   "id": "479364fc06bc2e72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='function helloWorld(): void {'),\n",
       " Document(metadata={}, page_content='console.log(\"Hello, World!\");\\n}'),\n",
       " Document(metadata={}, page_content='helloWorld();')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Markdown\n",
    "\n",
    "다음은 Markdown 텍스트 분할기를 사용한 예시입니다."
   ],
   "id": "c936b04fe7be81e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:43:57.413378Z",
     "start_time": "2025-12-27T13:43:57.408381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "markdown_text = \"\"\"\n",
    "# 🦜️🔗 LangChain\n",
    "\n",
    "⚡ LLM을 활용한 초스피드 애플리케이션 구축 ⚡\n",
    "\n",
    "## 빠른 설치\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "빠르게 발전하는 분야의 오픈 소스 프로젝트 입니다. 많관부 🙏\n",
    "\"\"\""
   ],
   "id": "8ad79ac57cc2939",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "분할하고 결과를 출력합니다.",
   "id": "8ffe79a44fe37db4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:44:10.495221Z",
     "start_time": "2025-12-27T13:44:10.488219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    # 마크다운 언어를 사용하여 텍스트 분할기 생성\n",
    "    language=Language.MARKDOWN,\n",
    "    # 청크 크기를 60으로 설정\n",
    "    chunk_size=60,\n",
    "    # 청크 간 중복되는 부분이 없도록 설정\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "# 마크다운 텍스트를 분할하여 문서 생성\n",
    "md_docs = md_splitter.create_documents([markdown_text])\n",
    "# 생성된 문서 출력\n",
    "md_docs"
   ],
   "id": "1bc2a22b58a6d11c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# 🦜️🔗 LangChain\\n\\n⚡ LLM을 활용한 초스피드 애플리케이션 구축 ⚡'),\n",
       " Document(metadata={}, page_content='## 빠른 설치'),\n",
       " Document(metadata={}, page_content='```bash\\npip install langchain'),\n",
       " Document(metadata={}, page_content='빠르게 발전하는 분야의 오픈 소스 프로젝트 입니다. 많관부 🙏')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Latex\n",
    "\n",
    "LaTeX는 문서 작성을 위한 마크업 언어로, 수학 기호와 수식을 표현하는 데 널리 사용됩니다.\n",
    "\n",
    "다음은 LaTeX 텍스트의 예시입니다."
   ],
   "id": "2642520a217237c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:44:30.138821Z",
     "start_time": "2025-12-27T13:44:30.134373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latex_text = \"\"\"\n",
    "\\documentclass{article}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\section{Introduction}\n",
    "% LLM은 방대한 양의 텍스트 데이터로 학습하여 사람과 유사한 언어를 생성할 수 있는 기계 학습 모델의 한 유형입니다.\n",
    "% 최근 몇 년 동안 LLM은 언어 번역, 텍스트 생성, 감성 분석 등 다양한 자연어 처리 작업에서 상당한 발전을 이루었습니다.\n",
    "\n",
    "\\subsection{History of LLMs}\n",
    "% 초기 LLM은 1980년대와 1990년대에 개발되었지만, 처리할 수 있는 데이터 양과 당시 사용 가능한 컴퓨팅 능력으로 인해 제한되었습니다.\n",
    "% 그러나 지난 10년 동안 하드웨어와 소프트웨어의 발전으로 대규모 데이터 세트에 대해 LLM을 학습시킬 수 있게 되었고, 이는 성능의 큰 향상으로 이어졌습니다.\n",
    "\n",
    "\\subsection{Applications of LLMs}\n",
    "% LLM은 챗봇, 콘텐츠 생성, 가상 어시스턴트 등 산업 분야에서 많은 응용 분야를 가지고 있습니다.\n",
    "% 또한 언어학, 심리학, 컴퓨터 언어학 연구를 위해 학계에서도 사용될 수 있습니다.\n",
    "\n",
    "\\end{document}\n",
    "\"\"\""
   ],
   "id": "d754e84081fe027",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "분할하고 결과를 출력합니다.",
   "id": "88af053d467ae6b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:44:39.905123Z",
     "start_time": "2025-12-27T13:44:39.895879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latex_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    # 마크다운 언어를 사용하여 텍스트를 분할합니다.\n",
    "    language=Language.LATEX,\n",
    "    # 각 청크의 크기를 60자로 설정합니다.\n",
    "    chunk_size=60,\n",
    "    # 청크 간의 중복되는 문자 수를 0으로 설정합니다.\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "# latex_text를 분할하여 문서 목록을 생성합니다.\n",
    "latex_docs = latex_splitter.create_documents([latex_text])\n",
    "# 생성된 문서 목록을 출력합니다.\n",
    "latex_docs"
   ],
   "id": "5b0bd43f4a6f9e9d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\\\documentclass{article}\\n\\n\\x08egin{document}\\n\\n\\\\maketitle'),\n",
       " Document(metadata={}, page_content='\\\\section{Introduction}\\n% LLM은 방대한 양의 텍스트 데이터로 학습하여 사람과 유사한'),\n",
       " Document(metadata={}, page_content='언어를 생성할 수 있는 기계 학습 모델의 한 유형입니다.\\n% 최근 몇 년 동안 LLM은 언어 번역, 텍스트'),\n",
       " Document(metadata={}, page_content='생성, 감성 분석 등 다양한 자연어 처리 작업에서 상당한 발전을 이루었습니다.'),\n",
       " Document(metadata={}, page_content='\\\\subsection{History of LLMs}\\n% 초기 LLM은 1980년대와 1990년대에'),\n",
       " Document(metadata={}, page_content='개발되었지만, 처리할 수 있는 데이터 양과 당시 사용 가능한 컴퓨팅 능력으로 인해 제한되었습니다.\\n%'),\n",
       " Document(metadata={}, page_content='그러나 지난 10년 동안 하드웨어와 소프트웨어의 발전으로 대규모 데이터 세트에 대해 LLM을 학습시킬 수'),\n",
       " Document(metadata={}, page_content='있게 되었고, 이는 성능의 큰 향상으로 이어졌습니다.'),\n",
       " Document(metadata={}, page_content='\\\\subsection{Applications of LLMs}\\n% LLM은 챗봇, 콘텐츠 생성, 가상'),\n",
       " Document(metadata={}, page_content='어시스턴트 등 산업 분야에서 많은 응용 분야를 가지고 있습니다.\\n% 또한 언어학, 심리학, 컴퓨터 언어학'),\n",
       " Document(metadata={}, page_content='연구를 위해 학계에서도 사용될 수 있습니다.\\n\\n\\\\end{document}')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## HTML\n",
    "\n",
    "HTML 텍스트 분할기를 사용한 예제는 다음과 같습니다."
   ],
   "id": "3448c859f83b885b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:44:58.937012Z",
     "start_time": "2025-12-27T13:44:58.931951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "html_text = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>🦜️🔗 LangChain</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "            }\n",
    "            h1 {\n",
    "                color: darkblue;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>\n",
    "            <h1>🦜️🔗 LangChain</h1>\n",
    "            <p>⚡ Building applications with LLMs through composability ⚡</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n"
   ],
   "id": "7d3def6f3f3664aa",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "분할하고 결과를 출력합니다.",
   "id": "a3ec9fa2953c2140"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:45:10.303759Z",
     "start_time": "2025-12-27T13:45:10.294752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "html_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    # HTML 언어를 사용하여 텍스트 분할기 생성\n",
    "    language=Language.HTML,\n",
    "    # 청크 크기를 60으로 설정\n",
    "    chunk_size=60,\n",
    "    # 청크 간 중복되는 부분이 없도록 설정\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "# 주어진 HTML 텍스트를 분할하여 문서 생성\n",
    "html_docs = html_splitter.create_documents([html_text])\n",
    "# 생성된 문서 출력\n",
    "html_docs"
   ],
   "id": "76824a76d2550f55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='<!DOCTYPE html>\\n<html>'),\n",
       " Document(metadata={}, page_content='<head>\\n        <title>🦜️🔗 LangChain</title>'),\n",
       " Document(metadata={}, page_content='<style>\\n            body {\\n                font-family: Aria'),\n",
       " Document(metadata={}, page_content='l, sans-serif;  \\n            }\\n            h1 {'),\n",
       " Document(metadata={}, page_content='color: darkblue;\\n            }\\n        </style>\\n    </he'),\n",
       " Document(metadata={}, page_content='ad>'),\n",
       " Document(metadata={}, page_content='<body>'),\n",
       " Document(metadata={}, page_content='<div>\\n            <h1>🦜️🔗 LangChain</h1>'),\n",
       " Document(metadata={}, page_content='<p>⚡ Building applications with LLMs through composability ⚡'),\n",
       " Document(metadata={}, page_content='</p>  \\n        </div>'),\n",
       " Document(metadata={}, page_content='<div>\\n            As an open-source project in a rapidly dev'),\n",
       " Document(metadata={}, page_content='eloping field, we are extremely open to contributions.'),\n",
       " Document(metadata={}, page_content='</div>\\n    </body>\\n</html>')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Solidity\n",
    "\n",
    "Solidity 텍스트 분할기를 사용한 예제는 다음과 같습니다:\n",
    "\n",
    "- Solidity 코드를 문자열 형태로 SOL_CODE 변수에 저장합니다.\n",
    "- RecursiveCharacterTextSplitter를 사용하여 Solidity 코드를 청크 단위로 분할하는 sol_splitter를 생성합니다.\n",
    "- language 매개변수를 Language.SOL로 설정하여 Solidity 언어를 지정합니다.\n",
    "- chunk_size를 128로 설정하여 각 청크의 최대 크기를 지정합니다.\n",
    "- chunk_overlap을 0으로 설정하여 청크 간의 중복을 없앱니다.\n",
    "- sol_splitter.create_documents() 메서드를 사용하여 SOL_CODE를 청크 단위로 분할하고, 분할된 청크를 sol_docs 변수에 저장합니다.\n",
    "- sol_docs를 출력하여 분할된 Solidity 코드 청크를 확인합니다."
   ],
   "id": "f806344db64b3a8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:45:48.195122Z",
     "start_time": "2025-12-27T13:45:48.189065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SOL_CODE = \"\"\"\n",
    "pragma solidity ^0.8.20;\n",
    "contract HelloWorld {\n",
    "   function add(uint a, uint b) pure public returns(uint) {\n",
    "       return a + b;\n",
    "   }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 분할하고 결과를 출력합니다.\n",
    "sol_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.SOL, chunk_size=128, chunk_overlap=0\n",
    ")\n",
    "\n",
    "sol_docs = sol_splitter.create_documents([SOL_CODE])\n",
    "sol_docs\n",
    "\n"
   ],
   "id": "5da3b28c95d9557b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='pragma solidity ^0.8.20;'),\n",
       " Document(metadata={}, page_content='contract HelloWorld {  \\n   function add(uint a, uint b) pure public returns(uint) {\\n       return a + b;\\n   }\\n}')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## C\n",
    "\n",
    "C# 텍스트 분할기를 사용한 예제는 다음과 같습니다."
   ],
   "id": "2e066942c4d43c81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:46:36.564540Z",
     "start_time": "2025-12-27T13:46:36.557187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "C_CODE = \"\"\"\n",
    "using System;\n",
    "class Program\n",
    "{\n",
    "    static void Main()\n",
    "    {\n",
    "        Console.WriteLine(\"Enter a number (1-5):\");\n",
    "        int input = Convert.ToInt32(Console.ReadLine());\n",
    "        for (int i = 1; i <= input; i++)\n",
    "        {\n",
    "            if (i % 2 == 0)\n",
    "            {\n",
    "                Console.WriteLine($\"{i} is even.\");\n",
    "            }\n",
    "            else\n",
    "            {\n",
    "                Console.WriteLine($\"{i} is odd.\");\n",
    "            }\n",
    "        }\n",
    "        Console.WriteLine(\"Goodbye!\");\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 분할하고 결과를 출력합니다.\n",
    "c_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.CSHARP, chunk_size=128, chunk_overlap=0\n",
    ")\n",
    "c_docs = c_splitter.create_documents([C_CODE])\n",
    "c_docs"
   ],
   "id": "47df70d99260a5fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='using System;'),\n",
       " Document(metadata={}, page_content='class Program\\n{\\n    static void Main()\\n    {\\n        Console.WriteLine(\"Enter a number (1-5):\");'),\n",
       " Document(metadata={}, page_content='int input = Convert.ToInt32(Console.ReadLine());\\n        for (int i = 1; i <= input; i++)\\n        {'),\n",
       " Document(metadata={}, page_content='if (i % 2 == 0)\\n            {\\n                Console.WriteLine($\"{i} is even.\");\\n            }\\n            else'),\n",
       " Document(metadata={}, page_content='{\\n                Console.WriteLine($\"{i} is odd.\");\\n            }\\n        }\\n        Console.WriteLine(\"Goodbye!\");'),\n",
       " Document(metadata={}, page_content='}\\n}')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 06. 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter)",
   "id": "89755df85c3e3ce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "MarkdownHeaderTextSplitter\n",
    "\n",
    "마크다운 파일의 구조를 이해하고 효율적으로 다루는 것은 문서 작업에 있어 매우 중요할 수 있습니다. 특히, 문서의 전체적인 맥락과 구조를 고려하여 의미 있는 방식으로 텍스트를 임베딩하는 과정은, 광범위한 의미와 주제를 더 잘 포착할 수 있는 포괄적인 벡터 표현을 생성하는 데 큰 도움이 됩니다.\n",
    "\n",
    "이러한 맥락에서, 마크다운 파일의 특정 부분, 즉 헤더별로 내용을 나누고 싶을 때가 있습니다. 예를 들어, 문서 내에서 각각의 헤더 아래에 있는 내용을 기반으로 서로 연관된 정보 덩어리, 즉 '청크'를 만들고 싶은 경우가 그러합니다. 이는 텍스트의 공통된 맥락을 유지하면서도, 문서의 구조적 요소를 효과적으로 활용하려는 시도입니다.\n",
    "\n",
    "이런 과제를 해결하기 위해, MarkdownHeaderTextSplitter 라는 도구를 활용할 수 있습니다. 이 도구는 문서를 지정된 헤더 집합에 따라 분할하여, 각 헤더 그룹 아래의 내용을 별도의 청크로 관리할 수 있게 합니다. 이 방법을 통해, 문서의 전반적인 구조를 유지하면서도 내용을 더 세밀하게 다룰 수 있게 되며, 이는 다양한 처리 과정에서 유용하게 활용될 수 있습니다."
   ],
   "id": "9123da45eeba6c2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- MarkdownHeaderTextSplitter를 사용하여 마크다운 형식의 텍스트를 헤더 단위로 분할합니다.\n",
    "\n",
    "- 마크다운 문서의 헤더(#, ##, ### 등)를 기준으로 텍스트를 분할하는 역할을 합니다.\n",
    "\n",
    "- markdown_document 변수에 마크다운 형식의 문서가 할당됩니다.\n",
    "\n",
    "- headers_to_split_on 리스트에는 마크다운 헤더 레벨과 해당 레벨의 이름이 튜플 형태로 정의됩니다.\n",
    "\n",
    "- MarkdownHeaderTextSplitter 클래스를 사용하여 markdown_splitter 객체를 생성하며, headers_to_split_on 매개변수로 분할 기준이 되는 헤더 레벨을 전달합니다.\n",
    "\n",
    "- split_text 메서드를 호출하여 markdown_document를 헤더 레벨에 따라 분할합니다."
   ],
   "id": "145f18257654be76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:03:50.600151Z",
     "start_time": "2025-12-27T14:03:50.594748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# 마크다운 형식의 문서를 문자열로 정의합니다.\n",
    "markdown_document = \"# Title\\n\\n## 1. SubTitle\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n### 1-1. Sub-SubTitle \\n\\nHi this is Lance \\n\\n## 2. Baz\\n\\nHi this is Molly\"\n",
    "print(markdown_document)"
   ],
   "id": "e726a7f7d9ab22b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Title\n",
      "\n",
      "## 1. SubTitle\n",
      "\n",
      "Hi this is Jim\n",
      "\n",
      "Hi this is Joe\n",
      "\n",
      "### 1-1. Sub-SubTitle \n",
      "\n",
      "Hi this is Lance \n",
      "\n",
      "## 2. Baz\n",
      "\n",
      "Hi this is Molly\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:04:01.467775Z",
     "start_time": "2025-12-27T14:04:01.462308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "headers_to_split_on = [  # 문서를 분할할 헤더 레벨과 해당 레벨의 이름을 정의합니다.\n",
    "    (\n",
    "        \"#\",\n",
    "        \"Header 1\",\n",
    "    ),  # 헤더 레벨 1은 '#'로 표시되며, 'Header 1'이라는 이름을 가집니다.\n",
    "    (\n",
    "        \"##\",\n",
    "        \"Header 2\",\n",
    "    ),  # 헤더 레벨 2는 '##'로 표시되며, 'Header 2'라는 이름을 가집니다.\n",
    "    (\n",
    "        \"###\",\n",
    "        \"Header 3\",\n",
    "    ),  # 헤더 레벨 3은 '###'로 표시되며, 'Header 3'이라는 이름을 가집니다.\n",
    "]\n",
    "\n",
    "# 마크다운 헤더를 기준으로 텍스트를 분할하는 MarkdownHeaderTextSplitter 객체를 생성합니다.\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "# markdown_document를 헤더를 기준으로 분할하여 md_header_splits에 저장합니다.\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in md_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n"
   ],
   "id": "31a36a7f6c97d239",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi this is Jim  \n",
      "Hi this is Joe\n",
      "{'Header 1': 'Title', 'Header 2': '1. SubTitle'}\n",
      "=====================\n",
      "Hi this is Lance\n",
      "{'Header 1': 'Title', 'Header 2': '1. SubTitle', 'Header 3': '1-1. Sub-SubTitle'}\n",
      "=====================\n",
      "Hi this is Molly\n",
      "{'Header 1': 'Title', 'Header 2': '2. Baz'}\n",
      "=====================\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "기본적으로 MarkdownHeaderTextSplitter는 분할되는 헤더를 출력 청크의 내용에서 제거합니다.\n",
    "\n",
    "이는 strip_headers = False로 설정하여 비활성화할 수 있습니다."
   ],
   "id": "575e11649329ccaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:04:30.298974Z",
     "start_time": "2025-12-27T14:04:30.294768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    # 분할할 헤더를 지정합니다.\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    # 헤더를 제거하지 않도록 설정합니다.\n",
    "    strip_headers=False,\n",
    ")\n",
    "# 마크다운 문서를 헤더를 기준으로 분할합니다.\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in md_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ],
   "id": "9becd0efd05a902c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Title  \n",
      "## 1. SubTitle  \n",
      "Hi this is Jim  \n",
      "Hi this is Joe\n",
      "{'Header 1': 'Title', 'Header 2': '1. SubTitle'}\n",
      "=====================\n",
      "### 1-1. Sub-SubTitle  \n",
      "Hi this is Lance\n",
      "{'Header 1': 'Title', 'Header 2': '1. SubTitle', 'Header 3': '1-1. Sub-SubTitle'}\n",
      "=====================\n",
      "## 2. Baz  \n",
      "Hi this is Molly\n",
      "{'Header 1': 'Title', 'Header 2': '2. Baz'}\n",
      "=====================\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "각 마크다운 그룹 내에서는 원하는 텍스트 분할기(text splitter)를 적용할 수 있습니다.",
   "id": "d3c4eadc1b23878f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:04:55.213030Z",
     "start_time": "2025-12-27T14:04:55.208946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "markdown_document = \"# Intro \\n\\n## History \\n\\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\nMarkdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n## Rise and divergence \\n\\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n#### Standardization \\n\\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n## Implementations \\n\\nImplementations of Markdown are available for over a dozen programming languages.\"\n",
    "print(markdown_document)\n"
   ],
   "id": "f4f9b30b77958d8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Intro \n",
      "\n",
      "## History \n",
      "\n",
      "Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \n",
      "\n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \n",
      "\n",
      "## Rise and divergence \n",
      "\n",
      "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \n",
      "\n",
      "additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \n",
      "\n",
      "#### Standardization \n",
      "\n",
      "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \n",
      "\n",
      "## Implementations \n",
      "\n",
      "Implementations of Markdown are available for over a dozen programming languages.\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "먼저, MarkdownHeaderTextSplitter 사용하여 마크다운 문서를 헤더를 기준으로 분할합니다.",
   "id": "8f24aae029c94938"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:05:15.450726Z",
     "start_time": "2025-12-27T14:05:15.445088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),  # 분할할 헤더 레벨과 해당 레벨의 이름을 지정합니다.\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "# Markdown 문서를 헤더 레벨에 따라 분할합니다.\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in md_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ],
   "id": "ed3e9dc28bfaf26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Intro  \n",
      "## History  \n",
      "Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "## Rise and divergence  \n",
      "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \n",
      "additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \n",
      "#### Standardization  \n",
      "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.\n",
      "{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}\n",
      "=====================\n",
      "## Implementations  \n",
      "Implementations of Markdown are available for over a dozen programming languages.\n",
      "{'Header 1': 'Intro', 'Header 2': 'Implementations'}\n",
      "=====================\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "이전의 MarkdownHeaderTextSplitter 로 분할된 결과를 다시 RecursiveCharacterTextSplitter 로 분할합니다.",
   "id": "2dd3d0b3b118dd33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:05:32.223701Z",
     "start_time": "2025-12-27T14:05:32.216812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunk_size = 200  # 분할된 청크의 크기를 지정합니다.\n",
    "chunk_overlap = 20  # 분할된 청크 간의 중복되는 문자 수를 지정합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# 문서를 문자 단위로 분할합니다.\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ],
   "id": "d74ef474e9ce14e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Intro  \n",
      "## History\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "readers in its source code form.[9]\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "## Rise and divergence  \n",
      "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for\n",
      "{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}\n",
      "=====================\n",
      "additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \n",
      "#### Standardization\n",
      "{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}\n",
      "=====================\n",
      "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.\n",
      "{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}\n",
      "=====================\n",
      "## Implementations  \n",
      "Implementations of Markdown are available for over a dozen programming languages.\n",
      "{'Header 1': 'Intro', 'Header 2': 'Implementations'}\n",
      "=====================\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 07. HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter)",
   "id": "726b34af0af2f489"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "HTMLHeaderTextSplitter\n",
    "\n",
    "MarkdownHeaderTextSplitter와 개념적으로 유사한 HTMLHeaderTextSplitter는 텍스트를 요소 수준에서 분할하고 각 헤더에 대한 메타데이터를 추가하는 \"구조 인식\" 청크 생성기입니다.\n",
    "\n",
    "이는 각 청크와 \"관련된\" 메타데이터를 추가합니다.\n",
    "\n",
    "HTMLHeaderTextSplitter는 요소별로 청크를 반환하거나 동일한 메타데이터를 가진 요소를 결합할 수 있으며\n",
    "\n",
    "(a) 관련 텍스트를 의미론적으로 (대략적으로) 그룹화하고\n",
    "\n",
    "(b) 문서 구조에 인코딩된 컨텍스트 풍부한 정보를 보존하는 것을 목표로 합니다."
   ],
   "id": "1eaf7539f127dc63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ① HTML 문자열을 사용하는 경우",
   "id": "35f297698cd26a62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "headers_to_split_on 리스트에 분할 기준이 되는 헤더 태그와 해당 헤더의 이름을 튜플 형태로 지정합니다.\n",
    "\n",
    "HTMLHeaderTextSplitter 객체를 생성하면서 headers_to_split_on 매개변수에 분할 기준 헤더 리스트를 전달합니다."
   ],
   "id": "10ba7e998079275f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:25:17.956982Z",
     "start_time": "2025-12-27T14:25:17.923268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),  # 분할할 헤더 태그와 해당 헤더의 이름을 지정합니다.\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "# 지정된 헤더를 기준으로 HTML 텍스트를 분할하는 HTMLHeaderTextSplitter 객체를 생성합니다.\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "# HTML 문자열을 분할하여 결과를 html_header_splits 변수에 저장합니다.\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in html_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n"
   ],
   "id": "1f9c1318936ac7d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n",
      "Some intro text about Foo.\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n",
      "Bar main section\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section'}\n",
      "=====================\n",
      "Some intro text about Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section'}\n",
      "=====================\n",
      "Bar subsection 1\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}\n",
      "=====================\n",
      "Some text about the first subtopic of Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}\n",
      "=====================\n",
      "Bar subsection 2\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}\n",
      "=====================\n",
      "Some text about the second subtopic of Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}\n",
      "=====================\n",
      "Baz\n",
      "{'Header 1': 'Foo', 'Header 2': 'Baz'}\n",
      "=====================\n",
      "Some text about Baz  \n",
      "Some concluding text about Foo\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ② 다른 splitter와 파이프라인으로 연결하고, 웹 URL에서 HTML을 로드하는 경우입니다.",
   "id": "db770c244529a04d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "이 예시에서는 웹 URL로부터 HTML 콘텐츠를 로드한 후, 이를 다른 splitter와 파이프라인으로 연결하여 처리하는 과정입니다.",
   "id": "d22ab3f2ce0ebe41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:25:47.790472Z",
     "start_time": "2025-12-27T14:25:45.552785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"  # 분할할 텍스트의 URL을 지정합니다.\n",
    "\n",
    "headers_to_split_on = [  # 분할할 HTML 헤더 태그와 해당 헤더의 이름을 지정합니다.\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "# HTML 헤더를 기준으로 텍스트를 분할하는 HTMLHeaderTextSplitter 객체를 생성합니다.\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# URL에서 텍스트를 가져와 HTML 헤더를 기준으로 분할합니다.\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "\n",
    "chunk_size = 500  # 텍스트를 분할할 청크의 크기를 지정합니다.\n",
    "chunk_overlap = 30  # 분할된 청크 간의 중복되는 문자 수를 지정합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(  # 텍스트를 재귀적으로 분할하는 RecursiveCharacterTextSplitter 객체를 생성합니다.\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# HTML 헤더로 분할된 텍스트를 다시 청크 크기에 맞게 분할합니다.\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "\n",
    "# 분할된 텍스트 중 80번째부터 85번째까지의 청크를 출력합니다.\n",
    "for header in splits[80:85]:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ],
   "id": "5a194f90cfbb9b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2 The proof of the First Incompleteness Theorem\n",
      "{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}\n",
      "=====================\n",
      "We now describe the proof of the two theorems, formulating\n",
      "Gödel’s results in Peano arithmetic. Gödel himself\n",
      "used a system related to that defined in Principia Mathematica, but\n",
      "containing Peano arithmetic. In our presentation of the First and\n",
      "Second Incompleteness Theorems we refer to Peano arithmetic as\n",
      "\\(P\\), following Gödel’s notation.  \n",
      "Before proceeding to the details of the formal proof, we define the\n",
      "notion of \\(\\omega\\)-consistency used by Gödel in the First\n",
      "{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}\n",
      "=====================\n",
      "Incompleteness Theorem: \\(P\\) is if\n",
      "\\(P \\vdash \\neg \\phi(\\underline{n})\\) for all \\(n\\)\n",
      "implies \\(P \\not\\vdash \\exists x\\phi(x)\\).\n",
      "Naturally this implies consistency and follows from the assumption\n",
      "that the natural numbers satisfy the axioms of Peano arithmetic.  \n",
      "\\(\\omega\\)-consistent  \n",
      "One of the main technical tools used in the proof is , a mechanism which assigns natural numbers to terms and\n",
      "formulas of our formal theory \\(P\\). There are different ways of\n",
      "{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}\n",
      "=====================\n",
      "doing this. The most common is based on the unique representation of\n",
      "natural numbers as products of powers of primes. Each symbol\n",
      "\\(s\\) of number theory is assigned a positive natural number\n",
      "\\(\\num(s)\\) in a fixed but arbitrary way, e.g.  \n",
      "Gödel\n",
      "numbering  \n",
      "The natural number corresponding to a sequence \\(w = \\lt w_0 ,\\ldots ,w_k \\gt\\)\n",
      "of symbols is  \n",
      "where \\(p_k\\) is the \\(k+1\\)st prime. It\n",
      "is called its Gödel number and denoted by\n",
      "\\(\\ulcorner w\\urcorner\\). In this way we can\n",
      "{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}\n",
      "=====================\n",
      "assign Gödel numbers to formulas, sequences of formulas (once a\n",
      "method for distinguishing when one formula ends and another begins has\n",
      "been adopted), and most notably, proofs.  \n",
      "An essential point here is that when a formula is construed as a\n",
      "natural number, then the numeral corresponding to that natural number\n",
      "can occur as the argument of a formula, thus enabling the syntax to\n",
      "“refer” to itself, so to speak (i.e., when a numeral is\n",
      "{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}\n",
      "=====================\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "한계\n",
    "\n",
    "HTMLHeaderTextSplitter는 HTML 문서 간의 구조적 차이를 처리하려고 시도하지만, 때로는 특정 헤더를 누락할 수 있습니다.\n",
    "\n",
    "예를 들어, 이 알고리즘은 헤더가 항상 관련 텍스트보다 \"위\"에 있는 노드, 즉 이전 형제 노드, 조상 노드 및 이들의 조합에 위치한다고 가정합니다.\n",
    "\n",
    "다음 뉴스 기사(이 문서 작성 시점 기준)에서는 최상위 헤드라인의 텍스트가 \"h1\"으로 태그되어 있지만, 우리가 예상하는 텍스트 요소와는 별개의 하위 트리 에 있는 것을 볼 수 있습니다.\n",
    "\n",
    "따라서 \"h1\" 요소와 관련 텍스트는 청크 메타데이터에 나타나지 않지만, 해당되는 경우 \"h2\"와 관련 텍스트는 볼 수 있습니다."
   ],
   "id": "f4ba2ee4cd710c49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:26:39.309008Z",
     "start_time": "2025-12-27T14:26:34.284432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 분할할 HTML 페이지의 URL을 지정합니다.\n",
    "url = \"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),  # 분할할 헤더 태그와 해당 헤더의 이름을 지정합니다.\n",
    "    (\"h2\", \"Header 2\"),  # 분할할 헤더 태그와 해당 헤더의 이름을 지정합니다.\n",
    "]\n",
    "\n",
    "# 지정된 헤더를 기준으로 HTML 텍스트를 분할하는 HTMLHeaderTextSplitter 객체를 생성합니다.\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# 지정된 URL의 HTML 페이지를 분할하여 결과를 html_header_splits 변수에 저장합니다.\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in html_header_splits:\n",
    "    print(f\"{header.page_content[:100]}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ],
   "id": "d8e86ba9c82c0d6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN values your feedback  \n",
      "1. How relevant is this ad to you?  \n",
      "2. Did you encounter any technical i\n",
      "{}\n",
      "=====================\n",
      "An El Niño winter is coming. Here’s what that could mean for the US\n",
      "{'Header 1': 'An El Niño winter is coming. Here’s what that could mean for the US'}\n",
      "=====================\n",
      "By , CNN Meteorologist  \n",
      "Mary Gilbert  \n",
      "3 min read  \n",
      "Published\n",
      "          4:44 AM EDT, Mon September \n",
      "{'Header 1': 'An El Niño winter is coming. Here’s what that could mean for the US'}\n",
      "=====================\n",
      "What could this winter look like?\n",
      "{'Header 1': 'An El Niño winter is coming. Here’s what that could mean for the US', 'Header 2': 'What could this winter look like?'}\n",
      "=====================\n",
      "No two El Niño winters are the same, but many have temperature and precipitation trends in common.  \n",
      "{'Header 1': 'An El Niño winter is coming. Here’s what that could mean for the US'}\n",
      "=====================\n",
      "Download the CNN app\n",
      "{'Header 1': 'An El Niño winter is coming. Here’s what that could mean for the US', 'Header 2': 'Download the CNN app'}\n",
      "=====================\n",
      "Scan the QR code to download the CNN app on Google Play.  \n",
      "xml version=\"1.0\" encoding=\"UTF-8\"?  \n",
      "Gen\n",
      "{'Header 1': 'An El Niño winter is coming. Here’s what that could mean for the US'}\n",
      "=====================\n",
      "Download the CNN app\n",
      "{'Header 1': 'An El Niño winter is coming. Here’s what that could mean for the US', 'Header 2': 'Download the CNN app'}\n",
      "=====================\n",
      "Scan the QR code to download the CNN app from the Apple Store.  \n",
      "// <![CDATA[\n",
      "        window.modules\n",
      "{}\n",
      "=====================\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 08. 재귀적 JSON 분할(RecursiveJsonSplitter)",
   "id": "87aae6bed288b8b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "RecursiveJsonSplitter\n",
    "\n",
    "이 JSON 분할기는 JSON 데이터를 깊이 우선 탐색(depth-first traversal)하여 더 작은 JSON 청크(chunk)를 생성합니다.\n",
    "\n",
    "이 분할기는 중첩된 JSON 객체를 가능한 한 유지하려고 시도하지만, 청크의 크기를 min_chunk_size와 max_chunk_size 사이로 유지하기 위해 필요한 경우 객체를 분할합니다. 값이 중첩된 JSON이 아니라 매우 큰 문자열인 경우, 해당 문자열은 분할되지 않습니다.\n",
    "\n",
    "청크 크기에 대한 엄격한 제한이 필요한 경우, 이 분할기 이후에 Recursive Text Splitter를 사용하여 해당 청크를 처리하는 것을 고려해 볼 수 있습니다."
   ],
   "id": "f5944c053cf45405"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "분할하는 기준\n",
    "\n",
    "- 텍스트 분할 방식: JSON 값 기준\n",
    "\n",
    "- 청크 크기 측정 방식: 문자 수 기준"
   ],
   "id": "3d7ecc21177c66fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "requests.get() 함수를 사용하여 \"https://api.smith.langchain.com/openapi.json\" URL에서 JSON 데이터를 가져옵니다.\n",
    "\n",
    "가져온 JSON 데이터는 json() 메서드를 통해 Python 딕셔너리 형태로 변환되어 json_data 변수에 저장됩니다."
   ],
   "id": "7d6a29bd8cded775"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:41:10.297218Z",
     "start_time": "2025-12-27T14:41:06.878093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "# JSON 데이터를 로드합니다.\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()"
   ],
   "id": "f935754b751b4b03",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "RecursiveJsonSplitter를 사용하여 JSON 데이터를 분할하는 예제입니다.",
   "id": "1ae330ed202e7f5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:41:26.357033Z",
     "start_time": "2025-12-27T14:41:26.352027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "# JSON 데이터를 최대 300 크기의 청크로 분할하는 RecursiveJsonSplitter 객체를 생성합니다.\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)"
   ],
   "id": "3437d174886643e9",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "splitter.split_json() 함수를 사용하여 JSON 데이터를 재귀적으로 분할합니다.",
   "id": "4661241b6b0c3d72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:41:40.730437Z",
     "start_time": "2025-12-27T14:41:39.172759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# JSON 데이터를 재귀적으로 분할합니다. 작은 JSON 조각에 접근하거나 조작해야 하는 경우에 사용합니다.\n",
    "json_chunks = splitter.split_json(json_data=json_data)"
   ],
   "id": "34a159292863a4da",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- splitter.create_documents() 메서드를 사용하여 JSON 데이터를 문서 형식으로 변환합니다.\n",
    "- splitter.split_text() 메서드를 사용하여 JSON 데이터를 문자열 리스트로 분할합니다."
   ],
   "id": "39abc4f6f7867353"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:41:58.724714Z",
     "start_time": "2025-12-27T14:41:58.413837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# JSON 데이터를 기반으로 문서를 생성합니다.\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "# JSON 데이터를 기반으로 문자열 청크를 생성합니다.\n",
    "texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "# 첫 번째 문자열을 출력합니다.\n",
    "print(docs[0].page_content)\n",
    "\n",
    "print(\"===\" * 20)\n",
    "# 분할된 문자열 청크를 출력합니다.\n",
    "print(texts[0])"
   ],
   "id": "ec9b4c61cdcc0668",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"description\": \"The LangSmith API is used to programmatically create and manage LangSmith resources.\\n\\n## Host\\nhttps://api.smith.langchain.com\\n\\n## Authentication\\nTo authenticate with the LangSmith API, set the `X-Api-Key` header\\nto a valid [LangSmith API key](https://docs.langchain.com/langsmith/create-account-api-key#create-an-api-key).\\n\\n\"}}\n",
      "============================================================\n",
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"description\": \"The LangSmith API is used to programmatically create and manage LangSmith resources.\\n\\n## Host\\nhttps://api.smith.langchain.com\\n\\n## Authentication\\nTo authenticate with the LangSmith API, set the `X-Api-Key` header\\nto a valid [LangSmith API key](https://docs.langchain.com/langsmith/create-account-api-key#create-an-api-key).\\n\\n\"}}\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- texts[2]을 출력하여 큰 청크 중 하나를 검토한 결과, 해당 청크에 리스트 객체가 포함되어 있음을 확인할 수 있습니다.\n",
    "\n",
    "2번째 청크의 크기가 제한(300) 을 초과하는 데에는 리스트 객체인 이유가 있습니다.\n",
    "\n",
    "이는 RecursiveJsonSplitter 가 리스트 객체는 분할하지 않기 때문 입니다."
   ],
   "id": "be4e00d36bf33ba8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:42:27.018305Z",
     "start_time": "2025-12-27T14:42:27.013221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 청크의 크기를 확인해 봅시다.\n",
    "print([len(text) for text in texts][:10])\n",
    "\n",
    "# 더 큰 청크 중 하나를 검토해 보면 리스트 객체가 있는 것을 볼 수 있습니다.\n",
    "print(texts[1])"
   ],
   "id": "89f13347b207280b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[404, 127, 460, 176, 1327, 211, 202, 214, 238, 344]\n",
      "{\"info\": {\"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"tags\": [\"audit-logs\"], \"summary\": \"Get Audit Logs\"}}}}\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2번 index 청크를 다음과 같이 json 모듈을 사용하여 파싱할 수 있습니다.",
   "id": "f604e739f5764411"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:42:44.096642Z",
     "start_time": "2025-12-27T14:42:44.089287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "json_data = json.loads(texts[2])\n",
    "json_data[\"paths\"]"
   ],
   "id": "6bd2daeed32bb4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/api/v1/audit-logs': {'get': {'description': \"Retrieve audit log records for the authenticated user's organization in OCSF format.\\n\\nRequires both start_time and end_time parameters to filter logs within a date range.\\nSupports cursor-based pagination.\\n\\nReturns results in OCSF API Activity (Class UID: 6003) format,\\nwhich is compatible with security monitoring and SIEM tools.\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\"}}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "convert_lists 매개변수를 True로 설정하여 JSON 내의 리스트를 index:item 형태의 key:value 쌍으로 변환할 수 있습니다.",
   "id": "39b58269d62f7a27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:43:33.865342Z",
     "start_time": "2025-12-27T14:43:30.027700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 다음은 JSON을 전처리하고 리스트를 인덱스:항목을 키:값 쌍으로 하는 딕셔너리로 변환합니다.\n",
    "\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "texts = splitter.split_text(json_data=json_data, convert_lists=True)"
   ],
   "id": "1cd37b172873d8f6",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:43:33.877707Z",
     "start_time": "2025-12-27T14:43:33.874383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 리스트가 딕셔너리로 변환되었고, 그 결과를 확인합니다.\n",
    "print(texts[2])"
   ],
   "id": "87ff492e4df7d76b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"description\": \"Retrieve audit log records for the authenticated user's organization in OCSF format.\\n\\nRequires both start_time and end_time parameters to filter logs within a date range.\\nSupports cursor-based pagination.\\n\\nReturns results in OCSF API Activity (Class UID: 6003) format,\\nwhich is compatible with security monitoring and SIEM tools.\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\"}}}}\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "docs 리스트의 특정 인덱스에 해당하는 문서를 확인할 수 있습니다.",
   "id": "a3248066b9d4ba83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:43:44.638143Z",
     "start_time": "2025-12-27T14:43:44.632696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2번 문서를 확인합니다.\n",
    "docs[2]"
   ],
   "id": "678a586b5327847a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='{\"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"description\": \"Retrieve audit log records for the authenticated user\\'s organization in OCSF format.\\\\n\\\\nRequires both start_time and end_time parameters to filter logs within a date range.\\\\nSupports cursor-based pagination.\\\\n\\\\nReturns results in OCSF API Activity (Class UID: 6003) format,\\\\nwhich is compatible with security monitoring and SIEM tools.\\\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\"}}}}')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "df62fd4a9adbed9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
