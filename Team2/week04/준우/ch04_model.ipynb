{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 01. 다양한 LLM 활용",
   "id": "25660aa3011c4c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T13:59:04.445107Z",
     "start_time": "2025-11-15T13:59:04.433550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ],
   "id": "96addd5a9279ff98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T14:53:49.460485Z",
     "start_time": "2025-11-15T14:53:49.454741Z"
    }
   },
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH04-Models\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OpenAI",
   "id": "fa5ea36432d15eaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OpenAI는 **채팅 전용 Large Language Model (LLM)** 을 제공합니다. 이 모델을 생성할 때 다양한 옵션을 지정할 수 있으며, 이러한 옵션들은 모델의 동작 방식에 영향을 미칩니다",
   "id": "a83dcdaac99065e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "옵션 상세 설명\n",
    "1. `temperature`\n",
    "\n",
    "**샘플링 온도**를 설정하는 옵션입니다. 값은 0과 2 사이에서 선택할 수 있습니다. 높은 값(예: 0.8)은 출력을 더 무작위하게 만들고, 낮은 값(예: 0.2)은 출력을 더 집중되고 결정론적으로 만듭니다.\n",
    "\n",
    "2. `max_tokens`\n",
    "\n",
    "채팅 완성에서 생성할 **토큰의 최대 개수**를 지정합니다. 이 옵션은 모델이 한 번에 생성할 수 있는 텍스트의 길이를 제어합니다.\n",
    "\n",
    "3. `model_name`\n",
    "적용 가능한 모델을 선택하는 옵션입니다. 더 자세한 정보는 OpenAI 모델 문서에서 확인할 수 있습니다.\n",
    "\n",
    "4. 모델 스펙\n",
    "\n",
    "링크: https://platform.openai.com/docs/models/gpt-4o"
   ],
   "id": "6e21bae830220728"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:53:52.700920Z",
     "start_time": "2025-11-15T14:53:52.066739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# ChatOpenAI 객체를 생성합니다.\n",
    "gpt = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o\",  # 모델명\n",
    ")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = gpt.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)\n"
   ],
   "id": "4642c0c7d38cdf1d",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ChatOpenAI\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_teddynote\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmessages\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m stream_response\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# ChatOpenAI 객체를 생성합니다.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_openai\\__init__.py:3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"Module for OpenAI integrations.\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AzureChatOpenAI, ChatOpenAI\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AzureOpenAI, OpenAI\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"Module for OpenAI chat models.\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mazure\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AzureChatOpenAI\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ChatOpenAI\n\u001B[32m      6\u001B[39m __all__ = [\u001B[33m\"\u001B[39m\u001B[33mAzureChatOpenAI\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mChatOpenAI\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py:12\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mopenai\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlanguage_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LanguageModelInput\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlanguage_models\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LangSmithParams\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01moutputs\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ChatGenerationChunk, ChatResult\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Runnable\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e63926723833d0a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:54:00.844921Z",
     "start_time": "2025-11-15T14:54:00.278569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# ChatAnthropic 객체를 생성합니다.\n",
    "anthropic = ChatAnthropic(model_name=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = anthropic.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)\n"
   ],
   "id": "32c7a425cb93141a",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_anthropic\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ChatAnthropic\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# ChatAnthropic 객체를 생성합니다.\u001B[39;00m\n\u001B[32m      4\u001B[39m anthropic = ChatAnthropic(model_name=\u001B[33m\"\u001B[39m\u001B[33mclaude-3-5-sonnet-20241022\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_anthropic\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_anthropic\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      2\u001B[39m     ChatAnthropic,\n\u001B[32m      3\u001B[39m     ChatAnthropicMessages,\n\u001B[32m      4\u001B[39m     convert_to_anthropic_tool,\n\u001B[32m      5\u001B[39m )\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_anthropic\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Anthropic, AnthropicLLM\n\u001B[32m      8\u001B[39m __all__ = [\n\u001B[32m      9\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mChatAnthropicMessages\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     10\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mChatAnthropic\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     13\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mAnthropicLLM\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     14\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:31\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexceptions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OutputParserException\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlanguage_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LanguageModelInput\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlanguage_models\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     32\u001B[39m     BaseChatModel,\n\u001B[32m     33\u001B[39m     LangSmithParams,\n\u001B[32m     34\u001B[39m     agenerate_from_stream,\n\u001B[32m     35\u001B[39m     generate_from_stream,\n\u001B[32m     36\u001B[39m )\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmessages\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     38\u001B[39m     AIMessage,\n\u001B[32m     39\u001B[39m     AIMessageChunk,\n\u001B[32m   (...)\u001B[39m\u001B[32m     44\u001B[39m     ToolMessage,\n\u001B[32m     45\u001B[39m )\n\u001B[32m     46\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmessages\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mai\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m InputTokenDetails, UsageMetadata\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py)"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 02. 캐싱(Cache)",
   "id": "8f7673cf80e3bc10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:00:38.425672Z",
     "start_time": "2025-11-15T14:00:38.420673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 모델을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{country} 에 대해서 200자 내외로 요약해줘\")\n",
    "\n",
    "# 체인을 생성합니다.\n",
    "chain = prompt | llm\n"
   ],
   "id": "d42f35b5a939cbaf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:00:45.014497Z",
     "start_time": "2025-11-15T14:00:40.694819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)\n"
   ],
   "id": "9ded4e058580cb74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 고도 경제대국으로, 문화와 역사적으로도 다양한 매력을 지니고 있다. 대한민국과 북한으로 나뉘어 있으며, 수도는 서울에 위치해 있다. 산업화와 IT 기술 발전으로 세계에서 주요 경제 국가로 발전하였고, K-pop이나 K-drama 등의 문화 산업 역시 글로벌한 인기를 누리고 있다. 또한, 전통적인 문화와 성장한 대도시들 사이의 조화로움이 독특한 매력을 지니고 있으며, 맛있는 음식과 아름다운 자연환경도 많이 갖고 있다. 현대적인 발전과 전통적인 가치를 동시에 제공하는 한국은 매년 많은 관광객들의 방문을 유치하고 있다.\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## InMemoryCache",
   "id": "3357dc33c030aee9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "인메모리 캐시를 사용하여 동일 질문에 대한 답변을 저장하고, 캐시에 저장된 답변을 반환",
   "id": "3c7e264f63ad5979"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:00:48.563971Z",
     "start_time": "2025-11-15T14:00:45.035043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# 인메모리 캐시를 사용합니다.\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)\n"
   ],
   "id": "6558b237c91580c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 반도 국가로, 수도는 서울이다.  대한민국과 북한으로 나뉘어져 있으며, 한국어를 공용어로 사용한다. 높은 경제성장률과 혁신적인 기술력으로 세계적으로 주목받는 나라이다. 또한 한류 열풍을 일으키며 한국 문화뿐만 아니라 음악, 드라마, 영화 등이 전 세계에서 인기를 끌고 있다. 한국은 고대부터 현대까지 다채로운 역사와 전통 문화를 지녔으며, 한반도의 아름다운 자연환경과 맛있는 음식으로도 유명하다. 현재는 선진화된 도시와 전통적인 문화가 공존하는 다양한 모습을 보여주고 있다.\n",
      "CPU times: total: 219 ms\n",
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "두번쨰로 실행할떄는 CPU Times가 0 ns 로 나온다",
   "id": "2be06228bfe7c442"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:00:50.174776Z",
     "start_time": "2025-11-15T14:00:50.165809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)\n"
   ],
   "id": "929552f2d0da5ef0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 반도 국가로, 수도는 서울이다.  대한민국과 북한으로 나뉘어져 있으며, 한국어를 공용어로 사용한다. 높은 경제성장률과 혁신적인 기술력으로 세계적으로 주목받는 나라이다. 또한 한류 열풍을 일으키며 한국 문화뿐만 아니라 음악, 드라마, 영화 등이 전 세계에서 인기를 끌고 있다. 한국은 고대부터 현대까지 다채로운 역사와 전통 문화를 지녔으며, 한반도의 아름다운 자연환경과 맛있는 음식으로도 유명하다. 현재는 선진화된 도시와 전통적인 문화가 공존하는 다양한 모습을 보여주고 있다.\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 5.76 ms\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SQLite Cache",
   "id": "fd2865fb5833fb67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T09:54:13.713701Z",
     "start_time": "2025-11-15T09:54:13.661425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "import os\n",
    "\n",
    "# 캐시 디렉토리를 생성합니다.\n",
    "if not os.path.exists(\"cache\"):\n",
    "    os.makedirs(\"cache\")\n",
    "\n",
    "# SQLiteCache를 사용합니다.\n",
    "set_llm_cache(SQLiteCache(database_path=\"cache/llm_cache.db\"))\n"
   ],
   "id": "4a0305c9cecf54d3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T09:54:32.262357Z",
     "start_time": "2025-11-15T09:54:32.247017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)\n"
   ],
   "id": "97300a73fdc66bc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 고도로 발달한 경제를 갖춘 선진국가이다. 주요 산업은 자동차, 전자제품, 선박 등이며 세계적으로 유명한 기업들이 많이 위치해 있다. 문화적으로는 한글과 한국 전통 의상, 음식 등이 유명하며 K-pop, K-drama 등 한류 열풍이 전 세계적으로 인기를 끌고 있다. 또한 이곳에서는 서로를 존경하고 예의 바른 태도를 중요시하는 문화가 뿌리깊게 자리잡고 있다. 정치적으로는 북한과 한반도 문제, 미국과의 관계 등 여러 이슈가 있지만 안정적으로 발전하며 세계에서 주목받는 나라 중 하나이다.\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 11.5 ms\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "마찬가지로 재실행할 시 저장된 값이 그대로 출력된다.",
   "id": "15f2ebe550ff8f2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:01:22.538681Z",
     "start_time": "2025-11-15T14:01:22.530007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ],
   "id": "a5ebd28844cea19d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 반도 국가로, 수도는 서울이다.  대한민국과 북한으로 나뉘어져 있으며, 한국어를 공용어로 사용한다. 높은 경제성장률과 혁신적인 기술력으로 세계적으로 주목받는 나라이다. 또한 한류 열풍을 일으키며 한국 문화뿐만 아니라 음악, 드라마, 영화 등이 전 세계에서 인기를 끌고 있다. 한국은 고대부터 현대까지 다채로운 역사와 전통 문화를 지녔으며, 한반도의 아름다운 자연환경과 맛있는 음식으로도 유명하다. 현재는 선진화된 도시와 전통적인 문화가 공존하는 다양한 모습을 보여주고 있다.\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 5.5 ms\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 03. 모델 직렬화(Serialization) - 저장 및 불러오기",
   "id": "48c2942b3f55e5de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "직렬화(Serialization)\n",
    "직렬화(Serialization) 란?\n",
    "\n",
    "정의\n",
    "\n",
    ": 모델을 저장 가능한 형식으로 변환하는 과정\n",
    "목적\n",
    "\n",
    "- 모델 재사용 (재훈련 없이)\n",
    "- 모델 배포 및 공유 용이\n",
    "- 계산 리소스 절약\n",
    "\n",
    "장점\n",
    "\n",
    "- 빠른 모델 로딩\n",
    "- 버전 관리 가능\n",
    "- 다양한 환경에서 사용 가능\n",
    "\n",
    "- 모델 직렬화는 AI 개발 및 배포 과정에서 중요한 단계로, 효율적인 모델 관리와 재사용을 가능하게 합니다.\n",
    "\n",
    "is_lc_serializable 클래스 메서드로 실행하여 LangChain 클래스가 직렬화 가능한지 확인할 수 있습니다"
   ],
   "id": "710a7896b8e87b9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:01:30.902800Z",
     "start_time": "2025-11-15T14:01:30.863549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 프롬프트 템플릿을 사용하여 질문을 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{fruit}의 색상이 무엇입니까?\")\n"
   ],
   "id": "86667035584d23b5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:01:32.788602Z",
     "start_time": "2025-11-15T14:01:32.785630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 직렬화가 가능한지 체크합니다.\n",
    "print(f\"ChatOpenAI: {ChatOpenAI.is_lc_serializable()}\")"
   ],
   "id": "1c6391cdd35095c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI: True\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:01:33.713380Z",
     "start_time": "2025-11-15T14:01:33.709190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 직렬화가 가능한지 체크합니다.\n",
    "print(f\"ChatOpenAI: {llm.is_lc_serializable()}\")"
   ],
   "id": "bc089c8b8f162186",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI: True\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:19:43.915245Z",
     "start_time": "2025-11-15T10:19:43.908766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 체인을 생성합니다.\n",
    "chain = prompt | llm\n",
    "\n",
    "# 직렬화가 가능한지 체크합니다.\n",
    "chain.is_lc_serializable()"
   ],
   "id": "6c11368e39e32466",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 체인(Chain) 직렬화(dumps, dumpd)",
   "id": "71116c00aabeeb2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "체인 직렬화는 직렬화 가능한 모든 객체를 딕셔너리 또는 JSON 문자열로 변환하는 과정을 의미합니다.\n",
    "\n",
    "직렬화 방법\n",
    "- 객체의 속성 및 데이터를 키-값 쌍으로 저장하여 딕셔너리 형태로 변환합니다.\n",
    "\n",
    "- 이러한 직렬화 방식은 객체를 쉽게 저장하고 전송할 수 있게 하며, 다양한 환경에서 객체를 재구성할 수 있도록 합니다.\n",
    "\n",
    "참고\n",
    "\n",
    "`dumps`: 객체를 JSON 문자열로 직렬화\n",
    "\n",
    "`dumpd`: 객체를 딕셔너리로 직렬화"
   ],
   "id": "8bae0219bc1c1efc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:01:36.854897Z",
     "start_time": "2025-11-15T14:01:36.848953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.load import dumpd, dumps\n",
    "\n",
    "dumpd_chain = dumpd(chain)\n",
    "dumpd_chain"
   ],
   "id": "45dd98afa089f278",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'],\n",
       " 'kwargs': {'first': {'lc': 1,\n",
       "   'type': 'constructor',\n",
       "   'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],\n",
       "   'kwargs': {'input_variables': ['country'],\n",
       "    'template': '{country} 에 대해서 200자 내외로 요약해줘',\n",
       "    'template_format': 'f-string'},\n",
       "   'name': 'PromptTemplate'},\n",
       "  'last': {'lc': 1,\n",
       "   'type': 'constructor',\n",
       "   'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       "   'kwargs': {'model_name': 'gpt-3.5-turbo',\n",
       "    'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']},\n",
       "    'stream_usage': True,\n",
       "    'output_version': 'v0'},\n",
       "   'name': 'ChatOpenAI'}},\n",
       " 'name': 'RunnableSequence'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:24:03.435213Z",
     "start_time": "2025-11-15T10:24:03.428063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 직렬화된 체인의 타입을 확인합니다.\n",
    "type(dumpd_chain)"
   ],
   "id": "8f5401975d86c93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:24:09.461966Z",
     "start_time": "2025-11-15T10:24:09.455447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dumps 함수를 사용하여 직렬화된 체인을 확인합니다.\n",
    "dumps_chain = dumps(chain)\n",
    "dumps_chain"
   ],
   "id": "39dfcd8e5351cc27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"runnable\", \"RunnableSequence\"], \"kwargs\": {\"first\": {\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"prompts\", \"prompt\", \"PromptTemplate\"], \"kwargs\": {\"input_variables\": [\"fruit\"], \"template\": \"{fruit}\\\\uc758 \\\\uc0c9\\\\uc0c1\\\\uc774 \\\\ubb34\\\\uc5c7\\\\uc785\\\\ub2c8\\\\uae4c?\", \"template_format\": \"f-string\"}, \"name\": \"PromptTemplate\"}, \"last\": {\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"model_name\": \"gpt-3.5-turbo\", \"temperature\": 0.0, \"openai_api_key\": {\"lc\": 1, \"type\": \"secret\", \"id\": [\"OPENAI_API_KEY\"]}, \"output_version\": \"v0\"}, \"name\": \"ChatOpenAI\"}}, \"name\": \"RunnableSequence\"}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pickle 파일을 활용한 직렬화",
   "id": "ff997a0f7e182e20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:28:40.364693Z",
     "start_time": "2025-11-15T10:28:40.359779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# fuit_chain.pkl 파일로 직렬화된 체인을 저장합니다.\n",
    "with open(\"fruit_chain.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dumpd_chain, f)\n"
   ],
   "id": "280821d0cc9c0262",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "JSON을 활용하는 방법 또한 앞서 본 것처럼 가능하다",
   "id": "d3e536534dda1f4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:28:41.260596Z",
     "start_time": "2025-11-15T10:28:41.255537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"fruit_chain.json\", \"w\") as fp:\n",
    "    json.dump(dumpd_chain, fp)"
   ],
   "id": "e47f5734d1ce993c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## load: 저장한 모델 불러오기",
   "id": "d55d1cc3308073d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:28:58.389528Z",
     "start_time": "2025-11-15T10:28:58.384009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# pickle 파일을 로드합니다.\n",
    "with open(\"fruit_chain.pkl\", \"rb\") as f:\n",
    "    loaded_chain = pickle.load(f)\n"
   ],
   "id": "35d457c57e75be5c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:29:04.378179Z",
     "start_time": "2025-11-15T10:29:02.366605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.load import load\n",
    "\n",
    "# 체인을 로드합니다.\n",
    "chain_from_file = load(loaded_chain)\n",
    "\n",
    "# 체인을 실행합니다.\n",
    "print(chain_from_file.invoke({\"fruit\": \"사과\"}))\n"
   ],
   "id": "9a8b0b598980b7c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osca0\\AppData\\Local\\Temp\\ipykernel_20460\\1490994347.py:4: LangChainBetaWarning: The function `load` is in beta. It is actively being worked on, so the API may change.\n",
      "  chain_from_file = load(loaded_chain)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='사과의 색상은 주로 빨간색이지만, 녹색, 노란색, 주황색 등 다양한 색상의 사과도 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 24, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cc7oc0Y37967OIUs6na9uX3WDB0DZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c515b5d6-83b5-4b0c-9b3f-bf9a0487cce2-0' usage_metadata={'input_tokens': 24, 'output_tokens': 51, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:29:50.886176Z",
     "start_time": "2025-11-15T10:29:50.862980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.load import load, loads\n",
    "\n",
    "load_chain = load(\n",
    "    loaded_chain, secrets_map={\"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"]}\n",
    ")\n",
    "\n",
    "# 불러온 체인이 정상 동작하는지 확인합니다.\n",
    "load_chain.invoke({\"fruit\": \"사과\"})\n"
   ],
   "id": "1167b872add507f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='사과의 색상은 주로 빨간색이지만, 녹색, 노란색, 주황색 등 다양한 색상의 사과도 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 24, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cc7oc0Y37967OIUs6na9uX3WDB0DZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c515b5d6-83b5-4b0c-9b3f-bf9a0487cce2-0', usage_metadata={'input_tokens': 24, 'output_tokens': 51, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}, 'total_cost': 0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:29:52.259213Z",
     "start_time": "2025-11-15T10:29:52.254368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"fruit_chain.json\", \"r\") as fp:\n",
    "    loaded_from_json_chain = json.load(fp)\n",
    "    loads_chain = load(loaded_from_json_chain)\n"
   ],
   "id": "e5159bba7f041e0e",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:29:59.595586Z",
     "start_time": "2025-11-15T10:29:59.571389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 불러온 체인이 정상 동작하는지 확인합니다.\n",
    "loads_chain.invoke({\"fruit\": \"사과\"})"
   ],
   "id": "7b63f2a5880be512",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='사과의 색상은 주로 빨간색이지만, 녹색, 노란색, 주황색 등 다양한 색상의 사과도 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 24, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cc7oc0Y37967OIUs6na9uX3WDB0DZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c515b5d6-83b5-4b0c-9b3f-bf9a0487cce2-0', usage_metadata={'input_tokens': 24, 'output_tokens': 51, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}, 'total_cost': 0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 04. 토큰 사용량 확인",
   "id": "c819b71738007b3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "특정 호출에 대한 토큰 사용량을 추적하는 방법에 대해 설명합니다.\n",
    "\n",
    "이 기능은 현재 OpenAI API 에만 구현되어 있습니다.\n",
    "\n",
    "먼저 단일 Chat 모델 호출에 대한 토큰 사용량을 추적하는 매우 간단한 예를 살펴보겠습니다."
   ],
   "id": "d301352a3008a5b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:30:50.204944Z",
     "start_time": "2025-11-15T10:30:50.175447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델을 불러옵니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")"
   ],
   "id": "ae190793cb57e227",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with get_openai_callback() 구문안에서 실행되는 모든 토큰 사용량/요금이 추적됩니다.",
   "id": "6e4fd17fdadb094c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:31:01.250838Z",
     "start_time": "2025-11-15T10:31:00.161287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# callback을 사용하여 추적합니다.\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"대한민국의 수도는 어디야?\")\n",
    "    print(cb)\n"
   ],
   "id": "3d7588e4d2a216f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 23\n",
      "\tPrompt Tokens: 15\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 8\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00011750000000000001\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T10:31:10.401013Z",
     "start_time": "2025-11-15T10:31:10.381639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# callback을 사용하여 추적합니다.\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"대한민국의 수도는 어디야?\")\n",
    "    result = llm.invoke(\"대한민국의 수도는 어디야?\")\n",
    "    print(f\"총 사용된 토큰수: \\t\\t{cb.total_tokens}\")\n",
    "    print(f\"프롬프트에 사용된 토큰수: \\t{cb.prompt_tokens}\")\n",
    "    print(f\"답변에 사용된 토큰수: \\t{cb.completion_tokens}\")\n",
    "    print(f\"호출에 청구된 금액(USD): \\t${cb.total_cost}\")"
   ],
   "id": "f46217049eb50e82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 사용된 토큰수: \t\t46\n",
      "프롬프트에 사용된 토큰수: \t30\n",
      "답변에 사용된 토큰수: \t16\n",
      "호출에 청구된 금액(USD): \t$0.00023500000000000002\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 05. 구글 생성 AI(Google Generative AI)",
   "id": "972d89814866e9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Google AI chat models (gemini-pro)\n",
    "- 패키지 버전 문제로 직접 실행 불가\n",
    "\n",
    "Google AI의 gemini와 gemini-vision 모델뿐만 아니라 다른 생성 모델에 접근하려면 langchain-google-genai 통합 패키지의 ChatGoogleGenerativeAI 클래스를 사용하면 됩니다."
   ],
   "id": "3a73d485c8f8ec64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:56:13.048718Z",
     "start_time": "2025-11-15T14:55:53.732414Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -qU langchain-google-genai",
   "id": "e7c4dd3beaac4878",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-generativeai 0.3.2 requires google-ai-generativelanguage==0.4.0, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\n",
      "langchain 0.1.19 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain 0.1.19 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.4.43 which is incompatible.\n",
      "langchain-anthropic 0.3.10 requires langchain-core<1.0.0,>=0.3.45, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-azure-ai 0.1.2 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-chroma 0.2.2 requires langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-cohere 0.4.3 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.0.38 which is incompatible.\n",
      "langchain-cohere 0.4.3 requires langchain-core<0.4.0,>=0.3.27, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-community 0.0.38 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.4.43 which is incompatible.\n",
      "langchain-elasticsearch 0.3.2 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-experimental 0.3.4 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.0.38 which is incompatible.\n",
      "langchain-experimental 0.3.4 requires langchain-core<0.4.0,>=0.3.28, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-huggingface 0.1.2 requires langchain-core<0.4.0,>=0.3.15, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-milvus 0.1.8 requires langchain-core<0.4,>=0.2.38, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-ollama 0.2.3 requires langchain-core<0.4.0,>=0.3.33, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-openai 0.3.35 requires langchain-core<1.0.0,>=0.3.78, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain-teddynote 0.5.4 requires langchain>=0.3.27, but you have langchain 0.1.19 which is incompatible.\n",
      "langchain-text-splitters 0.0.2 requires langchain-core<0.3,>=0.1.28, but you have langchain-core 1.0.5 which is incompatible.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 패키지 호환성 문제 해결",
   "id": "7d76afe01712618a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:03:22.232561Z",
     "start_time": "2025-11-15T14:03:20.906949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 현재 버전을 삭제하고\n",
    "!pip uninstall -y langchain-core langchain-google-genai langchain langchain-community langsmith\n",
    "# 다운그레이드된 버전을 설치한다/\n",
    "!pip install \"langchain==0.1.19\" \"langchain-core==0.1.52\" \"langchain-google-genai==0.0.9\" \"google-generativeai==0.3.2\" \"google-ai-generativelanguage==0.4.0\""
   ],
   "id": "c23c159cae3e6385",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Skipping langchain-core as it is not installed.\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Skipping langchain-google-genai as it is not installed.\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Skipping langchain as it is not installed.\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Skipping langchain-community as it is not installed.\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Skipping langsmith as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Collecting langchain==0.1.19\n",
      "  Using cached langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-core==0.1.52\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-google-genai==0.0.9\n",
      "  Using cached langchain_google_genai-0.0.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: google-generativeai==0.3.2 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (0.3.2)\n",
      "Collecting google-ai-generativelanguage==0.4.0\n",
      "  Using cached google_ai_generativelanguage-0.4.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain==0.1.19) (6.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain==0.1.19) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain==0.1.19) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain==0.1.19) (0.6.7)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain==0.1.19)\n",
      "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.19)\n",
      "  Using cached langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.19)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain==0.1.19) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain==0.1.19) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain==0.1.19) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain==0.1.19) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langchain-core==0.1.52) (1.33)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core==0.1.52)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: google-auth in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-generativeai==0.3.2) (2.41.1)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-generativeai==0.3.2) (1.34.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-generativeai==0.3.2) (4.15.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-generativeai==0.3.2) (3.20.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-generativeai==0.3.2) (4.67.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.4.0) (1.26.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (0.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-api-core->google-generativeai==0.3.2) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0) (1.67.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0) (1.48.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-auth->google-generativeai==0.3.2) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-auth->google-generativeai==0.3.2) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from google-auth->google-generativeai==0.3.2) (4.9.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.1.52) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.19) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.19) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.1.19) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.1.19) (2.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth->google-generativeai==0.3.2) (0.6.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.19) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from tqdm->google-generativeai==0.3.2) (0.4.6)\n",
      "Using cached langchain-0.1.19-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "Using cached langchain_google_genai-0.0.9-py3-none-any.whl (17 kB)\n",
      "Using cached google_ai_generativelanguage-0.4.0-py3-none-any.whl (598 kB)\n",
      "Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "Using cached langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: packaging, langsmith, langchain-core, langchain-text-splitters, langchain-community, google-ai-generativelanguage, langchain, langchain-google-genai\n",
      "\n",
      "  Attempting uninstall: packaging\n",
      "\n",
      "    Found existing installation: packaging 23.2\n",
      "\n",
      "    Uninstalling packaging-23.2:\n",
      "\n",
      "      Successfully uninstalled packaging-23.2\n",
      "\n",
      "   ---------------------------------------- 0/8 [packaging]\n",
      "   ----- ---------------------------------- 1/8 [langsmith]\n",
      "   ----- ---------------------------------- 1/8 [langsmith]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: 'C:\\\\Users\\\\osca0\\\\PycharmProjects\\\\StudyLangChain\\\\.venv\\\\Lib\\\\site-packages\\\\langsmith\\\\wrappers\\\\_openai.py'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:03:24.520682Z",
     "start_time": "2025-11-15T14:03:24.515565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ],
   "id": "1627f77c242375f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "langchain_google_genai 패키지에서 ChatGoogleGenerativeAI 클래스를 가져옵니다.\n",
    "\n",
    "\n",
    "ChatGoogleGenerativeAI 클래스는 Google의 Generative AI 모델을 사용하여 대화형 AI 시스템을 구현하는 데 사용됩니다.\n",
    "\n",
    "이 클래스를 통해 사용자는 Google의 대화형 AI 모델과 상호 작용할 수 있습니다.\n",
    "\n",
    "모델과의 대화는 채팅 형식으로 이루어지며, 사용자의 입력에 따라 모델이 적절한 응답을 생성합니다.\n",
    "\n",
    "ChatGoogleGenerativeAI 클래스는 LangChain 프레임워크와 통합되어 있어, 다른 LangChain 컴포넌트와 함께 사용할 수 있습니다.\n",
    "\n",
    "지원되는 모델 정보: https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko"
   ],
   "id": "6a8d52697ce5dec6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:03:26.711425Z",
     "start_time": "2025-11-15T14:03:26.706420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "  langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ChatGoogleGenerativeAI 언어 모델을 초기화합니다.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
    "\n",
    "# 프롬프트를 전달하여 결과를 생성합니다.\n",
    "answer = llm.stream(\"자연어처리에 대해서 간략히 설명해 줘\")\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "stream_response(answer)"
   ],
   "id": "60a705c7932889fb",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1727184390.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mlangchain_google_genai import ChatGoogleGenerativeAI\u001B[39m\n    ^\n\u001B[31mIndentationError\u001B[39m\u001B[31m:\u001B[39m unexpected indent\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# ChatGoogleGenerativeAI 언어 모델을 초기화합니다.\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",  # 사용할 모델을 지정합니다.\n",
    ")\n",
    "\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"예/아니오 질문에 대답하세요. {question}는 과일입니까?\"\n",
    ")\n",
    "\n",
    "# 체인을 생성합니다.\n",
    "chain = prompt | model\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "stream_response(chain.stream({\"question\": \"사과\"}))\n"
   ],
   "id": "e58c6a59d58950c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Safety Settings",
   "id": "f38d4671556211cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Gemini 모델에는 기본 안전 설정(Satety Settings) 이 있지만, 이를 재정의할 수 있습니다.\n",
    "\n",
    "\n",
    "만약 모델로부터 많은 \"Safety Warnings\"를 받고 있다면, 모델의 safety_settings 속성을 조정해 볼 수 있습니다.\n",
    "\n",
    "\n",
    "Google의 Safety Setting Types 문서에서는 사용 가능한 카테고리와 임계값에 대한 열거형 정보를 제공합니다.\n",
    "\n",
    "\n",
    "이 문서에는 콘텐츠 필터링 및 안전 설정과 관련된 다양한 카테고리와 해당 임계값이 정의되어 있어, 개발자들이 생성형 AI 모델을 활용할 때 적절한 안전 설정을 선택하고 적용하는 데 도움을 줍니다.\n",
    "\n",
    "\n",
    "이를 통해 개발자들은 모델이 생성하는 콘텐츠의 안전성과 적절성을 보장하고, 사용자에게 유해하거나 부적절한 내용이 노출되는 것을 방지할 수 있습니다."
   ],
   "id": "1cf8a66b62f8d28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_google_genai import (\n",
    "    ChatGoogleGenerativeAI,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    # 사용할 모델을 \"gemini-pro\"로 지정합니다.\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    safety_settings={\n",
    "        # 위험한 콘텐츠에 대한 차단 임계값을 설정합니다.\n",
    "        # 이 경우 위험한 콘텐츠를 차단하지 않도록 설정되어 있습니다. (그럼에도 기본적인 차단이 있을 수 있습니다.)\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    ")"
   ],
   "id": "94026c72e51803f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Batch 단위 실행",
   "id": "3dd6a76d8620bb7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    # 사용할 모델을 \"gemini-pro\"로 지정합니다.\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    ")\n",
    "\n",
    "results = llm.batch(\n",
    "    [\n",
    "        \"대한민국의 수도는?\",\n",
    "        \"대한민국의 주요 관광지 5곳을 나열하세요\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    # 각 결과의 내용을 출력합니다.\n",
    "    print(res.content)"
   ],
   "id": "fa3028ae390b75e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multimodal 모델",
   "id": "8f26f2c232948a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "langchain-teddynote 에서 구현한 멀티모달 모델에 gemini-1.5-pro 모델을 활용하여 이미지를 텍스트로 변환 가능합니다.",
   "id": "31131aaf691f00f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_teddynote.models import MultiModal\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 객체 생성\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"당신은 시인입니다. 당신의 임무는 주어진 이미지를 가지고 시를 작성하는 것입니다.\"\n",
    ")\n",
    "\n",
    "user_prompt = \"다음의 이미지에 대한 시를 작성해주세요.\"\n",
    "\n",
    "# 멀티모달 객체 생성\n",
    "multimodal_gemini = MultiModal(\n",
    "    llm, system_prompt=system_prompt, user_prompt=user_prompt\n",
    ")"
   ],
   "id": "1e3b639e2f0b2343"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 샘플 이미지 경로(파일의 경로, URL 등)를 지정합니다.\n",
    "IMAGE_URL = \"images/jeju-beach.jpg\"\n",
    "\n",
    "# 이미지 파일로 부터 질의\n",
    "answer = multimodal_gemini.stream(IMAGE_URL)\n",
    "\n",
    "# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)\n",
    "stream_response(answer)\n"
   ],
   "id": "f80bdaf99d1849ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 06. 허깅페이스 엔드포인트(HuggingFace Endpoints)",
   "id": "de814eab7d826f20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Huggingface Endpoints\n",
    "\n",
    "Hugging Face Hub은 12만 개 이상의 모델, 2만 개의 데이터셋, 5만 개의 데모 앱(Spaces)을 보유한 플랫폼으로, 모두 오픈 소스이며 공개적으로 사용 가능합니다. 이 온라인 플랫폼에서 사람들은 쉽게 협업하고 함께 머신러닝을 구축할 수 있습니다.\n",
    "\n",
    "\n",
    "Hugging Face Hub은 또한 다양한 ML 애플리케이션을 구축하기 위한 다양한 엔드포인트를 제공합니다. 이 예제는 다양한 유형의 엔드포인트에 연결하는 방법을 보여줍니다.\n",
    "\n",
    "\n",
    "특히, 텍스트 생성 추론은 Text Generation Inference에 의해 구동됩니다. 이는 매우 빠른 텍스트 생성 추론을 위해 맞춤 제작된 Rust, Python, gRPC 서버입니다."
   ],
   "id": "bb2be13e2eb1fe88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "사용하기 위해서는 Python의 huggingface_hub 패키지를 설치해야 합니다.",
   "id": "875db15903a4f76e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:54:20.345222Z",
     "start_time": "2025-11-15T14:54:18.589937Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -U huggingface_hub",
   "id": "fa7f1a878e99b6c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\osca0\\pycharmprojects\\studylangchain\\.venv\\lib\\site-packages (from typer-slim->huggingface_hub) (8.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:36:09.816382Z",
     "start_time": "2025-11-15T14:36:09.800311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "1af3c53da241d893",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:54:11.636207Z",
     "start_time": "2025-11-15T14:54:11.625066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()\n"
   ],
   "id": "666bf10603f924e2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:55:20.585640Z",
     "start_time": "2025-11-15T14:55:20.581345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ],
   "id": "d45a9f7e60be1ff1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Serverless Endpoints",
   "id": "ed1349f6e6a2f2d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Inference API 는 무료로 사용할 수 있으며 요금은 제한되어 있습니다. 프로덕션을 위한 추론 솔루션이 필요한 경우, Inference Endpoints 서비스를 확인하세요. Inference Endpoints 를 사용하면 모든 머신 러닝 모델을 전용 및 완전 관리형 인프라에 손쉽게 배포할 수 있습니다. 클라우드, 지역, 컴퓨팅 인스턴스, 자동 확장 범위 및 보안 수준을 선택하여 모델, 지연 시간, 처리량 및 규정 준수 요구 사항에 맞게 설정하세요.\n",
    "\n",
    "다음은 Inference API 에 액세스하는 방법의 예시입니다."
   ],
   "id": "f3e1226e7cda19e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "repo_id 변수에 HuggingFace 모델의 repo ID(저장소 ID) 를 할당합니다.",
   "id": "1e44b09abca4c063"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "microsoft/Phi-3-mini-4k-instruct 모델: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",
   "id": "68ab7fedb03fc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:55:24.315240Z",
     "start_time": "2025-11-15T14:55:24.211697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# 사용할 모델의 저장소 ID를 설정합니다.\n",
    "repo_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.\n",
    "    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정합니다.\n",
    "    temperature=0.1,\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # 허깅페이스 토큰\n",
    ")\n",
    "\n",
    "# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.\n",
    "response = chain.invoke({\"question\": \"what is the capital of South Korea?\"})\n",
    "print(response)\n"
   ],
   "id": "f3f90ab23620e6c1",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'from_env' from 'langchain_core.utils' (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_core\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01moutput_parsers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StrOutputParser\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HuggingFaceEndpoint\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# 사용할 모델의 저장소 ID를 설정합니다.\u001B[39;00m\n\u001B[32m      6\u001B[39m repo_id = \u001B[33m\"\u001B[39m\u001B[33mmicrosoft/Phi-3-mini-4k-instruct\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_huggingface\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      2\u001B[39m     ChatHuggingFace,  \u001B[38;5;66;03m# type: ignore[import-not-found]\u001B[39;00m\n\u001B[32m      3\u001B[39m )\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      5\u001B[39m     HuggingFaceEmbeddings,\n\u001B[32m      6\u001B[39m     HuggingFaceEndpointEmbeddings,\n\u001B[32m      7\u001B[39m )\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      9\u001B[39m     HuggingFaceEndpoint,\n\u001B[32m     10\u001B[39m     HuggingFacePipeline,\n\u001B[32m     11\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_huggingface\\chat_models\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mhuggingface\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# type: ignore[import-not-found]\u001B[39;00m\n\u001B[32m      2\u001B[39m     TGI_MESSAGE,\n\u001B[32m      3\u001B[39m     TGI_RESPONSE,\n\u001B[32m      4\u001B[39m     ChatHuggingFace,\n\u001B[32m      5\u001B[39m     _convert_message_to_chat_message,\n\u001B[32m      6\u001B[39m     _convert_TGI_message_to_LC_message,\n\u001B[32m      7\u001B[39m )\n\u001B[32m      9\u001B[39m __all__ = [\n\u001B[32m     10\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mChatHuggingFace\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     11\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m_convert_message_to_chat_message\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mTGI_RESPONSE\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     15\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:38\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpydantic\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m model_validator\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Self\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllms\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mhuggingface_endpoint\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HuggingFaceEndpoint\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllms\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mhuggingface_pipeline\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HuggingFacePipeline\n\u001B[32m     41\u001B[39m DEFAULT_SYSTEM_PROMPT = \u001B[33m\"\"\"\u001B[39m\u001B[33mYou are a helpful, respectful, and honest assistant.\u001B[39m\u001B[33m\"\"\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_huggingface\\llms\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllms\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mhuggingface_endpoint\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      2\u001B[39m     HuggingFaceEndpoint,  \u001B[38;5;66;03m# type: ignore[import-not-found]\u001B[39;00m\n\u001B[32m      3\u001B[39m )\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_huggingface\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllms\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mhuggingface_pipeline\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HuggingFacePipeline\n\u001B[32m      6\u001B[39m __all__ = [\n\u001B[32m      7\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mHuggingFaceEndpoint\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      8\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mHuggingFacePipeline\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      9\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:13\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlanguage_models\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LLM\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01moutputs\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GenerationChunk\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m from_env, get_pydantic_field_names\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpydantic\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ConfigDict, Field, model_validator\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Self\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'from_env' from 'langchain_core.utils' (C:\\Users\\osca0\\PycharmProjects\\StudyLangChain\\.venv\\Lib\\site-packages\\langchain_core\\utils\\__init__.py)"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d6bbed9f9a702c8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
